{"created":"2024-07-22 17:59:56","title":"AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description","abstract":"Our objective is to generate Audio Descriptions (ADs) for both movies and TV series in a training-free manner. We use the power of off-the-shelf Visual-Language Models (VLMs) and Large Language Models (LLMs), and develop visual and text prompting strategies for this task. Our contributions are three-fold: (i) We demonstrate that a VLM can successfully name and refer to characters if directly prompted with character information through visual indications without requiring any fine-tuning; (ii) A two-stage process is developed to generate ADs, with the first stage asking the VLM to comprehensively describe the video, followed by a second stage utilising a LLM to summarise dense textual information into one succinct AD sentence; (iii) A new dataset for TV audio description is formulated. Our approach, named AutoAD-Zero, demonstrates outstanding performance (even competitive with some models fine-tuned on ground truth ADs) in AD generation for both movies and TV series, achieving state-of-the-art CRITIC scores.","sentences":["Our objective is to generate Audio Descriptions (ADs) for both movies and TV series in a training-free manner.","We use the power of off-the-shelf Visual-Language Models (VLMs) and Large Language Models (LLMs), and develop visual and text prompting strategies for this task.","Our contributions are three-fold: (i) We demonstrate that a VLM can successfully name and refer to characters if directly prompted with character information through visual indications without requiring any fine-tuning; (ii) A two-stage process is developed to generate ADs, with the first stage asking the VLM to comprehensively describe the video, followed by a second stage utilising a LLM to summarise dense textual information into one succinct AD sentence; (iii) A new dataset for TV audio description is formulated.","Our approach, named AutoAD-Zero, demonstrates outstanding performance (even competitive with some models fine-tuned on ground truth ADs) in AD generation for both movies and TV series, achieving state-of-the-art CRITIC scores."],"url":"http://arxiv.org/abs/2407.15850v1"}
{"created":"2024-07-22 17:59:46","title":"BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis in Large-scale Scenes","abstract":"While Neural Radiance Fields (NeRFs) have demonstrated exceptional quality, their protracted training duration remains a limitation. Generalizable and MVS-based NeRFs, although capable of mitigating training time, often incur tradeoffs in quality. This paper presents a novel approach called BoostMVSNeRFs to enhance the rendering quality of MVS-based NeRFs in large-scale scenes. We first identify limitations in MVS-based NeRF methods, such as restricted viewport coverage and artifacts due to limited input views. Then, we address these limitations by proposing a new method that selects and combines multiple cost volumes during volume rendering. Our method does not require training and can adapt to any MVS-based NeRF methods in a feed-forward fashion to improve rendering quality. Furthermore, our approach is also end-to-end trainable, allowing fine-tuning on specific scenes. We demonstrate the effectiveness of our method through experiments on large-scale datasets, showing significant rendering quality improvements in large-scale scenes and unbounded outdoor scenarios. We release the source code of BoostMVSNeRFs at https://su-terry.github.io/BoostMVSNeRFs/.","sentences":["While Neural Radiance Fields (NeRFs) have demonstrated exceptional quality, their protracted training duration remains a limitation.","Generalizable and MVS-based NeRFs, although capable of mitigating training time, often incur tradeoffs in quality.","This paper presents a novel approach called BoostMVSNeRFs to enhance the rendering quality of MVS-based NeRFs in large-scale scenes.","We first identify limitations in MVS-based NeRF methods, such as restricted viewport coverage and artifacts due to limited input views.","Then, we address these limitations by proposing a new method that selects and combines multiple cost volumes during volume rendering.","Our method does not require training and can adapt to any MVS-based NeRF methods in a feed-forward fashion to improve rendering quality.","Furthermore, our approach is also end-to-end trainable, allowing fine-tuning on specific scenes.","We demonstrate the effectiveness of our method through experiments on large-scale datasets, showing significant rendering quality improvements in large-scale scenes and unbounded outdoor scenarios.","We release the source code of BoostMVSNeRFs at https://su-terry.github.io/BoostMVSNeRFs/."],"url":"http://arxiv.org/abs/2407.15848v1"}
{"created":"2024-07-22 17:59:46","title":"WayEx: Waypoint Exploration using a Single Demonstration","abstract":"We propose WayEx, a new method for learning complex goal-conditioned robotics tasks from a single demonstration. Our approach distinguishes itself from existing imitation learning methods by demanding fewer expert examples and eliminating the need for information about the actions taken during the demonstration. This is accomplished by introducing a new reward function and employing a knowledge expansion technique. We demonstrate the effectiveness of WayEx, our waypoint exploration strategy, across six diverse tasks, showcasing its applicability in various environments. Notably, our method significantly reduces training time by 50% as compared to traditional reinforcement learning methods. WayEx obtains a higher reward than existing imitation learning methods given only a single demonstration. Furthermore, we demonstrate its success in tackling complex environments where standard approaches fall short. More information is available at: https://waypoint-ex.github.io.","sentences":["We propose WayEx, a new method for learning complex goal-conditioned robotics tasks from a single demonstration.","Our approach distinguishes itself from existing imitation learning methods by demanding fewer expert examples and eliminating the need for information about the actions taken during the demonstration.","This is accomplished by introducing a new reward function and employing a knowledge expansion technique.","We demonstrate the effectiveness of WayEx, our waypoint exploration strategy, across six diverse tasks, showcasing its applicability in various environments.","Notably, our method significantly reduces training time by 50% as compared to traditional reinforcement learning methods.","WayEx obtains a higher reward than existing imitation learning methods given only a single demonstration.","Furthermore, we demonstrate its success in tackling complex environments where standard approaches fall short.","More information is available at: https://waypoint-ex.github.io."],"url":"http://arxiv.org/abs/2407.15849v1"}
{"created":"2024-07-22 17:59:45","title":"LLMmap: Fingerprinting For Large Language Models","abstract":"We introduce LLMmap, a first-generation fingerprinting attack targeted at LLM-integrated applications. LLMmap employs an active fingerprinting approach, sending carefully crafted queries to the application and analyzing the responses to identify the specific LLM model in use. With as few as 8 interactions, LLMmap can accurately identify LLMs with over 95% accuracy. More importantly, LLMmap is designed to be robust across different application layers, allowing it to identify LLMs operating under various system prompts, stochastic sampling hyperparameters, and even complex generation frameworks such as RAG or Chain-of-Thought.","sentences":["We introduce LLMmap, a first-generation fingerprinting attack targeted at LLM-integrated applications.","LLMmap employs an active fingerprinting approach, sending carefully crafted queries to the application and analyzing the responses to identify the specific LLM model in use.","With as few as 8 interactions, LLMmap can accurately identify LLMs with over 95% accuracy.","More importantly, LLMmap is designed to be robust across different application layers, allowing it to identify LLMs operating under various system prompts, stochastic sampling hyperparameters, and even complex generation frameworks such as RAG or Chain-of-Thought."],"url":"http://arxiv.org/abs/2407.15847v1"}
{"created":"2024-07-22 17:59:10","title":"Reconstructing Training Data From Real World Models Trained with Transfer Learning","abstract":"Current methods for reconstructing training data from trained classifiers are restricted to very small models, limited training set sizes, and low-resolution images. Such restrictions hinder their applicability to real-world scenarios. In this paper, we present a novel approach enabling data reconstruction in realistic settings for models trained on high-resolution images. Our method adapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios -- specifically, targeting models trained via transfer learning over image embeddings of large pre-trained models like DINO-ViT and CLIP. Our work employs data reconstruction in the embedding space rather than in the image space, showcasing its applicability beyond visual data. Moreover, we introduce a novel clustering-based method to identify good reconstructions from thousands of candidates. This significantly improves on previous works that relied on knowledge of the training set to identify good reconstructed images. Our findings shed light on a potential privacy risk for data leakage from models trained using transfer learning.","sentences":["Current methods for reconstructing training data from trained classifiers are restricted to very small models, limited training set sizes, and low-resolution images.","Such restrictions hinder their applicability to real-world scenarios.","In this paper, we present a novel approach enabling data reconstruction in realistic settings for models trained on high-resolution images.","Our method adapts the reconstruction scheme of arXiv:2206.07758 to real-world scenarios -- specifically, targeting models trained via transfer learning over image embeddings of large pre-trained models like DINO-ViT and CLIP.","Our work employs data reconstruction in the embedding space rather than in the image space, showcasing its applicability beyond visual data.","Moreover, we introduce a novel clustering-based method to identify good reconstructions from thousands of candidates.","This significantly improves on previous works that relied on knowledge of the training set to identify good reconstructed images.","Our findings shed light on a potential privacy risk for data leakage from models trained using transfer learning."],"url":"http://arxiv.org/abs/2407.15845v1"}
{"created":"2024-07-22 17:59:01","title":"CarFormer: Self-Driving with Learned Object-Centric Representations","abstract":"The choice of representation plays a key role in self-driving. Bird's eye view (BEV) representations have shown remarkable performance in recent years. In this paper, we propose to learn object-centric representations in BEV to distill a complex scene into more actionable information for self-driving. We first learn to place objects into slots with a slot attention model on BEV sequences. Based on these object-centric representations, we then train a transformer to learn to drive as well as reason about the future of other vehicles. We found that object-centric slot representations outperform both scene-level and object-level approaches that use the exact attributes of objects. Slot representations naturally incorporate information about objects from their spatial and temporal context such as position, heading, and speed without explicitly providing it. Our model with slots achieves an increased completion rate of the provided routes and, consequently, a higher driving score, with a lower variance across multiple runs, affirming slots as a reliable alternative in object-centric approaches. Additionally, we validate our model's performance as a world model through forecasting experiments, demonstrating its capability to predict future slot representations accurately. The code and the pre-trained models can be found at https://kuis-ai.github.io/CarFormer/.","sentences":["The choice of representation plays a key role in self-driving.","Bird's eye view (BEV) representations have shown remarkable performance in recent years.","In this paper, we propose to learn object-centric representations in BEV to distill a complex scene into more actionable information for self-driving.","We first learn to place objects into slots with a slot attention model on BEV sequences.","Based on these object-centric representations, we then train a transformer to learn to drive as well as reason about the future of other vehicles.","We found that object-centric slot representations outperform both scene-level and object-level approaches that use the exact attributes of objects.","Slot representations naturally incorporate information about objects from their spatial and temporal context such as position, heading, and speed without explicitly providing it.","Our model with slots achieves an increased completion rate of the provided routes and, consequently, a higher driving score, with a lower variance across multiple runs, affirming slots as a reliable alternative in object-centric approaches.","Additionally, we validate our model's performance as a world model through forecasting experiments, demonstrating its capability to predict future slot representations accurately.","The code and the pre-trained models can be found at https://kuis-ai.github.io/CarFormer/."],"url":"http://arxiv.org/abs/2407.15843v1"}
{"created":"2024-07-22 17:59:01","title":"HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning","abstract":"Predicting camera-space hand meshes from single RGB images is crucial for enabling realistic hand interactions in 3D virtual and augmented worlds. Previous work typically divided the task into two stages: given a cropped image of the hand, predict meshes in relative coordinates, followed by lifting these predictions into camera space in a separate and independent stage, often resulting in the loss of valuable contextual and scale information. To prevent the loss of these cues, we propose unifying these two stages into an end-to-end solution that addresses the 2D-3D correspondence problem. This solution enables back-propagation from camera space outputs to the rest of the network through a new differentiable global positioning module. We also introduce an image rectification step that harmonizes both the training dataset and the input image as if they were acquired with the same camera, helping to alleviate the inherent scale-depth ambiguity of the problem. We validate the effectiveness of our framework in evaluations against several baselines and state-of-the-art approaches across three public benchmarks.","sentences":["Predicting camera-space hand meshes from single RGB images is crucial for enabling realistic hand interactions in 3D virtual and augmented worlds.","Previous work typically divided the task into two stages: given a cropped image of the hand, predict meshes in relative coordinates, followed by lifting these predictions into camera space in a separate and independent stage, often resulting in the loss of valuable contextual and scale information.","To prevent the loss of these cues, we propose unifying these two stages into an end-to-end solution that addresses the 2D-3D correspondence problem.","This solution enables back-propagation from camera space outputs to the rest of the network through a new differentiable global positioning module.","We also introduce an image rectification step that harmonizes both the training dataset and the input image as if they were acquired with the same camera, helping to alleviate the inherent scale-depth ambiguity of the problem.","We validate the effectiveness of our framework in evaluations against several baselines and state-of-the-art approaches across three public benchmarks."],"url":"http://arxiv.org/abs/2407.15844v1"}
{"created":"2024-07-22 17:58:05","title":"Artist: Aesthetically Controllable Text-Driven Stylization without Training","abstract":"Diffusion models entangle content and style generation during the denoising process, leading to undesired content modification when directly applied to stylization tasks. Existing methods struggle to effectively control the diffusion model to meet the aesthetic-level requirements for stylization. In this paper, we introduce \\textbf{Artist}, a training-free approach that aesthetically controls the content and style generation of a pretrained diffusion model for text-driven stylization. Our key insight is to disentangle the denoising of content and style into separate diffusion processes while sharing information between them. We propose simple yet effective content and style control methods that suppress style-irrelevant content generation, resulting in harmonious stylization results. Extensive experiments demonstrate that our method excels at achieving aesthetic-level stylization requirements, preserving intricate details in the content image and aligning well with the style prompt. Furthermore, we showcase the highly controllability of the stylization strength from various perspectives. Code will be released, project home page: https://DiffusionArtist.github.io","sentences":["Diffusion models entangle content and style generation during the denoising process, leading to undesired content modification when directly applied to stylization tasks.","Existing methods struggle to effectively control the diffusion model to meet the aesthetic-level requirements for stylization.","In this paper, we introduce \\textbf{Artist}, a training-free approach that aesthetically controls the content and style generation of a pretrained diffusion model for text-driven stylization.","Our key insight is to disentangle the denoising of content and style into separate diffusion processes while sharing information between them.","We propose simple yet effective content and style control methods that suppress style-irrelevant content generation, resulting in harmonious stylization results.","Extensive experiments demonstrate that our method excels at achieving aesthetic-level stylization requirements, preserving intricate details in the content image and aligning well with the style prompt.","Furthermore, we showcase the highly controllability of the stylization strength from various perspectives.","Code will be released, project home page: https://DiffusionArtist.github.io"],"url":"http://arxiv.org/abs/2407.15842v1"}
{"created":"2024-07-22 17:58:04","title":"SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models","abstract":"We propose SlowFast-LLaVA (or SF-LLaVA for short), a training-free video large language model (LLM) that can jointly capture the detailed spatial semantics and long-range temporal context without exceeding the token budget of commonly used LLMs. This is realized by using a two-stream SlowFast design of inputs for Video LLMs to aggregate features from sampled video frames in an effective way. Specifically, the Slow pathway extracts features at a low frame rate while keeping as many spatial details as possible (e.g., with 24x24 tokens), and the Fast pathway operates on a high frame rate but uses a larger spatial pooling stride (e.g., downsampling 6x) to focus on the motion cues. As a result, this design allows us to adequately capture both spatial and temporal features that are beneficial for understanding details along the video. Experimental results show that SF-LLaVA outperforms existing training-free methods on a wide range of video tasks. On some benchmarks, it achieves comparable or even better performance compared to state-of-the-art Video LLMs that are fine-tuned on video datasets.","sentences":["We propose SlowFast-LLaVA (or SF-LLaVA for short), a training-free video large language model (LLM) that can jointly capture the detailed spatial semantics and long-range temporal context without exceeding the token budget of commonly used LLMs.","This is realized by using a two-stream SlowFast design of inputs for Video LLMs to aggregate features from sampled video frames in an effective way.","Specifically, the Slow pathway extracts features at a low frame rate while keeping as many spatial details as possible (e.g., with 24x24 tokens), and the Fast pathway operates on a high frame rate but uses a larger spatial pooling stride (e.g., downsampling 6x) to focus on the motion cues.","As a result, this design allows us to adequately capture both spatial and temporal features that are beneficial for understanding details along the video.","Experimental results show that SF-LLaVA outperforms existing training-free methods on a wide range of video tasks.","On some benchmarks, it achieves comparable or even better performance compared to state-of-the-art Video LLMs that are fine-tuned on video datasets."],"url":"http://arxiv.org/abs/2407.15841v1"}
{"created":"2024-07-22 17:57:59","title":"QueST: Self-Supervised Skill Abstractions for Learning Continuous Control","abstract":"Generalization capabilities, or rather a lack thereof, is one of the most important unsolved problems in the field of robot learning, and while several large scale efforts have set out to tackle this problem, unsolved it remains. In this paper, we hypothesize that learning temporal action abstractions using latent variable models (LVMs), which learn to map data to a compressed latent space and back, is a promising direction towards low-level skills that can readily be used for new tasks. Although several works have attempted to show this, they have generally been limited by architectures that do not faithfully capture shareable representations. To address this we present Quantized Skill Transformer (QueST), which learns a larger and more flexible latent encoding that is more capable of modeling the breadth of low-level skills necessary for a variety of tasks. To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations. We compare to state-of-the-art imitation learning and LVM baselines and see that QueST's architecture leads to strong performance on several multitask and few-shot learning benchmarks. Further results and videos are available at https://quest-model.github.io/","sentences":["Generalization capabilities, or rather a lack thereof, is one of the most important unsolved problems in the field of robot learning, and while several large scale efforts have set out to tackle this problem, unsolved it remains.","In this paper, we hypothesize that learning temporal action abstractions using latent variable models (LVMs), which learn to map data to a compressed latent space and back, is a promising direction towards low-level skills that can readily be used for new tasks.","Although several works have attempted to show this, they have generally been limited by architectures that do not faithfully capture shareable representations.","To address this we present Quantized Skill Transformer (QueST), which learns a larger and more flexible latent encoding that is more capable of modeling the breadth of low-level skills necessary for a variety of tasks.","To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations.","We compare to state-of-the-art imitation learning and LVM baselines and see that QueST's architecture leads to strong performance on several multitask and few-shot learning benchmarks.","Further results and videos are available at https://quest-model.github.io/"],"url":"http://arxiv.org/abs/2407.15840v1"}
{"created":"2024-07-22 17:57:12","title":"Importance Sampling-Guided Meta-Training for Intelligent Agents in Highly Interactive Environments","abstract":"Training intelligent agents to navigate highly interactive environments presents significant challenges. While guided meta reinforcement learning (RL) approach that first trains a guiding policy to train the ego agent has proven effective in improving generalizability across various levels of interaction, the state-of-the-art method tends to be overly sensitive to extreme cases, impairing the agents' performance in the more common scenarios. This study introduces a novel training framework that integrates guided meta RL with importance sampling (IS) to optimize training distributions for navigating highly interactive driving scenarios, such as T-intersections. Unlike traditional methods that may underrepresent critical interactions or overemphasize extreme cases during training, our approach strategically adjusts the training distribution towards more challenging driving behaviors using IS proposal distributions and applies the importance ratio to de-bias the result. By estimating a naturalistic distribution from real-world datasets and employing a mixture model for iterative training refinements, the framework ensures a balanced focus across common and extreme driving scenarios. Experiments conducted with both synthetic dataset and T-intersection scenarios from the InD dataset demonstrate not only accelerated training but also improvement in agent performance under naturalistic conditions, showcasing the efficacy of combining IS with meta RL in training reliable autonomous agents for highly interactive navigation tasks.","sentences":["Training intelligent agents to navigate highly interactive environments presents significant challenges.","While guided meta reinforcement learning (RL) approach that first trains a guiding policy to train the ego agent has proven effective in improving generalizability across various levels of interaction, the state-of-the-art method tends to be overly sensitive to extreme cases, impairing the agents' performance in the more common scenarios.","This study introduces a novel training framework that integrates guided meta RL with importance sampling (IS) to optimize training distributions for navigating highly interactive driving scenarios, such as T-intersections.","Unlike traditional methods that may underrepresent critical interactions or overemphasize extreme cases during training, our approach strategically adjusts the training distribution towards more challenging driving behaviors using IS proposal distributions and applies the importance ratio to de-bias the result.","By estimating a naturalistic distribution from real-world datasets and employing a mixture model for iterative training refinements, the framework ensures a balanced focus across common and extreme driving scenarios.","Experiments conducted with both synthetic dataset and T-intersection scenarios from the InD dataset demonstrate not only accelerated training but also improvement in agent performance under naturalistic conditions, showcasing the efficacy of combining IS with meta RL in training reliable autonomous agents for highly interactive navigation tasks."],"url":"http://arxiv.org/abs/2407.15839v1"}
{"created":"2024-07-22 17:55:22","title":"MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with Extensive Diversity","abstract":"Despite the effectiveness of vision-language supervised fine-tuning in enhancing the performance of Vision Large Language Models (VLLMs). However, existing visual instruction tuning datasets include the following limitations: (1) Instruction annotation quality: despite existing VLLMs exhibiting strong performance, instructions generated by those advanced VLLMs may still suffer from inaccuracies, such as hallucinations. (2) Instructions and image diversity: the limited range of instruction types and the lack of diversity in image data may impact the model's ability to generate diversified and closer to real-world scenarios outputs. To address these challenges, we construct a high-quality, diverse visual instruction tuning dataset MMInstruct, which consists of 973K instructions from 24 domains. There are four instruction types: Judgement, Multiple-Choice, Long Visual Question Answering and Short Visual Question Answering. To construct MMInstruct, we propose an instruction generation data engine that leverages GPT-4V, GPT-3.5, and manual correction. Our instruction generation engine enables semi-automatic, low-cost, and multi-domain instruction generation at 1/6 the cost of manual construction. Through extensive experiment validation and ablation experiments, we demonstrate that MMInstruct could significantly improve the performance of VLLMs, e.g., the model fine-tuning on MMInstruct achieves new state-of-the-art performance on 10 out of 12 benchmarks. The code and data shall be available at https://github.com/yuecao0119/MMInstruct.","sentences":["Despite the effectiveness of vision-language supervised fine-tuning in enhancing the performance of Vision Large Language Models (VLLMs).","However, existing visual instruction tuning datasets include the following limitations: (1) Instruction annotation quality: despite existing VLLMs exhibiting strong performance, instructions generated by those advanced VLLMs may still suffer from inaccuracies, such as hallucinations.","(2) Instructions and image diversity: the limited range of instruction types and the lack of diversity in image data may impact the model's ability to generate diversified and closer to real-world scenarios outputs.","To address these challenges, we construct a high-quality, diverse visual instruction tuning dataset MMInstruct, which consists of 973K instructions from 24 domains.","There are four instruction types: Judgement, Multiple-Choice, Long Visual Question Answering and Short Visual Question Answering.","To construct MMInstruct, we propose an instruction generation data engine that leverages GPT-4V, GPT-3.5, and manual correction.","Our instruction generation engine enables semi-automatic, low-cost, and multi-domain instruction generation at 1/6 the cost of manual construction.","Through extensive experiment validation and ablation experiments, we demonstrate that MMInstruct could significantly improve the performance of VLLMs, e.g., the model fine-tuning on MMInstruct achieves new state-of-the-art performance on 10 out of 12 benchmarks.","The code and data shall be available at https://github.com/yuecao0119/MMInstruct."],"url":"http://arxiv.org/abs/2407.15838v1"}
{"created":"2024-07-22 17:54:41","title":"Towards Latent Masked Image Modeling for Self-Supervised Visual Representation Learning","abstract":"Masked Image Modeling (MIM) has emerged as a promising method for deriving visual representations from unlabeled image data by predicting missing pixels from masked portions of images. It excels in region-aware learning and provides strong initializations for various tasks, but struggles to capture high-level semantics without further supervised fine-tuning, likely due to the low-level nature of its pixel reconstruction objective. A promising yet unrealized framework is learning representations through masked reconstruction in latent space, combining the locality of MIM with the high-level targets. However, this approach poses significant training challenges as the reconstruction targets are learned in conjunction with the model, potentially leading to trivial or suboptimal solutions.Our study is among the first to thoroughly analyze and address the challenges of such framework, which we refer to as Latent MIM. Through a series of carefully designed experiments and extensive analysis, we identify the source of these challenges, including representation collapsing for joint online/target optimization, learning objectives, the high region correlation in latent space and decoding conditioning. By sequentially addressing these issues, we demonstrate that Latent MIM can indeed learn high-level representations while retaining the benefits of MIM models.","sentences":["Masked Image Modeling (MIM) has emerged as a promising method for deriving visual representations from unlabeled image data by predicting missing pixels from masked portions of images.","It excels in region-aware learning and provides strong initializations for various tasks, but struggles to capture high-level semantics without further supervised fine-tuning, likely due to the low-level nature of its pixel reconstruction objective.","A promising yet unrealized framework is learning representations through masked reconstruction in latent space, combining the locality of MIM with the high-level targets.","However, this approach poses significant training challenges as the reconstruction targets are learned in conjunction with the model, potentially leading to trivial or suboptimal solutions.","Our study is among the first to thoroughly analyze and address the challenges of such framework, which we refer to as Latent MIM.","Through a series of carefully designed experiments and extensive analysis, we identify the source of these challenges, including representation collapsing for joint online/target optimization, learning objectives, the high region correlation in latent space and decoding conditioning.","By sequentially addressing these issues, we demonstrate that Latent MIM can indeed learn high-level representations while retaining the benefits of MIM models."],"url":"http://arxiv.org/abs/2407.15837v1"}
{"created":"2024-07-22 17:51:53","title":"dMel: Speech Tokenization made Simple","abstract":"Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated complicated speech tokenization methods to discretize continuous speech signals so that language modeling techniques can be applied to speech data. However, existing approaches either model semantic tokens, potentially losing acoustic information, or model acoustic tokens, risking the loss of semantic information. Having multiple token types also complicates the architecture and requires additional pretraining. Here we show that discretizing mel-filterbank channels into discrete intensity bins produces a simple representation (dMel), that performs better than other existing speech tokenization methods. Using a transformer decoder-only architecture for speech-text modeling, we comprehensively evaluate different speech tokenization methods on speech recognition (ASR), speech synthesis (TTS). Our results demonstrate the effectiveness of dMel in achieving high performance on both tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.","sentences":["Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data.","Inspired by this success, researchers have investigated complicated speech tokenization methods to discretize continuous speech signals so that language modeling techniques can be applied to speech data.","However, existing approaches either model semantic tokens, potentially losing acoustic information, or model acoustic tokens, risking the loss of semantic information.","Having multiple token types also complicates the architecture and requires additional pretraining.","Here we show that discretizing mel-filterbank channels into discrete intensity bins produces a simple representation (dMel), that performs better than other existing speech tokenization methods.","Using a transformer decoder-only architecture for speech-text modeling, we comprehensively evaluate different speech tokenization methods on speech recognition (ASR), speech synthesis (TTS).","Our results demonstrate the effectiveness of dMel in achieving high performance on both tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text."],"url":"http://arxiv.org/abs/2407.15835v1"}
{"created":"2024-07-22 17:51:15","title":"Inequalities in Computational Thinking Among Incoming Students in an STEM Chilean University","abstract":"While computational thinking arises as an essential skill worldwide, formal primary and secondary education in Latin America rarely incorporates mechanisms to develop it in their curricula. The extent to which students in the region acquire computational thinking skills remains largely unknown. To start addressing this void, this article presents findings from a cross-sectional study that characterizes the computational thinking abilities of incoming students at a Chilean university with a strong emphasis on STEM disciplines. Based on more than 500 responses, this study provides evidence of significant inequalities in computational thinking across gender, type of school (private or no), and prior programming knowledge. The discussion offers insights into how these disparities relate to contextual factors of the country, such as a highly socio-economically segregated educational system, public policies focused mainly on technology access, and heavy reliance on voluntary initiatives, to develop computational thinking. The findings can enlighten upcoming research endeavors and formulate strategies to create a more equitable field for students entering STEM degrees in nations facing similar circumstances.","sentences":["While computational thinking arises as an essential skill worldwide, formal primary and secondary education in Latin America rarely incorporates mechanisms to develop it in their curricula.","The extent to which students in the region acquire computational thinking skills remains largely unknown.","To start addressing this void, this article presents findings from a cross-sectional study that characterizes the computational thinking abilities of incoming students at a Chilean university with a strong emphasis on STEM disciplines.","Based on more than 500 responses, this study provides evidence of significant inequalities in computational thinking across gender, type of school (private or no), and prior programming knowledge.","The discussion offers insights into how these disparities relate to contextual factors of the country, such as a highly socio-economically segregated educational system, public policies focused mainly on technology access, and heavy reliance on voluntary initiatives, to develop computational thinking.","The findings can enlighten upcoming research endeavors and formulate strategies to create a more equitable field for students entering STEM degrees in nations facing similar circumstances."],"url":"http://arxiv.org/abs/2407.15833v1"}
{"created":"2024-07-22 17:50:31","title":"NV-Retriever: Improving text embedding models with effective hard-negative mining","abstract":"Text embedding models have been popular for information retrieval applications such as semantic search and Question-Answering systems based on Retrieval-Augmented Generation (RAG). Those models are typically Transformer models that are fine-tuned with contrastive learning objectives. Many papers introduced new embedding model architectures and training approaches, however, one of the key ingredients, the process of mining negative passages, remains poorly explored or described. One of the challenging aspects of fine-tuning embedding models is the selection of high quality hard-negative passages for contrastive learning. In this paper we propose a family of positive-aware mining methods that leverage the positive relevance score for more effective false negatives removal. We also provide a comprehensive ablation study on hard-negative mining methods over their configurations, exploring different teacher and base models. We demonstrate the efficacy of our proposed methods by introducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval (BEIR) benchmark and 0.65 points higher than previous methods. The model placed 1st when it was published to MTEB Retrieval on July 07, 2024.","sentences":["Text embedding models have been popular for information retrieval applications such as semantic search and Question-Answering systems based on Retrieval-Augmented Generation (RAG).","Those models are typically Transformer models that are fine-tuned with contrastive learning objectives.","Many papers introduced new embedding model architectures and training approaches, however, one of the key ingredients, the process of mining negative passages, remains poorly explored or described.","One of the challenging aspects of fine-tuning embedding models is the selection of high quality hard-negative passages for contrastive learning.","In this paper we propose a family of positive-aware mining methods that leverage the positive relevance score for more effective false negatives removal.","We also provide a comprehensive ablation study on hard-negative mining methods over their configurations, exploring different teacher and base models.","We demonstrate the efficacy of our proposed methods by introducing the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval (BEIR) benchmark and 0.65 points higher than previous methods.","The model placed 1st when it was published to MTEB Retrieval on July 07, 2024."],"url":"http://arxiv.org/abs/2407.15831v1"}
{"created":"2024-07-22 17:47:05","title":"Investigating Benefits and Limitations of Migrating to a Micro-Frontends Architecture","abstract":"[Context] The adoption of micro-frontends architectures has gained traction as a promising approach to enhance modularity, scalability, and maintainability of web applications. [Goal] The primary aim of this research is to investigate the benefits and limitations of migrating a real-world application to a micro-frontends architecture from the perspective of the developers. [Method] Based on the action research approach, after diagnosis and planning, we applied an intervention of migrating the target web application to a micro-frontends architecture. Thereafter, the migration was evaluated in a workshop involving the remaining developers responsible for maintaining the application. During the workshop, these developers were presented with the migrated architecture, conducted a simple maintenance task, discussed benefits and limitations in a focus group to gather insights, and answered a questionnaire on the acceptance of the technology. [Results] Developers' perceptions gathered during the focus group reinforce the benefits and limitations reported in the literature. Key benefits included enhanced flexibility in technology choices, scalability of development teams, and gradual migration of technologies. However, the increased complexity of the architecture raised concerns among developers, particularly in dependency and environment management, debugging, and integration testing. [Conclusions] While micro-frontends represent a promising technology, unresolved issues still limit their broader applicability. Developers generally perceived the architecture as useful and moderately easy to use but hesitated to adopt it.","sentences":["[Context] The adoption of micro-frontends architectures has gained traction as a promising approach to enhance modularity, scalability, and maintainability of web applications.","[Goal] The primary aim of this research is to investigate the benefits and limitations of migrating a real-world application to a micro-frontends architecture from the perspective of the developers.","[Method] Based on the action research approach, after diagnosis and planning, we applied an intervention of migrating the target web application to a micro-frontends architecture.","Thereafter, the migration was evaluated in a workshop involving the remaining developers responsible for maintaining the application.","During the workshop, these developers were presented with the migrated architecture, conducted a simple maintenance task, discussed benefits and limitations in a focus group to gather insights, and answered a questionnaire on the acceptance of the technology.","[Results] Developers' perceptions gathered during the focus group reinforce the benefits and limitations reported in the literature.","Key benefits included enhanced flexibility in technology choices, scalability of development teams, and gradual migration of technologies.","However, the increased complexity of the architecture raised concerns among developers, particularly in dependency and environment management, debugging, and integration testing.","[Conclusions] While micro-frontends represent a promising technology, unresolved issues still limit their broader applicability.","Developers generally perceived the architecture as useful and moderately easy to use but hesitated to adopt it."],"url":"http://arxiv.org/abs/2407.15829v1"}
{"created":"2024-07-22 17:46:50","title":"J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling","abstract":"Spoken dialogue plays a crucial role in human-AI interactions, necessitating dialogue-oriented spoken language models (SLMs). To develop versatile SLMs, large-scale and diverse speech datasets are essential. Additionally, to ensure hiqh-quality speech generation, the data must be spontaneous like in-wild data and must be acoustically clean with noise removed. Despite the critical need, no open-source corpus meeting all these criteria has been available. This study addresses this gap by constructing and releasing a large-scale spoken dialogue corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly accessible. Furthermore, this paper presents a language-independent method for corpus construction and describes experiments on dialogue generation using SLMs trained on J-CHAT. Experimental results indicate that the collected data from multiple domains by our method improve the naturalness and meaningfulness of dialogue generation.","sentences":["Spoken dialogue plays a crucial role in human-AI interactions, necessitating dialogue-oriented spoken language models (SLMs).","To develop versatile SLMs, large-scale and diverse speech datasets are essential.","Additionally, to ensure hiqh-quality speech generation, the data must be spontaneous like in-wild data and must be acoustically clean with noise removed.","Despite the critical need, no open-source corpus meeting all these criteria has been available.","This study addresses this gap by constructing and releasing a large-scale spoken dialogue corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly accessible.","Furthermore, this paper presents a language-independent method for corpus construction and describes experiments on dialogue generation using SLMs trained on J-CHAT.","Experimental results indicate that the collected data from multiple domains by our method improve the naturalness and meaningfulness of dialogue generation."],"url":"http://arxiv.org/abs/2407.15828v1"}
{"created":"2024-07-22 17:37:04","title":"A Large-scale Benchmark Dataset for Commuting Origin-destination Matrix Generation","abstract":"The commuting origin-destination~(OD) matrix is a critical input for urban planning and transportation, providing crucial information about the population residing in one region and working in another within an interested area. Despite its importance, obtaining and updating the matrix is challenging due to high costs and privacy concerns. This has spurred research into generating commuting OD matrices for areas lacking historical data, utilizing readily available information via computational models. In this regard, existing research is primarily restricted to only a single or few large cities, preventing these models from being applied effectively in other areas with distinct characteristics, particularly in towns and rural areas where such data is urgently needed. To address this, we propose a large-scale dataset comprising commuting OD matrices for 3,233 diverse areas around the U.S. For each area, we provide the commuting OD matrix, combined with regional attributes including demographics and point-of-interests of each region in that area. We believe this comprehensive dataset will facilitate the development of more generalizable commuting OD matrix generation models, which can capture various patterns of distinct areas. Additionally, we use this dataset to benchmark a set of commuting OD generation models, including physical models, element-wise predictive models, and matrix-wise generative models. Surprisingly, we find a new paradigm, which considers the whole area combined with its commuting OD matrix as an attributed directed weighted graph and generates the weighted edges based on the node attributes, can achieve the optimal. This may inspire a new research direction from graph learning in this field.","sentences":["The commuting origin-destination~(OD) matrix is a critical input for urban planning and transportation, providing crucial information about the population residing in one region and working in another within an interested area.","Despite its importance, obtaining and updating the matrix is challenging due to high costs and privacy concerns.","This has spurred research into generating commuting OD matrices for areas lacking historical data, utilizing readily available information via computational models.","In this regard, existing research is primarily restricted to only a single or few large cities, preventing these models from being applied effectively in other areas with distinct characteristics, particularly in towns and rural areas where such data is urgently needed.","To address this, we propose a large-scale dataset comprising commuting OD matrices for 3,233 diverse areas around the U.S.","For each area, we provide the commuting OD matrix, combined with regional attributes including demographics and point-of-interests of each region in that area.","We believe this comprehensive dataset will facilitate the development of more generalizable commuting OD matrix generation models, which can capture various patterns of distinct areas.","Additionally, we use this dataset to benchmark a set of commuting OD generation models, including physical models, element-wise predictive models, and matrix-wise generative models.","Surprisingly, we find a new paradigm, which considers the whole area combined with its commuting OD matrix as an attributed directed weighted graph and generates the weighted edges based on the node attributes, can achieve the optimal.","This may inspire a new research direction from graph learning in this field."],"url":"http://arxiv.org/abs/2407.15823v1"}
{"created":"2024-07-22 17:35:18","title":"Towards Effective Collaboration between Software Engineers and Data Scientists developing Machine Learning-Enabled Systems","abstract":"Incorporating Machine Learning (ML) into existing systems is a demand that has grown among several organizations. However, the development of ML-enabled systems encompasses several social and technical challenges, which must be addressed by actors with different fields of expertise working together. This paper has the objective of understanding how to enhance the collaboration between two key actors in building these systems: software engineers and data scientists. We conducted two focus group sessions with experienced data scientists and software engineers working on real-world ML-enabled systems to assess the relevance of different recommendations for specific technical tasks. Our research has found that collaboration between these actors is important for effectively developing ML-enabled systems, especially when defining data access and ML model deployment. Participants provided concrete examples of how recommendations depicted in the literature can benefit collaboration during different tasks. For example, defining clear responsibilities for each team member and creating concise documentation can improve communication and overall performance. Our study contributes to a better understanding of how to foster effective collaboration between software engineers and data scientists creating ML-enabled systems.","sentences":["Incorporating Machine Learning (ML) into existing systems is a demand that has grown among several organizations.","However, the development of ML-enabled systems encompasses several social and technical challenges, which must be addressed by actors with different fields of expertise working together.","This paper has the objective of understanding how to enhance the collaboration between two key actors in building these systems: software engineers and data scientists.","We conducted two focus group sessions with experienced data scientists and software engineers working on real-world ML-enabled systems to assess the relevance of different recommendations for specific technical tasks.","Our research has found that collaboration between these actors is important for effectively developing ML-enabled systems, especially when defining data access and ML model deployment.","Participants provided concrete examples of how recommendations depicted in the literature can benefit collaboration during different tasks.","For example, defining clear responsibilities for each team member and creating concise documentation can improve communication and overall performance.","Our study contributes to a better understanding of how to foster effective collaboration between software engineers and data scientists creating ML-enabled systems."],"url":"http://arxiv.org/abs/2407.15821v1"}
{"created":"2024-07-22 17:33:49","title":"Accelerating Pre-training of Multimodal LLMs via Chain-of-Sight","abstract":"This paper introduces Chain-of-Sight, a vision-language bridge module that accelerates the pre-training of Multimodal Large Language Models (MLLMs). Our approach employs a sequence of visual resamplers that capture visual details at various spacial scales. This architecture not only leverages global and local visual contexts effectively, but also facilitates the flexible extension of visual tokens through a compound token scaling strategy, allowing up to a 16x increase in the token count post pre-training. Consequently, Chain-of-Sight requires significantly fewer visual tokens in the pre-training phase compared to the fine-tuning phase. This intentional reduction of visual tokens during pre-training notably accelerates the pre-training process, cutting down the wall-clock training time by ~73%. Empirical results on a series of vision-language benchmarks reveal that the pre-train acceleration through Chain-of-Sight is achieved without sacrificing performance, matching or surpassing the standard pipeline of utilizing all visual tokens throughout the entire training process. Further scaling up the number of visual tokens for pre-training leads to stronger performances, competitive to existing approaches in a series of benchmarks.","sentences":["This paper introduces Chain-of-Sight, a vision-language bridge module that accelerates the pre-training of Multimodal Large Language Models (MLLMs).","Our approach employs a sequence of visual resamplers that capture visual details at various spacial scales.","This architecture not only leverages global and local visual contexts effectively, but also facilitates the flexible extension of visual tokens through a compound token scaling strategy, allowing up to a 16x increase in the token count post pre-training.","Consequently, Chain-of-Sight requires significantly fewer visual tokens in the pre-training phase compared to the fine-tuning phase.","This intentional reduction of visual tokens during pre-training notably accelerates the pre-training process, cutting down the wall-clock training time by ~73%.","Empirical results on a series of vision-language benchmarks reveal that the pre-train acceleration through Chain-of-Sight is achieved without sacrificing performance, matching or surpassing the standard pipeline of utilizing all visual tokens throughout the entire training process.","Further scaling up the number of visual tokens for pre-training leads to stronger performances, competitive to existing approaches in a series of benchmarks."],"url":"http://arxiv.org/abs/2407.15819v1"}
{"created":"2024-07-22 17:31:57","title":"Efficient and generalizable prediction of molecular alterations in multiple cancer cohorts using H&E whole slide images","abstract":"Molecular testing of tumor samples for targetable biomarkers is restricted by a lack of standardization, turnaround-time, cost, and tissue availability across cancer types. Additionally, targetable alterations of low prevalence may not be tested in routine workflows. Algorithms that predict DNA alterations from routinely generated hematoxylin and eosin (H&E)-stained images could prioritize samples for confirmatory molecular testing. Costs and the necessity of a large number of samples containing mutations limit approaches that train individual algorithms for each alteration. In this work, models were trained for simultaneous prediction of multiple DNA alterations from H&E images using a multi-task approach. Compared to biomarker-specific models, this approach performed better on average, with pronounced gains for rare mutations. The models reasonably generalized to independent temporal-holdout, externally-stained, and multi-site TCGA test sets. Additionally, whole slide image embeddings derived using multi-task models demonstrated strong performance in downstream tasks that were not a part of training. Overall, this is a promising approach to develop clinically useful algorithms that provide multiple actionable predictions from a single slide.","sentences":["Molecular testing of tumor samples for targetable biomarkers is restricted by a lack of standardization, turnaround-time, cost, and tissue availability across cancer types.","Additionally, targetable alterations of low prevalence may not be tested in routine workflows.","Algorithms that predict DNA alterations from routinely generated hematoxylin and eosin (H&E)-stained images could prioritize samples for confirmatory molecular testing.","Costs and the necessity of a large number of samples containing mutations limit approaches that train individual algorithms for each alteration.","In this work, models were trained for simultaneous prediction of multiple DNA alterations from H&E images using a multi-task approach.","Compared to biomarker-specific models, this approach performed better on average, with pronounced gains for rare mutations.","The models reasonably generalized to independent temporal-holdout, externally-stained, and multi-site TCGA test sets.","Additionally, whole slide image embeddings derived using multi-task models demonstrated strong performance in downstream tasks that were not a part of training.","Overall, this is a promising approach to develop clinically useful algorithms that provide multiple actionable predictions from a single slide."],"url":"http://arxiv.org/abs/2407.15816v1"}
{"created":"2024-07-22 17:29:02","title":"Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning","abstract":"Can we endow visuomotor robots with generalization capabilities to operate in diverse open-world scenarios? In this paper, we propose \\textbf{Maniwhere}, a generalizable framework tailored for visual reinforcement learning, enabling the trained robot policies to generalize across a combination of multiple visual disturbance types. Specifically, we introduce a multi-view representation learning approach fused with Spatial Transformer Network (STN) module to capture shared semantic information and correspondences among different viewpoints. In addition, we employ a curriculum-based randomization and augmentation approach to stabilize the RL training process and strengthen the visual generalization ability. To exhibit the effectiveness of Maniwhere, we meticulously design 8 tasks encompassing articulate objects, bi-manual, and dexterous hand manipulation tasks, demonstrating Maniwhere's strong visual generalization and sim2real transfer abilities across 3 hardware platforms. Our experiments show that Maniwhere significantly outperforms existing state-of-the-art methods. Videos are provided at https://gemcollector.github.io/maniwhere/.","sentences":["Can we endow visuomotor robots with generalization capabilities to operate in diverse open-world scenarios?","In this paper, we propose \\textbf{Maniwhere}, a generalizable framework tailored for visual reinforcement learning, enabling the trained robot policies to generalize across a combination of multiple visual disturbance types.","Specifically, we introduce a multi-view representation learning approach fused with Spatial Transformer Network (STN) module to capture shared semantic information and correspondences among different viewpoints.","In addition, we employ a curriculum-based randomization and augmentation approach to stabilize the RL training process and strengthen the visual generalization ability.","To exhibit the effectiveness of Maniwhere, we meticulously design 8 tasks encompassing articulate objects, bi-manual, and dexterous hand manipulation tasks, demonstrating Maniwhere's strong visual generalization and sim2real transfer abilities across 3 hardware platforms.","Our experiments show that Maniwhere significantly outperforms existing state-of-the-art methods.","Videos are provided at https://gemcollector.github.io/maniwhere/."],"url":"http://arxiv.org/abs/2407.15815v1"}
{"created":"2024-07-22 17:26:12","title":"Perceptions of Linguistic Uncertainty by Language Models and Humans","abstract":"Uncertainty expressions such as ``probably'' or ``highly unlikely'' are pervasive in human language. While prior work has established that there is population-level agreement in terms of how humans interpret these expressions, there has been little inquiry into the abilities of language models to interpret such expressions. In this paper, we investigate how language models map linguistic expressions of uncertainty to numerical responses. Our approach assesses whether language models can employ theory of mind in this setting: understanding the uncertainty of another agent about a particular statement, independently of the model's own certainty about that statement. We evaluate both humans and 10 popular language models on a task created to assess these abilities. Unexpectedly, we find that 8 out of 10 models are able to map uncertainty expressions to probabilistic responses in a human-like manner. However, we observe systematically different behavior depending on whether a statement is actually true or false. This sensitivity indicates that language models are substantially more susceptible to bias based on their prior knowledge (as compared to humans). These findings raise important questions and have broad implications for human-AI alignment and AI-AI communication.","sentences":["Uncertainty expressions such as ``probably'' or ``highly unlikely'' are pervasive in human language.","While prior work has established that there is population-level agreement in terms of how humans interpret these expressions, there has been little inquiry into the abilities of language models to interpret such expressions.","In this paper, we investigate how language models map linguistic expressions of uncertainty to numerical responses.","Our approach assesses whether language models can employ theory of mind in this setting: understanding the uncertainty of another agent about a particular statement, independently of the model's own certainty about that statement.","We evaluate both humans and 10 popular language models on a task created to assess these abilities.","Unexpectedly, we find that 8 out of 10 models are able to map uncertainty expressions to probabilistic responses in a human-like manner.","However, we observe systematically different behavior depending on whether a statement is actually true or false.","This sensitivity indicates that language models are substantially more susceptible to bias based on their prior knowledge (as compared to humans).","These findings raise important questions and have broad implications for human-AI alignment and AI-AI communication."],"url":"http://arxiv.org/abs/2407.15814v1"}
{"created":"2024-07-22 17:23:28","title":"Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget","abstract":"As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to address this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose to randomly mask up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only \\$1,890 economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive FID and high-quality generations while incurring 118$\\times$ lower cost than stable diffusion models and 14$\\times$ lower cost than the current state-of-the-art approach that costs \\$28,400. We aim to release our end-to-end training pipeline to further democratize the training of large-scale diffusion models on micro-budgets.","sentences":["As scaling laws in generative AI push performance, they also simultaneously concentrate the development of these models among actors with large computational resources.","With a focus on text-to-image (T2I) generative models, we aim to address this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models.","As the computational cost of transformers increases with the number of patches in each image, we propose to randomly mask up to 75% of the image patches during training.","We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost.","We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training.","Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only \\$1,890 economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset.","Notably, our model achieves competitive FID and high-quality generations while incurring 118$\\times$ lower cost than stable diffusion models and 14$\\times$ lower cost than the current state-of-the-art approach that costs \\$28,400.","We aim to release our end-to-end training pipeline to further democratize the training of large-scale diffusion models on micro-budgets."],"url":"http://arxiv.org/abs/2407.15811v1"}
{"created":"2024-07-22 17:22:04","title":"Breaking the Global North Stereotype: A Global South-centric Benchmark Dataset for Auditing and Mitigating Biases in Facial Recognition Systems","abstract":"Facial Recognition Systems (FRSs) are being developed and deployed globally at unprecedented rates. Most platforms are designed in a limited set of countries but deployed in worldwide, without adequate checkpoints. This is especially problematic for Global South countries which lack strong legislation to safeguard persons facing disparate performance of these systems. A combination of unavailability of datasets, lack of understanding of FRS functionality and low-resource bias mitigation measures accentuate the problem. In this work, we propose a new face dataset composed of 6,579 unique male and female sportspersons from eight countries around the world. More than 50% of the dataset comprises individuals from the Global South countries and is demographically diverse. To aid adversarial audits and robust model training, each image has four adversarial variants, totaling over 40,000 images. We also benchmark five popular FRSs, both commercial and open-source, for the task of gender prediction (and country prediction for one of the open-source models as an example of red-teaming). Experiments on industrial FRSs reveal accuracies ranging from 98.2%--38.1%, with a large disparity between males and females in the Global South (max difference of 38.5%). Biases are also observed in all FRSs between females of the Global North and South (max difference of ~50%). Grad-CAM analysis identifies the nose, forehead and mouth as the regions of interest on one of the open-source FRSs. Utilizing this insight, we design simple, low-resource bias mitigation solutions using few-shot and novel contrastive learning techniques significantly improving the accuracy with disparity between males and females reducing from 50% to 1.5% in one of the settings. In the red-teaming experiment with the open-source Deepface model, contrastive learning proves more effective than simple fine-tuning.","sentences":["Facial Recognition Systems (FRSs) are being developed and deployed globally at unprecedented rates.","Most platforms are designed in a limited set of countries but deployed in worldwide, without adequate checkpoints.","This is especially problematic for Global South countries which lack strong legislation to safeguard persons facing disparate performance of these systems.","A combination of unavailability of datasets, lack of understanding of FRS functionality and low-resource bias mitigation measures accentuate the problem.","In this work, we propose a new face dataset composed of 6,579 unique male and female sportspersons from eight countries around the world.","More than 50% of the dataset comprises individuals from the Global South countries and is demographically diverse.","To aid adversarial audits and robust model training, each image has four adversarial variants, totaling over 40,000 images.","We also benchmark five popular FRSs, both commercial and open-source, for the task of gender prediction (and country prediction for one of the open-source models as an example of red-teaming).","Experiments on industrial FRSs reveal accuracies ranging from 98.2%--38.1%, with a large disparity between males and females in the Global South (max difference of 38.5%).","Biases are also observed in all FRSs between females of the Global North and South (max difference of ~50%).","Grad-CAM analysis identifies the nose, forehead and mouth as the regions of interest on one of the open-source FRSs.","Utilizing this insight, we design simple, low-resource bias mitigation solutions using few-shot and novel contrastive learning techniques significantly improving the accuracy with disparity between males and females reducing from 50% to 1.5% in one of the settings.","In the red-teaming experiment with the open-source Deepface model, contrastive learning proves more effective than simple fine-tuning."],"url":"http://arxiv.org/abs/2407.15810v1"}
{"created":"2024-07-22 17:21:07","title":"Universal Optimization for Non-Clairvoyant Subadditive Joint Replenishment","abstract":"The online joint replenishment problem (JRP) is a fundamental problem in the area of online problems with delay. Over the last decade, several works have studied generalizations of JRP with different cost functions for servicing requests. Most prior works on JRP and its generalizations have focused on the clairvoyant setting. Recently, Touitou [Tou23a] developed a non-clairvoyant framework that provided an $O(\\sqrt{n \\log n})$ upper bound for a wide class of generalized JRP, where $n$ is the number of request types.   We advance the study of non-clairvoyant algorithms by providing a simpler, modular framework that matches the competitive ratio established by Touitou for the same class of generalized JRP. Our key insight is to leverage universal algorithms for Set Cover to approximate arbitrary monotone subadditive functions using a simple class of functions termed \\textit{disjoint}. This allows us to reduce the problem to several independent instances of the TCP Acknowledgement problem, for which a simple 2-competitive non-clairvoyant algorithm is known. The modularity of our framework is a major advantage as it allows us to tailor the reduction to specific problems and obtain better competitive ratios. In particular, we obtain tight $O(\\sqrt{n})$-competitive algorithms for two significant problems: Multi-Level Aggregation and Weighted Symmetric Subadditive Joint Replenishment. We also show that, in contrast, Touitou's algorithm is $\\Omega(\\sqrt{n \\log n})$-competitive for both of these problems.","sentences":["The online joint replenishment problem (JRP) is a fundamental problem in the area of online problems with delay.","Over the last decade, several works have studied generalizations of JRP with different cost functions for servicing requests.","Most prior works on JRP and its generalizations have focused on the clairvoyant setting.","Recently, Touitou [Tou23a] developed a non-clairvoyant framework that provided an $O(\\sqrt{n \\log n})$ upper bound for a wide class of generalized JRP, where $n$ is the number of request types.   ","We advance the study of non-clairvoyant algorithms by providing a simpler, modular framework that matches the competitive ratio established by Touitou for the same class of generalized JRP.","Our key insight is to leverage universal algorithms for Set Cover to approximate arbitrary monotone subadditive functions using a simple class of functions termed \\textit{disjoint}.","This allows us to reduce the problem to several independent instances of the TCP Acknowledgement problem, for which a simple 2-competitive non-clairvoyant algorithm is known.","The modularity of our framework is a major advantage as it allows us to tailor the reduction to specific problems and obtain better competitive ratios.","In particular, we obtain tight $O(\\sqrt{n})$-competitive algorithms for two significant problems: Multi-Level Aggregation and Weighted Symmetric Subadditive Joint Replenishment.","We also show that, in contrast, Touitou's algorithm is $\\Omega(\\sqrt{n \\log n})$-competitive for both of these problems."],"url":"http://arxiv.org/abs/2407.15809v1"}
{"created":"2024-07-22 17:20:22","title":"FSboard: Over 3 million characters of ASL fingerspelling collected via smartphones","abstract":"Progress in machine understanding of sign languages has been slow and hampered by limited data. In this paper, we present FSboard, an American Sign Language fingerspelling dataset situated in a mobile text entry use case, collected from 147 paid and consenting Deaf signers using Pixel 4A selfie cameras in a variety of environments. Fingerspelling recognition is an incomplete solution that is only one small part of sign language translation, but it could provide some immediate benefit to Deaf/Hard of Hearing signers as more broadly capable technology develops. At >3 million characters in length and >250 hours in duration, FSboard is the largest fingerspelling recognition dataset to date by a factor of >10x. As a simple baseline, we finetune 30 Hz MediaPipe Holistic landmark inputs into ByT5-Small and achieve 11.1% Character Error Rate (CER) on a test set with unique phrases and signers. This quality degrades gracefully when decreasing frame rate and excluding face/body landmarks: plausible optimizations to help models run on device in real time.","sentences":["Progress in machine understanding of sign languages has been slow and hampered by limited data.","In this paper, we present FSboard, an American Sign Language fingerspelling dataset situated in a mobile text entry use case, collected from 147 paid and consenting Deaf signers using Pixel 4A selfie cameras in a variety of environments.","Fingerspelling recognition is an incomplete solution that is only one small part of sign language translation, but it could provide some immediate benefit to Deaf/Hard of Hearing signers as more broadly capable technology develops.","At >3 million characters in length and >250 hours in duration, FSboard is the largest fingerspelling recognition dataset to date by a factor of >10x.","As a simple baseline, we finetune 30","Hz MediaPipe Holistic landmark inputs into ByT5-Small and achieve 11.1% Character Error Rate (CER) on a test set with unique phrases and signers.","This quality degrades gracefully when decreasing frame rate and excluding face/body landmarks: plausible optimizations to help models run on device in real time."],"url":"http://arxiv.org/abs/2407.15806v1"}
{"created":"2024-07-22 17:18:26","title":"A simple and fast C++ thread pool implementation capable of running task graphs","abstract":"In this paper, the author presents a simple and fast C++ thread pool implementation capable of running task graphs. The implementation is publicly available on GitHub, see https://github.com/dpuyda/scheduling.","sentences":["In this paper, the author presents a simple and fast C++ thread pool implementation capable of running task graphs.","The implementation is publicly available on GitHub, see https://github.com/dpuyda/scheduling."],"url":"http://arxiv.org/abs/2407.15805v1"}
{"created":"2024-07-22 17:13:23","title":"Enhancing Mass Customization Manufacturing: Multiobjective Metaheuristic Algorithms for flow shop Production in Smart Industry","abstract":"The current landscape of massive production industries is undergoing significant transformations driven by emerging customer trends and new smart manufacturing technologies. One such change is the imperative to implement mass customization, wherein products are tailored to individual customer specifications while still ensuring cost efficiency through large-scale production processes. These shifts can profoundly impact various facets of the industry. This study focuses on the necessary adaptations in shop-floor production planning. Specifically, it proposes the use of efficient evolutionary algorithms to tackle the flowshop with missing operations, considering different optimization objectives: makespan, weighted total tardiness, and total completion time. An extensive computational experimentation is conducted across a range of realistic instances, encompassing varying numbers of jobs, operations, and probabilities of missing operations. The findings demonstrate the competitiveness of the proposed approach and enable the identification of the most suitable evolutionary algorithms for addressing this problem. Additionally, the impact of the probability of missing operations on optimization objectives is discussed.","sentences":["The current landscape of massive production industries is undergoing significant transformations driven by emerging customer trends and new smart manufacturing technologies.","One such change is the imperative to implement mass customization, wherein products are tailored to individual customer specifications while still ensuring cost efficiency through large-scale production processes.","These shifts can profoundly impact various facets of the industry.","This study focuses on the necessary adaptations in shop-floor production planning.","Specifically, it proposes the use of efficient evolutionary algorithms to tackle the flowshop with missing operations, considering different optimization objectives: makespan, weighted total tardiness, and total completion time.","An extensive computational experimentation is conducted across a range of realistic instances, encompassing varying numbers of jobs, operations, and probabilities of missing operations.","The findings demonstrate the competitiveness of the proposed approach and enable the identification of the most suitable evolutionary algorithms for addressing this problem.","Additionally, the impact of the probability of missing operations on optimization objectives is discussed."],"url":"http://arxiv.org/abs/2407.15802v1"}
{"created":"2024-07-22 17:00:02","title":"Robust Facial Reactions Generation: An Emotion-Aware Framework with Modality Compensation","abstract":"The objective of the Multiple Appropriate Facial Reaction Generation (MAFRG) task is to produce contextually appropriate and diverse listener facial behavioural responses based on the multimodal behavioural data of the conversational partner (i.e., the speaker). Current methodologies typically assume continuous availability of speech and facial modality data, neglecting real-world scenarios where these data may be intermittently unavailable, which often results in model failures. Furthermore, despite utilising advanced deep learning models to extract information from the speaker's multimodal inputs, these models fail to adequately leverage the speaker's emotional context, which is vital for eliciting appropriate facial reactions from human listeners. To address these limitations, we propose an Emotion-aware Modality Compensatory (EMC) framework. This versatile solution can be seamlessly integrated into existing models, thereby preserving their advantages while significantly enhancing performance and robustness in scenarios with missing modalities. Our framework ensures resilience when faced with missing modality data through the Compensatory Modality Alignment (CMA) module. It also generates more appropriate emotion-aware reactions via the Emotion-aware Attention (EA) module, which incorporates the speaker's emotional information throughout the entire encoding and decoding process. Experimental results demonstrate that our framework improves the appropriateness metric FRCorr by an average of 57.2\\% compared to the original model structure. In scenarios where speech modality data is missing, the performance of appropriate generation shows an improvement, and when facial data is missing, it only exhibits minimal degradation.","sentences":["The objective of the Multiple Appropriate Facial Reaction Generation (MAFRG) task is to produce contextually appropriate and diverse listener facial behavioural responses based on the multimodal behavioural data of the conversational partner (i.e., the speaker).","Current methodologies typically assume continuous availability of speech and facial modality data, neglecting real-world scenarios where these data may be intermittently unavailable, which often results in model failures.","Furthermore, despite utilising advanced deep learning models to extract information from the speaker's multimodal inputs, these models fail to adequately leverage the speaker's emotional context, which is vital for eliciting appropriate facial reactions from human listeners.","To address these limitations, we propose an Emotion-aware Modality Compensatory (EMC) framework.","This versatile solution can be seamlessly integrated into existing models, thereby preserving their advantages while significantly enhancing performance and robustness in scenarios with missing modalities.","Our framework ensures resilience when faced with missing modality data through the Compensatory Modality Alignment (CMA) module.","It also generates more appropriate emotion-aware reactions via the Emotion-aware Attention (EA) module, which incorporates the speaker's emotional information throughout the entire encoding and decoding process.","Experimental results demonstrate that our framework improves the appropriateness metric FRCorr by an average of 57.2\\% compared to the original model structure.","In scenarios where speech modality data is missing, the performance of appropriate generation shows an improvement, and when facial data is missing, it only exhibits minimal degradation."],"url":"http://arxiv.org/abs/2407.15798v1"}
{"created":"2024-07-22 16:59:49","title":"MILAN: Milli-Annotations for Lidar Semantic Segmentation","abstract":"Annotating lidar point clouds for autonomous driving is a notoriously expensive and time-consuming task. In this work, we show that the quality of recent self-supervised lidar scan representations allows a great reduction of the annotation cost. Our method has two main steps. First, we show that self-supervised representations allow a simple and direct selection of highly informative lidar scans to annotate: training a network on these selected scans leads to much better results than a random selection of scans and, more interestingly, to results on par with selections made by SOTA active learning methods. In a second step, we leverage the same self-supervised representations to cluster points in our selected scans. Asking the annotator to classify each cluster, with a single click per cluster, then permits us to close the gap with fully-annotated training sets, while only requiring one thousandth of the point labels.","sentences":["Annotating lidar point clouds for autonomous driving is a notoriously expensive and time-consuming task.","In this work, we show that the quality of recent self-supervised lidar scan representations allows a great reduction of the annotation cost.","Our method has two main steps.","First, we show that self-supervised representations allow a simple and direct selection of highly informative lidar scans to annotate: training a network on these selected scans leads to much better results than a random selection of scans and, more interestingly, to results on par with selections made by SOTA active learning methods.","In a second step, we leverage the same self-supervised representations to cluster points in our selected scans.","Asking the annotator to classify each cluster, with a single click per cluster, then permits us to close the gap with fully-annotated training sets, while only requiring one thousandth of the point labels."],"url":"http://arxiv.org/abs/2407.15797v1"}
{"created":"2024-07-22 16:52:37","title":"AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection","abstract":"Zero-shot anomaly detection (ZSAD) targets the identification of anomalies within images from arbitrary novel categories. This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP. AdaCLIP incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data. Two types of learnable prompts are proposed: static and dynamic. Static prompts are shared across all images, serving to preliminarily adapt CLIP for ZSAD. In contrast, dynamic prompts are generated for each test image, providing CLIP with dynamic adaptation capabilities. The combination of static and dynamic prompts is referred to as hybrid prompts, and yields enhanced ZSAD performance. Extensive experiments conducted across 14 real-world anomaly detection datasets from industrial and medical domains indicate that AdaCLIP outperforms other ZSAD methods and can generalize better to different categories and even domains. Finally, our analysis highlights the importance of diverse auxiliary data and optimized prompts for enhanced generalization capacity. Code is available at https://github.com/caoyunkang/AdaCLIP.","sentences":["Zero-shot anomaly detection (ZSAD) targets the identification of anomalies within images from arbitrary novel categories.","This study introduces AdaCLIP for the ZSAD task, leveraging a pre-trained vision-language model (VLM), CLIP.","AdaCLIP incorporates learnable prompts into CLIP and optimizes them through training on auxiliary annotated anomaly detection data.","Two types of learnable prompts are proposed:","static and dynamic.","Static prompts are shared across all images, serving to preliminarily adapt CLIP for ZSAD.","In contrast, dynamic prompts are generated for each test image, providing CLIP with dynamic adaptation capabilities.","The combination of static and dynamic prompts is referred to as hybrid prompts, and yields enhanced ZSAD performance.","Extensive experiments conducted across 14 real-world anomaly detection datasets from industrial and medical domains indicate that AdaCLIP outperforms other ZSAD methods and can generalize better to different categories and even domains.","Finally, our analysis highlights the importance of diverse auxiliary data and optimized prompts for enhanced generalization capacity.","Code is available at https://github.com/caoyunkang/AdaCLIP."],"url":"http://arxiv.org/abs/2407.15795v1"}
{"created":"2024-07-22 16:52:32","title":"Disentangling spatio-temporal knowledge for weakly supervised object detection and segmentation in surgical video","abstract":"Weakly supervised video object segmentation (WSVOS) enables the identification of segmentation maps without requiring an extensive training dataset of object masks, relying instead on coarse video labels indicating object presence. Current state-of-the-art methods either require multiple independent stages of processing that employ motion cues or, in the case of end-to-end trainable networks, lack in segmentation accuracy, in part due to the difficulty of learning segmentation maps from videos with transient object presence. This limits the application of WSVOS for semantic annotation of surgical videos where multiple surgical tools frequently move in and out of the field of view, a problem that is more difficult than typically encountered in WSVOS. This paper introduces Video Spatio-Temporal Disentanglement Networks (VDST-Net), a framework to disentangle spatiotemporal information using semi-decoupled knowledge distillation to predict high-quality class activation maps (CAMs). A teacher network designed to resolve temporal conflicts when specifics about object location and timing in the video are not provided works with a student network that integrates information over time by leveraging temporal dependencies. We demonstrate the efficacy of our framework on a public reference dataset and on a more challenging surgical video dataset where objects are, on average, present in less than 60\\% of annotated frames. Our method outperforms state-of-the-art techniques and generates superior segmentation masks under video-level weak supervision.","sentences":["Weakly supervised video object segmentation (WSVOS) enables the identification of segmentation maps without requiring an extensive training dataset of object masks, relying instead on coarse video labels indicating object presence.","Current state-of-the-art methods either require multiple independent stages of processing that employ motion cues or, in the case of end-to-end trainable networks, lack in segmentation accuracy, in part due to the difficulty of learning segmentation maps from videos with transient object presence.","This limits the application of WSVOS for semantic annotation of surgical videos where multiple surgical tools frequently move in and out of the field of view, a problem that is more difficult than typically encountered in WSVOS.","This paper introduces Video Spatio-Temporal Disentanglement Networks (VDST-Net), a framework to disentangle spatiotemporal information using semi-decoupled knowledge distillation to predict high-quality class activation maps (CAMs).","A teacher network designed to resolve temporal conflicts when specifics about object location and timing in the video are not provided works with a student network that integrates information over time by leveraging temporal dependencies.","We demonstrate the efficacy of our framework on a public reference dataset and on a more challenging surgical video dataset where objects are, on average, present in less than 60\\% of annotated frames.","Our method outperforms state-of-the-art techniques and generates superior segmentation masks under video-level weak supervision."],"url":"http://arxiv.org/abs/2407.15794v1"}
{"created":"2024-07-22 16:51:28","title":"CLIP with Generative Latent Replay: a Strong Baseline for Incremental Learning","abstract":"With the emergence of Transformers and Vision-Language Models (VLMs) such as CLIP, large pre-trained models have become a common strategy to enhance performance in Continual Learning scenarios. This led to the development of numerous prompting strategies to effectively fine-tune transformer-based models without succumbing to catastrophic forgetting. However, these methods struggle to specialize the model on domains significantly deviating from the pre-training and preserving its zero-shot capabilities. In this work, we propose Continual Generative training for Incremental prompt-Learning, a novel approach to mitigate forgetting while adapting a VLM, which exploits generative replay to align prompts to tasks. We also introduce a new metric to evaluate zero-shot capabilities within CL benchmarks. Through extensive experiments on different domains, we demonstrate the effectiveness of our framework in adapting to new tasks while improving zero-shot capabilities. Further analysis reveals that our approach can bridge the gap with joint prompt tuning. The codebase is available at https://github.com/aimagelab/mammoth.","sentences":["With the emergence of Transformers and Vision-Language Models (VLMs) such as CLIP, large pre-trained models have become a common strategy to enhance performance in Continual Learning scenarios.","This led to the development of numerous prompting strategies to effectively fine-tune transformer-based models without succumbing to catastrophic forgetting.","However, these methods struggle to specialize the model on domains significantly deviating from the pre-training and preserving its zero-shot capabilities.","In this work, we propose Continual Generative training for Incremental prompt-Learning, a novel approach to mitigate forgetting while adapting a VLM, which exploits generative replay to align prompts to tasks.","We also introduce a new metric to evaluate zero-shot capabilities within CL benchmarks.","Through extensive experiments on different domains, we demonstrate the effectiveness of our framework in adapting to new tasks while improving zero-shot capabilities.","Further analysis reveals that our approach can bridge the gap with joint prompt tuning.","The codebase is available at https://github.com/aimagelab/mammoth."],"url":"http://arxiv.org/abs/2407.15793v1"}
{"created":"2024-07-22 16:51:05","title":"Robust Mixture Learning when Outliers Overwhelm Small Groups","abstract":"We study the problem of estimating the means of well-separated mixtures when an adversary may add arbitrary outliers. While strong guarantees are available when the outlier fraction is significantly smaller than the minimum mixing weight, much less is known when outliers may crowd out low-weight clusters - a setting we refer to as list-decodable mixture learning (LD-ML). In this case, adversarial outliers can simulate additional spurious mixture components. Hence, if all means of the mixture must be recovered up to a small error in the output list, the list size needs to be larger than the number of (true) components. We propose an algorithm that obtains order-optimal error guarantees for each mixture mean with a minimal list-size overhead, significantly improving upon list-decodable mean estimation, the only existing method that is applicable for LD-ML. Although improvements are observed even when the mixture is non-separated, our algorithm achieves particularly strong guarantees when the mixture is separated: it can leverage the mixture structure to partially cluster the samples before carefully iterating a base learner for list-decodable mean estimation at different scales.","sentences":["We study the problem of estimating the means of well-separated mixtures when an adversary may add arbitrary outliers.","While strong guarantees are available when the outlier fraction is significantly smaller than the minimum mixing weight, much less is known when outliers may crowd out low-weight clusters - a setting we refer to as list-decodable mixture learning (LD-ML).","In this case, adversarial outliers can simulate additional spurious mixture components.","Hence, if all means of the mixture must be recovered up to a small error in the output list, the list size needs to be larger than the number of (true) components.","We propose an algorithm that obtains order-optimal error guarantees for each mixture mean with a minimal list-size overhead, significantly improving upon list-decodable mean estimation, the only existing method that is applicable for LD-ML.","Although improvements are observed even when the mixture is non-separated, our algorithm achieves particularly strong guarantees when the mixture is separated: it can leverage the mixture structure to partially cluster the samples before carefully iterating a base learner for list-decodable mean estimation at different scales."],"url":"http://arxiv.org/abs/2407.15792v1"}
{"created":"2024-07-22 16:49:58","title":"RADA: Robust and Accurate Feature Learning with Domain Adaptation","abstract":"Recent advancements in keypoint detection and descriptor extraction have shown impressive performance in local feature learning tasks. However, existing methods generally exhibit suboptimal performance under extreme conditions such as significant appearance changes and domain shifts. In this study, we introduce a multi-level feature aggregation network that incorporates two pivotal components to facilitate the learning of robust and accurate features with domain adaptation. First, we employ domain adaptation supervision to align high-level feature distributions across different domains to achieve invariant domain representations. Second, we propose a Transformer-based booster that enhances descriptor robustness by integrating visual and geometric information through wave position encoding concepts, effectively handling complex conditions. To ensure the accuracy and robustness of features, we adopt a hierarchical architecture to capture comprehensive information and apply meticulous targeted supervision to keypoint detection, descriptor extraction, and their coupled processing. Extensive experiments demonstrate that our method, RADA, achieves excellent results in image matching, camera pose estimation, and visual localization tasks.","sentences":["Recent advancements in keypoint detection and descriptor extraction have shown impressive performance in local feature learning tasks.","However, existing methods generally exhibit suboptimal performance under extreme conditions such as significant appearance changes and domain shifts.","In this study, we introduce a multi-level feature aggregation network that incorporates two pivotal components to facilitate the learning of robust and accurate features with domain adaptation.","First, we employ domain adaptation supervision to align high-level feature distributions across different domains to achieve invariant domain representations.","Second, we propose a Transformer-based booster that enhances descriptor robustness by integrating visual and geometric information through wave position encoding concepts, effectively handling complex conditions.","To ensure the accuracy and robustness of features, we adopt a hierarchical architecture to capture comprehensive information and apply meticulous targeted supervision to keypoint detection, descriptor extraction, and their coupled processing.","Extensive experiments demonstrate that our method, RADA, achieves excellent results in image matching, camera pose estimation, and visual localization tasks."],"url":"http://arxiv.org/abs/2407.15791v1"}
{"created":"2024-07-22 16:47:31","title":"Extracting Structured Insights from Financial News: An Augmented LLM Driven Approach","abstract":"Financial news plays a crucial role in decision-making processes across the financial sector, yet the efficient processing of this information into a structured format remains challenging. This paper presents a novel approach to financial news processing that leverages Large Language Models (LLMs) to overcome limitations that previously prevented the extraction of structured data from unstructured financial news. We introduce a system that extracts relevant company tickers from raw news article content, performs sentiment analysis at the company level, and generates summaries, all without relying on pre-structured data feeds. Our methodology combines the generative capabilities of LLMs, and recent prompting techniques, with a robust validation framework that uses a tailored string similarity approach. Evaluation on a dataset of 5530 financial news articles demonstrates the effectiveness of our approach, with 90% of articles not missing any tickers compared with current data providers, and 22% of articles having additional relevant tickers. In addition to this paper, the methodology has been implemented at scale with the resulting processed data made available through a live API endpoint, which is updated in real-time with the latest news. To the best of our knowledge, we are the first data provider to offer granular, per-company sentiment analysis from news articles, enhancing the depth of information available to market participants. We also release the evaluation dataset of 5530 processed articles as a static file, which we hope will facilitate further research leveraging financial news.","sentences":["Financial news plays a crucial role in decision-making processes across the financial sector, yet the efficient processing of this information into a structured format remains challenging.","This paper presents a novel approach to financial news processing that leverages Large Language Models (LLMs) to overcome limitations that previously prevented the extraction of structured data from unstructured financial news.","We introduce a system that extracts relevant company tickers from raw news article content, performs sentiment analysis at the company level, and generates summaries, all without relying on pre-structured data feeds.","Our methodology combines the generative capabilities of LLMs, and recent prompting techniques, with a robust validation framework that uses a tailored string similarity approach.","Evaluation on a dataset of 5530 financial news articles demonstrates the effectiveness of our approach, with 90% of articles not missing any tickers compared with current data providers, and 22% of articles having additional relevant tickers.","In addition to this paper, the methodology has been implemented at scale with the resulting processed data made available through a live API endpoint, which is updated in real-time with the latest news.","To the best of our knowledge, we are the first data provider to offer granular, per-company sentiment analysis from news articles, enhancing the depth of information available to market participants.","We also release the evaluation dataset of 5530 processed articles as a static file, which we hope will facilitate further research leveraging financial news."],"url":"http://arxiv.org/abs/2407.15788v1"}
{"created":"2024-07-22 16:47:29","title":"Unsupervised Mastoidectomy for Cochlear CT Mesh Reconstruction Using Highly Noisy Data","abstract":"Cochlear Implant (CI) procedures involve inserting an array of electrodes into the cochlea located inside the inner ear. Mastoidectomy is a surgical procedure that uses a high-speed drill to remove part of the mastoid region of the temporal bone, providing safe access to the cochlea through the middle and inner ear. We aim to develop an intraoperative navigation system that registers plans created using 3D preoperative Computerized Tomography (CT) volumes with the 2D surgical microscope view. Herein, we propose a method to synthesize the mastoidectomy volume using only the preoperative CT scan, where the mastoid is intact. We introduce an unsupervised learning framework designed to synthesize mastoidectomy. For model training purposes, this method uses postoperative CT scans to avoid manual data cleaning or labeling, even when the region removed during mastoidectomy is visible but affected by metal artifacts, low signal-to-noise ratio, or electrode wiring. Our approach estimates mastoidectomy regions with a mean dice score of 70.0%. This approach represents a major step forward for CI intraoperative navigation by predicting realistic mastoidectomy-removed regions in preoperative planning that can be used to register the pre-surgery plan to intraoperative microscopy.","sentences":["Cochlear Implant (CI) procedures involve inserting an array of electrodes into the cochlea located inside the inner ear.","Mastoidectomy is a surgical procedure that uses a high-speed drill to remove part of the mastoid region of the temporal bone, providing safe access to the cochlea through the middle and inner ear.","We aim to develop an intraoperative navigation system that registers plans created using 3D preoperative Computerized Tomography (CT) volumes with the 2D surgical microscope view.","Herein, we propose a method to synthesize the mastoidectomy volume using only the preoperative CT scan, where the mastoid is intact.","We introduce an unsupervised learning framework designed to synthesize mastoidectomy.","For model training purposes, this method uses postoperative CT scans to avoid manual data cleaning or labeling, even when the region removed during mastoidectomy is visible but affected by metal artifacts, low signal-to-noise ratio, or electrode wiring.","Our approach estimates mastoidectomy regions with a mean dice score of 70.0%.","This approach represents a major step forward for CI intraoperative navigation by predicting realistic mastoidectomy-removed regions in preoperative planning that can be used to register the pre-surgery plan to intraoperative microscopy."],"url":"http://arxiv.org/abs/2407.15787v1"}
{"created":"2024-07-22 16:46:33","title":"Concept-Based Interpretable Reinforcement Learning with Limited to No Human Labels","abstract":"Recent advances in reinforcement learning (RL) have predominantly leveraged neural network-based policies for decision-making, yet these models often lack interpretability, posing challenges for stakeholder comprehension and trust. Concept bottleneck models offer an interpretable alternative by integrating human-understandable concepts into neural networks. However, a significant limitation in prior work is the assumption that human annotations for these concepts are readily available during training, necessitating continuous real-time input from human annotators. To overcome this limitation, we introduce a novel training scheme that enables RL algorithms to efficiently learn a concept-based policy by only querying humans to label a small set of data, or in the extreme case, without any human labels. Our algorithm, LICORICE, involves three main contributions: interleaving concept learning and RL training, using a concept ensembles to actively select informative data points for labeling, and decorrelating the concept data with a simple strategy. We show how LICORICE reduces manual labeling efforts to to 500 or fewer concept labels in three environments. Finally, we present an initial study to explore how we can use powerful vision-language models to infer concepts from raw visual inputs without explicit labels at minimal cost to performance.","sentences":["Recent advances in reinforcement learning (RL) have predominantly leveraged neural network-based policies for decision-making, yet these models often lack interpretability, posing challenges for stakeholder comprehension and trust.","Concept bottleneck models offer an interpretable alternative by integrating human-understandable concepts into neural networks.","However, a significant limitation in prior work is the assumption that human annotations for these concepts are readily available during training, necessitating continuous real-time input from human annotators.","To overcome this limitation, we introduce a novel training scheme that enables RL algorithms to efficiently learn a concept-based policy by only querying humans to label a small set of data, or in the extreme case, without any human labels.","Our algorithm, LICORICE, involves three main contributions: interleaving concept learning and RL training, using a concept ensembles to actively select informative data points for labeling, and decorrelating the concept data with a simple strategy.","We show how LICORICE reduces manual labeling efforts to to 500 or fewer concept labels in three environments.","Finally, we present an initial study to explore how we can use powerful vision-language models to infer concepts from raw visual inputs without explicit labels at minimal cost to performance."],"url":"http://arxiv.org/abs/2407.15786v1"}
{"created":"2024-07-22 16:37:58","title":"Reconfigurable Intelligent Surface Empowered Full Duplex Systems: Opportunities and Challenges","abstract":"Reconfigurable intelligent surfaces (RISs) have emerged as a promising technology in wireless communications. Simultaneously transmitting and reflecting RIS (STAR-RISs) in particular have garnered significant attention due to their dual capabilities of simultaneous transmission and reflection, underscoring their potential applications in critical scenarios within the forthcoming sixth-generation (6G) technology landscape. Moreover, full-duplex (FD) systems have emerged as a breakthrough research direction in wireless transmission technology due to their high spectral efficiency. This paper explores the application potential of STAR-RIS in FD systems for future wireless communications, presenting an innovative technology that provides robust self-interference cancellation (SIC) capabilities for FD systems. We utilize the refraction functionality of STAR-RIS enhances the transmission capacity of FD systems, while its reflection functionality is used to eliminate self interference within the FD system. We delve into the applications of two different types of STAR-RIS in FD systems and compare their performance through simulations. Furthermore, we discuss the performance differences of STAR-RIS empowered FD systems under various configurations in a case study, and demonstrate the superiority of the proposed deep learning-based optimization algorithm. Finally, we discuss possible future research directions for STAR-RIS empowered FD systems.","sentences":["Reconfigurable intelligent surfaces (RISs) have emerged as a promising technology in wireless communications.","Simultaneously transmitting and reflecting RIS (STAR-RISs) in particular have garnered significant attention due to their dual capabilities of simultaneous transmission and reflection, underscoring their potential applications in critical scenarios within the forthcoming sixth-generation (6G) technology landscape.","Moreover, full-duplex (FD) systems have emerged as a breakthrough research direction in wireless transmission technology due to their high spectral efficiency.","This paper explores the application potential of STAR-RIS in FD systems for future wireless communications, presenting an innovative technology that provides robust self-interference cancellation (SIC) capabilities for FD systems.","We utilize the refraction functionality of STAR-RIS enhances the transmission capacity of FD systems, while its reflection functionality is used to eliminate self interference within the FD system.","We delve into the applications of two different types of STAR-RIS in FD systems and compare their performance through simulations.","Furthermore, we discuss the performance differences of STAR-RIS empowered FD systems under various configurations in a case study, and demonstrate the superiority of the proposed deep learning-based optimization algorithm.","Finally, we discuss possible future research directions for STAR-RIS empowered FD systems."],"url":"http://arxiv.org/abs/2407.15782v1"}
{"created":"2024-07-22 16:37:48","title":"Explaining Decisions in ML Models: a Parameterized Complexity Analysis","abstract":"This paper presents a comprehensive theoretical investigation into the parameterized complexity of explanation problems in various machine learning (ML) models. Contrary to the prevalent black-box perception, our study focuses on models with transparent internal mechanisms. We address two principal types of explanation problems: abductive and contrastive, both in their local and global variants. Our analysis encompasses diverse ML models, including Decision Trees, Decision Sets, Decision Lists, Ordered Binary Decision Diagrams, Random Forests, and Boolean Circuits, and ensembles thereof, each offering unique explanatory challenges. This research fills a significant gap in explainable AI (XAI) by providing a foundational understanding of the complexities of generating explanations for these models. This work provides insights vital for further research in the domain of XAI, contributing to the broader discourse on the necessity of transparency and accountability in AI systems.","sentences":["This paper presents a comprehensive theoretical investigation into the parameterized complexity of explanation problems in various machine learning (ML) models.","Contrary to the prevalent black-box perception, our study focuses on models with transparent internal mechanisms.","We address two principal types of explanation problems: abductive and contrastive, both in their local and global variants.","Our analysis encompasses diverse ML models, including Decision Trees, Decision Sets, Decision Lists, Ordered Binary Decision Diagrams, Random Forests, and Boolean Circuits, and ensembles thereof, each offering unique explanatory challenges.","This research fills a significant gap in explainable AI (XAI) by providing a foundational understanding of the complexities of generating explanations for these models.","This work provides insights vital for further research in the domain of XAI, contributing to the broader discourse on the necessity of transparency and accountability in AI systems."],"url":"http://arxiv.org/abs/2407.15780v1"}
{"created":"2024-07-22 16:31:18","title":"Analyzing the Impact of the Automatic Ball-Strike System in Professional Baseball: A Case Study on KBO League Data","abstract":"Recent advancements in professional baseball have led to the introduction of the Automated Ball-Strike (ABS) system, or ``robot umpires,'' which utilize machine learning, computer vision, and precise tracking technologies to automate ball-strike calls. The Korean Baseball Organization (KBO) league became the first professional baseball league to implement ABS during the 2024 season. This study analyzes pitching data for 2,515 games across multiple KBO seasons to compare decisions made by human umpires with those made by ABS, focusing specifically on differences within the ``gray zone'' of the strike zone. We propose and answer four research questions to examine the differences between human and robot umpires, player adaptation to ABS, assess the ABS system's fairness and consistency, and analyze its strategic implications for the game. Our findings offer valuable insights into the impact of technological integration in sports officiating, providing lessons relevant to future implementations in professional baseball and beyond.","sentences":["Recent advancements in professional baseball have led to the introduction of the Automated Ball-Strike (ABS) system, or ``robot umpires,'' which utilize machine learning, computer vision, and precise tracking technologies to automate ball-strike calls.","The Korean Baseball Organization (KBO) league became the first professional baseball league to implement ABS during the 2024 season.","This study analyzes pitching data for 2,515 games across multiple KBO seasons to compare decisions made by human umpires with those made by ABS, focusing specifically on differences within the ``gray zone'' of the strike zone.","We propose and answer four research questions to examine the differences between human and robot umpires, player adaptation to ABS, assess the ABS system's fairness and consistency, and analyze its strategic implications for the game.","Our findings offer valuable insights into the impact of technological integration in sports officiating, providing lessons relevant to future implementations in professional baseball and beyond."],"url":"http://arxiv.org/abs/2407.15779v1"}
{"created":"2024-07-22 16:25:41","title":"STAMP: Outlier-Aware Test-Time Adaptation with Stable Memory Replay","abstract":"Test-time adaptation (TTA) aims to address the distribution shift between the training and test data with only unlabeled data at test time. Existing TTA methods often focus on improving recognition performance specifically for test data associated with classes in the training set. However, during the open-world inference process, there are inevitably test data instances from unknown classes, commonly referred to as outliers. This paper pays attention to the problem that conducts both sample recognition and outlier rejection during inference while outliers exist. To address this problem, we propose a new approach called STAble Memory rePlay (STAMP), which performs optimization over a stable memory bank instead of the risky mini-batch. In particular, the memory bank is dynamically updated by selecting low-entropy and label-consistent samples in a class-balanced manner. In addition, we develop a self-weighted entropy minimization strategy that assigns higher weight to low-entropy samples. Extensive results demonstrate that STAMP outperforms existing TTA methods in terms of both recognition and outlier detection performance. The code is released at https://github.com/yuyongcan/STAMP.","sentences":["Test-time adaptation (TTA) aims to address the distribution shift between the training and test data with only unlabeled data at test time.","Existing TTA methods often focus on improving recognition performance specifically for test data associated with classes in the training set.","However, during the open-world inference process, there are inevitably test data instances from unknown classes, commonly referred to as outliers.","This paper pays attention to the problem that conducts both sample recognition and outlier rejection during inference while outliers exist.","To address this problem, we propose a new approach called STAble Memory rePlay (STAMP), which performs optimization over a stable memory bank instead of the risky mini-batch.","In particular, the memory bank is dynamically updated by selecting low-entropy and label-consistent samples in a class-balanced manner.","In addition, we develop a self-weighted entropy minimization strategy that assigns higher weight to low-entropy samples.","Extensive results demonstrate that STAMP outperforms existing TTA methods in terms of both recognition and outlier detection performance.","The code is released at https://github.com/yuyongcan/STAMP."],"url":"http://arxiv.org/abs/2407.15773v1"}
{"created":"2024-07-22 16:22:28","title":"Local Occupancy-Enhanced Object Grasping with Multiple Triplanar Projection","abstract":"This paper addresses the challenge of robotic grasping of general objects. Similar to prior research, the task reads a single-view 3D observation (i.e., point clouds) captured by a depth camera as input. Crucially, the success of object grasping highly demands a comprehensive understanding of the shape of objects within the scene. However, single-view observations often suffer from occlusions (including both self and inter-object occlusions), which lead to gaps in the point clouds, especially in complex cluttered scenes. This renders incomplete perception of the object shape and frequently causes failures or inaccurate pose estimation during object grasping. In this paper, we tackle this issue with an effective albeit simple solution, namely completing grasping-related scene regions through local occupancy prediction. Following prior practice, the proposed model first runs by proposing a number of most likely grasp points in the scene. Around each grasp point, a module is designed to infer any voxel in its neighborhood to be either void or occupied by some object. Importantly, the occupancy map is inferred by fusing both local and global cues. We implement a multi-group tri-plane scheme for efficiently aggregating long-distance contextual information. The model further estimates 6-DoF grasp poses utilizing the local occupancy-enhanced object shape information and returns the top-ranked grasp proposal. Comprehensive experiments on both the large-scale GraspNet-1Billion benchmark and real robotic arm demonstrate that the proposed method can effectively complete the unobserved parts in cluttered and occluded scenes. Benefiting from the occupancy-enhanced feature, our model clearly outstrips other competing methods under various performance metrics such as grasping average precision.","sentences":["This paper addresses the challenge of robotic grasping of general objects.","Similar to prior research, the task reads a single-view 3D observation (i.e., point clouds) captured by a depth camera as input.","Crucially, the success of object grasping highly demands a comprehensive understanding of the shape of objects within the scene.","However, single-view observations often suffer from occlusions (including both self and inter-object occlusions), which lead to gaps in the point clouds, especially in complex cluttered scenes.","This renders incomplete perception of the object shape and frequently causes failures or inaccurate pose estimation during object grasping.","In this paper, we tackle this issue with an effective albeit simple solution, namely completing grasping-related scene regions through local occupancy prediction.","Following prior practice, the proposed model first runs by proposing a number of most likely grasp points in the scene.","Around each grasp point, a module is designed to infer any voxel in its neighborhood to be either void or occupied by some object.","Importantly, the occupancy map is inferred by fusing both local and global cues.","We implement a multi-group tri-plane scheme for efficiently aggregating long-distance contextual information.","The model further estimates 6-DoF grasp poses utilizing the local occupancy-enhanced object shape information and returns the top-ranked grasp proposal.","Comprehensive experiments on both the large-scale GraspNet-1Billion benchmark and real robotic arm demonstrate that the proposed method can effectively complete the unobserved parts in cluttered and occluded scenes.","Benefiting from the occupancy-enhanced feature, our model clearly outstrips other competing methods under various performance metrics such as grasping average precision."],"url":"http://arxiv.org/abs/2407.15771v1"}
{"created":"2024-07-22 16:20:30","title":"Examining Inequality in Park Quality for Promoting Health Across 35 Global Cities","abstract":"Urban parks provide significant health benefits by offering spaces and facilities for various recreational and leisure activities. However, the capacity of specific park spaces and elements to foster health remains underexamined. Traditional studies have focused on parks' size, greenery, and accessibility, often overlooking their ability to facilitate specific health-promoting activities. To address this gap, we propose a taxonomy consisting of six categories of health-promoting activities in parks: physical, mind-body, nature appreciation, environmental, social, and cultural. We estimate the capacity of parks in 35 global cities to promote health by establishing a lexicon linking park spaces and elements with specific health-promoting activities from our taxonomy. Using this lexicon, we collected data on elements and spaces in all parks in 35 cities from OpenStreetMap. Our analysis covers 23,477 parks with a total of 827,038 elements and spaces. By first comparing similarly sized parks across cities, we found that North American parks offer more spaces for physical activities, while European parks focus more on nature appreciation. Second, by scoring parks based on both elements and spaces, we investigated the variability in their health-promoting potential. We found the most uniform provision across parks for physical activities and the highest disparities regarding social activities. Additionally, parks offering a variety of activities are usually located in city centers, while offerings diminish in parks towards the suburbs. Lastly, we identified significant inequalities in park standards across cities, regardless of their continental location: Tokyo and Paris offer the most uniform park standards, while Copenhagen and Rio de Janeiro exhibit the most pronounced disparities. Our study provides insights for making urban parks more equitable, engaging, and health-promoting.","sentences":["Urban parks provide significant health benefits by offering spaces and facilities for various recreational and leisure activities.","However, the capacity of specific park spaces and elements to foster health remains underexamined.","Traditional studies have focused on parks' size, greenery, and accessibility, often overlooking their ability to facilitate specific health-promoting activities.","To address this gap, we propose a taxonomy consisting of six categories of health-promoting activities in parks: physical, mind-body, nature appreciation, environmental, social, and cultural.","We estimate the capacity of parks in 35 global cities to promote health by establishing a lexicon linking park spaces and elements with specific health-promoting activities from our taxonomy.","Using this lexicon, we collected data on elements and spaces in all parks in 35 cities from OpenStreetMap.","Our analysis covers 23,477 parks with a total of 827,038 elements and spaces.","By first comparing similarly sized parks across cities, we found that North American parks offer more spaces for physical activities, while European parks focus more on nature appreciation.","Second, by scoring parks based on both elements and spaces, we investigated the variability in their health-promoting potential.","We found the most uniform provision across parks for physical activities and the highest disparities regarding social activities.","Additionally, parks offering a variety of activities are usually located in city centers, while offerings diminish in parks towards the suburbs.","Lastly, we identified significant inequalities in park standards across cities, regardless of their continental location:","Tokyo and Paris offer the most uniform park standards, while Copenhagen and Rio de Janeiro exhibit the most pronounced disparities.","Our study provides insights for making urban parks more equitable, engaging, and health-promoting."],"url":"http://arxiv.org/abs/2407.15770v1"}
{"created":"2024-07-22 16:16:38","title":"Towards Open-World Object-based Anomaly Detection via Self-Supervised Outlier Synthesis","abstract":"Object detection is a pivotal task in computer vision that has received significant attention in previous years. Nonetheless, the capability of a detector to localise objects out of the training distribution remains unexplored. Whilst recent approaches in object-level out-of-distribution (OoD) detection heavily rely on class labels, such approaches contradict truly open-world scenarios where the class distribution is often unknown. In this context, anomaly detection focuses on detecting unseen instances rather than classifying detections as OoD. This work aims to bridge this gap by leveraging an open-world object detector and an OoD detector via virtual outlier synthesis. This is achieved by using the detector backbone features to first learn object pseudo-classes via self-supervision. These pseudo-classes serve as the basis for class-conditional virtual outlier sampling of anomalous features that are classified by an OoD head. Our approach empowers our overall object detector architecture to learn anomaly-aware feature representations without relying on class labels, hence enabling truly open-world object anomaly detection. Empirical validation of our approach demonstrates its effectiveness across diverse datasets encompassing various imaging modalities (visible, infrared, and X-ray). Moreover, our method establishes state-of-the-art performance on object-level anomaly detection, achieving an average recall score improvement of over 5.4% for natural images and 23.5% for a security X-ray dataset compared to the current approaches. In addition, our method detects anomalies in datasets where current approaches fail. Code available at https://github.com/KostadinovShalon/oln-ssos.","sentences":["Object detection is a pivotal task in computer vision that has received significant attention in previous years.","Nonetheless, the capability of a detector to localise objects out of the training distribution remains unexplored.","Whilst recent approaches in object-level out-of-distribution (OoD) detection heavily rely on class labels, such approaches contradict truly open-world scenarios where the class distribution is often unknown.","In this context, anomaly detection focuses on detecting unseen instances rather than classifying detections as OoD. This work aims to bridge this gap by leveraging an open-world object detector and an OoD detector via virtual outlier synthesis.","This is achieved by using the detector backbone features to first learn object pseudo-classes via self-supervision.","These pseudo-classes serve as the basis for class-conditional virtual outlier sampling of anomalous features that are classified by an OoD head.","Our approach empowers our overall object detector architecture to learn anomaly-aware feature representations without relying on class labels, hence enabling truly open-world object anomaly detection.","Empirical validation of our approach demonstrates its effectiveness across diverse datasets encompassing various imaging modalities (visible, infrared, and X-ray).","Moreover, our method establishes state-of-the-art performance on object-level anomaly detection, achieving an average recall score improvement of over 5.4% for natural images and 23.5% for a security X-ray dataset compared to the current approaches.","In addition, our method detects anomalies in datasets where current approaches fail.","Code available at https://github.com/KostadinovShalon/oln-ssos."],"url":"http://arxiv.org/abs/2407.15763v1"}
{"created":"2024-07-22 16:13:38","title":"Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning","abstract":"Reward-based finetuning is crucial for aligning language policies with intended behaviors (e.g., creativity and safety). A key challenge here is to develop steerable language models that trade-off multiple (conflicting) objectives in a flexible and efficient manner. This paper presents Conditioned Language Policy (CLP), a general framework for finetuning language models on multiple objectives. Building on techniques from multi-task training and parameter-efficient finetuning, CLP can learn steerable models that effectively trade-off conflicting objectives at inference time. Notably, this does not require training or maintaining multiple models to achieve different trade-offs between the objectives. Through an extensive set of experiments and ablations, we show that the CLP framework learns steerable models that outperform and Pareto-dominate the current state-of-the-art approaches for multi-objective finetuning.","sentences":["Reward-based finetuning is crucial for aligning language policies with intended behaviors (e.g., creativity and safety).","A key challenge here is to develop steerable language models that trade-off multiple (conflicting) objectives in a flexible and efficient manner.","This paper presents Conditioned Language Policy (CLP), a general framework for finetuning language models on multiple objectives.","Building on techniques from multi-task training and parameter-efficient finetuning, CLP can learn steerable models that effectively trade-off conflicting objectives at inference time.","Notably, this does not require training or maintaining multiple models to achieve different trade-offs between the objectives.","Through an extensive set of experiments and ablations, we show that the CLP framework learns steerable models that outperform and Pareto-dominate the current state-of-the-art approaches for multi-objective finetuning."],"url":"http://arxiv.org/abs/2407.15762v1"}
{"created":"2024-07-22 16:06:51","title":"Model editing for distribution shifts in uranium oxide morphological analysis","abstract":"Deep learning still struggles with certain kinds of scientific data. Notably, pretraining data may not provide coverage of relevant distribution shifts (e.g., shifts induced via the use of different measurement instruments). We consider deep learning models trained to classify the synthesis conditions of uranium ore concentrates (UOCs) and show that model editing is particularly effective for improving generalization to distribution shifts common in this domain. In particular, model editing outperforms finetuning on two curated datasets comprising of micrographs taken of U$_{3}$O$_{8}$ aged in humidity chambers and micrographs acquired with different scanning electron microscopes, respectively.","sentences":["Deep learning still struggles with certain kinds of scientific data.","Notably, pretraining data may not provide coverage of relevant distribution shifts (e.g., shifts induced via the use of different measurement instruments).","We consider deep learning models trained to classify the synthesis conditions of uranium ore concentrates (UOCs) and show that model editing is particularly effective for improving generalization to distribution shifts common in this domain.","In particular, model editing outperforms finetuning on two curated datasets comprising of micrographs taken of U$_{3}$O$_{8}$ aged in humidity chambers and micrographs acquired with different scanning electron microscopes, respectively."],"url":"http://arxiv.org/abs/2407.15756v1"}
{"created":"2024-07-22 16:00:55","title":"LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding","abstract":"Large multimodal models (LMMs) are processing increasingly longer and richer inputs. Albeit the progress, few public benchmark is available to measure such development. To mitigate this gap, we introduce LongVideoBench, a question-answering benchmark that features video-language interleaved inputs up to an hour long. Our benchmark includes 3,763 varying-length web-collected videos with their subtitles across diverse themes, designed to comprehensively evaluate LMMs on long-term multimodal understanding. To achieve this, we interpret the primary challenge as to accurately retrieve and reason over detailed multimodal information from long inputs. As such, we formulate a novel video question-answering task termed referring reasoning. Specifically, as part of the question, it contains a referring query that references related video contexts, called referred context. The model is then required to reason over relevant video details from the referred context. Following the paradigm of referring reasoning, we curate 6,678 human-annotated multiple-choice questions in 17 fine-grained categories, establishing one of the most comprehensive benchmarks for long-form video understanding. Evaluations suggest that the LongVideoBench presents significant challenges even for the most advanced proprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their open-source counterparts show an even larger performance gap. In addition, our results indicate that model performance on the benchmark improves only when they are capable of processing more frames, positioning LongVideoBench as a valuable benchmark for evaluating future-generation long-context LMMs.","sentences":["Large multimodal models (LMMs) are processing increasingly longer and richer inputs.","Albeit the progress, few public benchmark is available to measure such development.","To mitigate this gap, we introduce LongVideoBench, a question-answering benchmark that features video-language interleaved inputs up to an hour long.","Our benchmark includes 3,763 varying-length web-collected videos with their subtitles across diverse themes, designed to comprehensively evaluate LMMs on long-term multimodal understanding.","To achieve this, we interpret the primary challenge as to accurately retrieve and reason over detailed multimodal information from long inputs.","As such, we formulate a novel video question-answering task termed referring reasoning.","Specifically, as part of the question, it contains a referring query that references related video contexts, called referred context.","The model is then required to reason over relevant video details from the referred context.","Following the paradigm of referring reasoning, we curate 6,678 human-annotated multiple-choice questions in 17 fine-grained categories, establishing one of the most comprehensive benchmarks for long-form video understanding.","Evaluations suggest that the LongVideoBench presents significant challenges even for the most advanced proprietary models (e.g. GPT-4o, Gemini-1.5-Pro, GPT-4-Turbo), while their open-source counterparts show an even larger performance gap.","In addition, our results indicate that model performance on the benchmark improves only when they are capable of processing more frames, positioning LongVideoBench as a valuable benchmark for evaluating future-generation long-context LMMs."],"url":"http://arxiv.org/abs/2407.15754v1"}
{"created":"2024-07-22 15:58:43","title":"Broad and Spectral-Efficient Beamforming for the Uni-polarized Reconfigurable Intelligent Surfaces","abstract":"A reconfigurable intelligent surface (RIS) is composed of low-cost elements that manipulate the propagation environment from a transmitter by intelligently applying phase shifts to incoming signals before they are reflected. This paper explores a uni-polarized RIS with linear shape aimed at transmitting a common signal to multiple user equipments (UEs) spread across a wide angular region. To achieve uniform coverage, the uni-polarized RIS is designed to emit a broad and spectral-efficient beam featuring a spatially flat-like array factor, diverging from the conventional narrow beam approach. To achieve this objective, we start by deriving probabilistic lower and upper bounds for the average spectral efficiency (SE) delivered to the UEs. Leveraging the insights from the lower bound, we focus on optimizing the minimum value of the power domain array factor (PDAF) across a range of azimuth angles from \\(-\\frac{\\pi}{2}\\) to \\(\\frac{\\pi}{2}\\). We employ the continuous genetic algorithm (CGA) for this optimization task, aiming to improve the SE delivered to the UEs while also creating a wide beam. Extensive simulation experiments are carried out to assess the performance of the proposed code, focusing on key metrics such as the minimum and average values of the PDAF and the SE delivered to the UEs. Our findings demonstrate that the proposed code enhances the minimum SE delivered to the UEs while maintaining the desired attribute of a broad beam. This performance is notably superior to that of established codes, including the Barker, Frank, and Chu codes.","sentences":["A reconfigurable intelligent surface (RIS) is composed of low-cost elements that manipulate the propagation environment from a transmitter by intelligently applying phase shifts to incoming signals before they are reflected.","This paper explores a uni-polarized RIS with linear shape aimed at transmitting a common signal to multiple user equipments (UEs) spread across a wide angular region.","To achieve uniform coverage, the uni-polarized RIS is designed to emit a broad and spectral-efficient beam featuring a spatially flat-like array factor, diverging from the conventional narrow beam approach.","To achieve this objective, we start by deriving probabilistic lower and upper bounds for the average spectral efficiency (SE) delivered to the UEs.","Leveraging the insights from the lower bound, we focus on optimizing the minimum value of the power domain array factor (PDAF) across a range of azimuth angles from \\(-\\frac{\\pi}{2}\\) to \\(\\frac{\\pi}{2}\\).","We employ the continuous genetic algorithm (CGA) for this optimization task, aiming to improve the SE delivered to the UEs while also creating a wide beam.","Extensive simulation experiments are carried out to assess the performance of the proposed code, focusing on key metrics such as the minimum and average values of the PDAF and the SE delivered to the UEs.","Our findings demonstrate that the proposed code enhances the minimum SE delivered to the UEs while maintaining the desired attribute of a broad beam.","This performance is notably superior to that of established codes, including the Barker, Frank, and Chu codes."],"url":"http://arxiv.org/abs/2407.15752v1"}
{"created":"2024-07-22 15:53:27","title":"MoRSE: Bridging the Gap in Cybersecurity Expertise with Retrieval Augmented Generation","abstract":"In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the first specialised AI chatbot for cybersecurity. MoRSE aims to provide comprehensive and complete knowledge about cybersecurity. MoRSE uses two RAG (Retrieval Augmented Generation) systems designed to retrieve and organize information from multidimensional cybersecurity contexts. MoRSE differs from traditional RAGs by using parallel retrievers that work together to retrieve semantically related information in different formats and structures. Unlike traditional Large Language Models (LLMs) that rely on Parametric Knowledge Bases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases in response to user queries. Subsequently, MoRSE uses this information to generate accurate answers. In addition, MoRSE benefits from real-time updates to its knowledge bases, enabling continuous knowledge enrichment without retraining. We have evaluated the effectiveness of MoRSE against other state-of-the-art LLMs, evaluating the system on 600 cybersecurity specific questions. The experimental evaluation has shown that the improvement in terms of relevance and correctness of the answer is more than 10\\% compared to known solutions such as GPT-4 and Mixtral 7x8.","sentences":["In this paper, we introduce MoRSE (Mixture of RAGs Security Experts), the first specialised AI chatbot for cybersecurity.","MoRSE aims to provide comprehensive and complete knowledge about cybersecurity.","MoRSE uses two RAG (Retrieval Augmented Generation) systems designed to retrieve and organize information from multidimensional cybersecurity contexts.","MoRSE differs from traditional RAGs by using parallel retrievers that work together to retrieve semantically related information in different formats and structures.","Unlike traditional Large Language Models (LLMs) that rely on Parametric Knowledge Bases, MoRSE retrieves relevant documents from Non-Parametric Knowledge Bases in response to user queries.","Subsequently, MoRSE uses this information to generate accurate answers.","In addition, MoRSE benefits from real-time updates to its knowledge bases, enabling continuous knowledge enrichment without retraining.","We have evaluated the effectiveness of MoRSE against other state-of-the-art LLMs, evaluating the system on 600 cybersecurity specific questions.","The experimental evaluation has shown that the improvement in terms of relevance and correctness of the answer is more than 10\\% compared to known solutions such as GPT-4 and Mixtral 7x8."],"url":"http://arxiv.org/abs/2407.15748v1"}
{"created":"2024-07-22 15:42:59","title":"Enhanced Achievable DoF Bounds for Cache-Aided MIMO Communication Systems","abstract":"Integrating coded caching (CC) into multiple-input multiple-output (MIMO) communications may significantly enhance the achievable degrees of freedom (DoF) of the wireless networks. In this paper, we consider a cache-aided MIMO configuration with a CC gain $t$, where a server with $L$ Tx antennas communicates with $K$ users, each with $G$ Rx antennas. In the proposed content-aware MIMO strategy, we carefully adjust the number of users $\\Omega$ and the number of parallel streams decoded by each user $\\beta$ served in each transmission to maximize the DoF. As a result, we achieve a DoF of ${\\max_{\\beta, \\Omega }}{\\Omega \\beta}$, where ${\\beta \\le \\mathrm{min}\\big(G,\\frac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t-1)\\binom{\\Omega-1}{t}}\\big)}$. To prove the achievability of the proposed DoF bound, we provide a novel transmission strategy based on the simultaneous unicasting of multiple data streams. In this strategy, the missing data packets are scheduled such that the number of parallel streams per transmission is maximized while the decodability of all useful terms by each target user is guaranteed. Numerical simulations validate the findings, confirming the enhanced DoF and improved performance of the proposed design.","sentences":["Integrating coded caching (CC) into multiple-input multiple-output (MIMO) communications may significantly enhance the achievable degrees of freedom (DoF) of the wireless networks.","In this paper, we consider a cache-aided MIMO configuration with a CC gain $t$, where a server with $L$ Tx antennas communicates with $K$ users, each with $G$ Rx antennas.","In the proposed content-aware MIMO strategy, we carefully adjust the number of users $\\Omega$ and the number of parallel streams decoded by each user $\\beta$ served in each transmission to maximize the DoF. As a result, we achieve a DoF of ${\\max_{\\beta, \\Omega }}{\\Omega \\beta}$, where ${\\beta \\le \\mathrm{min}\\big(G,\\frac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t-1)\\binom{\\Omega-1}{t}}\\big)}$.","To prove the achievability of the proposed DoF bound, we provide a novel transmission strategy based on the simultaneous unicasting of multiple data streams.","In this strategy, the missing data packets are scheduled such that the number of parallel streams per transmission is maximized while the decodability of all useful terms by each target user is guaranteed.","Numerical simulations validate the findings, confirming the enhanced DoF and improved performance of the proposed design."],"url":"http://arxiv.org/abs/2407.15743v1"}
{"created":"2024-07-22 15:42:06","title":"The syzygy distinguisher","abstract":"We present a new distinguisher for alternant and Goppa codes, whose complexity is subexponential in the code length. It does not suffer from the strong regime limitations of the previous distinguishers or structure recovery algorithms: in particular, it applies to the codes used in the Classic McEliece candidate for postquantum cryptography standardization. The invariants that allow us to distinguish are graded Betti numbers of the homogeneous coordinate ring of a shortening of the dual code.   Since its introduction in 1978, this is the first time an analysis of the McEliece cryptosystem breaks the exponential barrier.","sentences":["We present a new distinguisher for alternant and Goppa codes, whose complexity is subexponential in the code length.","It does not suffer from the strong regime limitations of the previous distinguishers or structure recovery algorithms: in particular, it applies to the codes used in the Classic McEliece candidate for postquantum cryptography standardization.","The invariants that allow us to distinguish are graded Betti numbers of the homogeneous coordinate ring of a shortening of the dual code.   ","Since its introduction in 1978, this is the first time an analysis of the McEliece cryptosystem breaks the exponential barrier."],"url":"http://arxiv.org/abs/2407.15740v1"}
{"created":"2024-07-22 15:41:37","title":"Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond","abstract":"In recent years, research on out-of-distribution (OoD) detection for semantic segmentation has mainly focused on road scenes -- a domain with a constrained amount of semantic diversity. In this work, we challenge this constraint and extend the domain of this task to general natural images. To this end, we introduce: 1. the ADE-OoD benchmark, which is based on the ADE20k dataset and includes images from diverse domains with a high semantic diversity, and 2. a novel approach that uses Diffusion score matching for OoD detection (DOoD) and is robust to the increased semantic diversity. ADE-OoD features indoor and outdoor images, defines 150 semantic categories as in-distribution, and contains a variety of OoD objects. For DOoD, we train a diffusion model with an MLP architecture on semantic in-distribution embeddings and build on the score matching interpretation to compute pixel-wise OoD scores at inference time. On common road scene OoD benchmarks, DOoD performs on par or better than the state of the art, without using outliers for training or making assumptions about the data domain. On ADE-OoD, DOoD outperforms previous approaches, but leaves much room for future improvements.","sentences":["In recent years, research on out-of-distribution (OoD) detection for semantic segmentation has mainly focused on road scenes -- a domain with a constrained amount of semantic diversity.","In this work, we challenge this constraint and extend the domain of this task to general natural images.","To this end, we introduce:","1. the ADE-OoD benchmark, which is based on the ADE20k dataset and includes images from diverse domains with a high semantic diversity, and 2.","a novel approach that uses Diffusion score matching for OoD detection (DOoD) and is robust to the increased semantic diversity.","ADE-OoD features indoor and outdoor images, defines 150 semantic categories as in-distribution, and contains a variety of OoD objects.","For DOoD, we train a diffusion model with an MLP architecture on semantic in-distribution embeddings and build on the score matching interpretation to compute pixel-wise OoD scores at inference time.","On common road scene OoD benchmarks, DOoD performs on par or better than the state of the art, without using outliers for training or making assumptions about the data domain.","On ADE-OoD, DOoD outperforms previous approaches, but leaves much room for future improvements."],"url":"http://arxiv.org/abs/2407.15739v1"}
{"created":"2024-07-22 15:41:23","title":"Parallel Split Learning with Global Sampling","abstract":"The expansion of IoT devices and the demands of Deep Learning have highlighted significant challenges in Distributed Deep Learning (DDL) systems. Parallel Split Learning (PSL) has emerged as a promising derivative of Split Learning that is well suited for distributed learning on resource-constrained devices. However, PSL faces several obstacles, such as large effective batch sizes, non-IID data distributions, and the straggler effect. We view these issues as a sampling dilemma and propose to address them by orchestrating the mini-batch sampling process on the server side. We introduce the Uniform Global Sampling (UGS) method to decouple the effective batch size from the number of clients and reduce mini-batch deviation in non-IID settings. To address the straggler effect, we introduce the Latent Dirichlet Sampling (LDS) method, which generalizes UGS to balance the trade-off between batch deviation and training time. Our simulations reveal that our proposed methods enhance model accuracy by up to 34.1% in non-IID settings and reduce the training time in the presence of stragglers by up to 62%. In particular, LDS effectively mitigates the straggler effect without compromising model accuracy or adding significant computational overhead compared to UGS. Our results demonstrate the potential of our methods as a promising solution for DDL in real applications.","sentences":["The expansion of IoT devices and the demands of Deep Learning have highlighted significant challenges in Distributed Deep Learning (DDL) systems.","Parallel Split Learning (PSL) has emerged as a promising derivative of Split Learning that is well suited for distributed learning on resource-constrained devices.","However, PSL faces several obstacles, such as large effective batch sizes, non-IID data distributions, and the straggler effect.","We view these issues as a sampling dilemma and propose to address them by orchestrating the mini-batch sampling process on the server side.","We introduce the Uniform Global Sampling (UGS) method to decouple the effective batch size from the number of clients and reduce mini-batch deviation in non-IID settings.","To address the straggler effect, we introduce the Latent Dirichlet Sampling (LDS) method, which generalizes UGS to balance the trade-off between batch deviation and training time.","Our simulations reveal that our proposed methods enhance model accuracy by up to 34.1% in non-IID settings and reduce the training time in the presence of stragglers by up to 62%.","In particular, LDS effectively mitigates the straggler effect without compromising model accuracy or adding significant computational overhead compared to UGS.","Our results demonstrate the potential of our methods as a promising solution for DDL in real applications."],"url":"http://arxiv.org/abs/2407.15738v1"}
{"created":"2024-07-22 15:41:09","title":"Scheduling on a Stochastic Number of Machines","abstract":"We consider a new scheduling problem on parallel identical machines in which the number of machines is initially not known, but it follows a given probability distribution. Only after all jobs are assigned to a given number of bags, the actual number of machines is revealed. Subsequently, the jobs need to be assigned to the machines without splitting the bags. This is the stochastic version of a related problem introduced by Stein and Zhong [SODA 2018, TALG 2020] and it is, for example, motivated by bundling jobs that need to be scheduled by data centers. We present two PTASs for the stochastic setting, computing job-to-bag assignments that (i) minimize the expected maximum machine load and (ii) maximize the expected minimum machine load (like in the Santa Claus problem), respectively. The former result follows by careful enumeration combined with known PTASs. For the latter result, we introduce an intricate dynamic program that we apply to a suitably rounded instance.","sentences":["We consider a new scheduling problem on parallel identical machines in which the number of machines is initially not known, but it follows a given probability distribution.","Only after all jobs are assigned to a given number of bags, the actual number of machines is revealed.","Subsequently, the jobs need to be assigned to the machines without splitting the bags.","This is the stochastic version of a related problem introduced by Stein and Zhong","[SODA 2018, TALG 2020] and it is, for example, motivated by bundling jobs that need to be scheduled by data centers.","We present two PTASs for the stochastic setting, computing job-to-bag assignments that (i) minimize the expected maximum machine load and (ii) maximize the expected minimum machine load (like in the Santa Claus problem), respectively.","The former result follows by careful enumeration combined with known PTASs.","For the latter result, we introduce an intricate dynamic program that we apply to a suitably rounded instance."],"url":"http://arxiv.org/abs/2407.15737v1"}
{"created":"2024-07-22 15:40:17","title":"OMoS-QA: A Dataset for Cross-Lingual Extractive Question Answering in a German Migration Context","abstract":"When immigrating to a new country, it is easy to feel overwhelmed by the need to obtain information on financial support, housing, schooling, language courses, and other issues. If relocation is rushed or even forced, the necessity for high-quality answers to such questions is all the more urgent. Official immigration counselors are usually overbooked, and online systems could guide newcomers to the requested information or a suitable counseling service.   To this end, we present OMoS-QA, a dataset of German and English questions paired with relevant trustworthy documents and manually annotated answers, specifically tailored to this scenario. Questions are automatically generated with an open-source large language model (LLM) and answer sentences are selected by crowd workers with high agreement. With our data, we conduct a comparison of 5 pretrained LLMs on the task of extractive question answering (QA) in German and English. Across all models and both languages, we find high precision and low-to-mid recall in selecting answer sentences, which is a favorable trade-off to avoid misleading users. This performance even holds up when the question language does not match the document language. When it comes to identifying unanswerable questions given a context, there are larger differences between the two languages.","sentences":["When immigrating to a new country, it is easy to feel overwhelmed by the need to obtain information on financial support, housing, schooling, language courses, and other issues.","If relocation is rushed or even forced, the necessity for high-quality answers to such questions is all the more urgent.","Official immigration counselors are usually overbooked, and online systems could guide newcomers to the requested information or a suitable counseling service.   ","To this end, we present OMoS-QA, a dataset of German and English questions paired with relevant trustworthy documents and manually annotated answers, specifically tailored to this scenario.","Questions are automatically generated with an open-source large language model (LLM) and answer sentences are selected by crowd workers with high agreement.","With our data, we conduct a comparison of 5 pretrained LLMs on the task of extractive question answering (QA) in German and English.","Across all models and both languages, we find high precision and low-to-mid recall in selecting answer sentences, which is a favorable trade-off to avoid misleading users.","This performance even holds up when the question language does not match the document language.","When it comes to identifying unanswerable questions given a context, there are larger differences between the two languages."],"url":"http://arxiv.org/abs/2407.15736v1"}
{"created":"2024-07-22 15:37:41","title":"TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON","abstract":"TaskGen is an open-sourced agentic framework which uses an Agent to solve an arbitrary task by breaking them down into subtasks. Each subtask is mapped to an Equipped Function or another Agent to execute. In order to reduce verbosity (and hence token usage), TaskGen uses StrictJSON that ensures JSON output from the Large Language Model (LLM), along with additional features such as type checking and iterative error correction. Key to the philosophy of TaskGen is the management of information/memory on a need-to-know basis. We empirically evaluate TaskGen on various environments such as 40x40 dynamic maze navigation with changing obstacle locations (100% solve rate), TextWorld escape room solving with dense rewards and detailed goals (96% solve rate), web browsing (69% of actions successful), solving the MATH dataset (71% solve rate over 100 Level-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset (F1 score of 47.03%)","sentences":["TaskGen is an open-sourced agentic framework which uses an Agent to solve an arbitrary task by breaking them down into subtasks.","Each subtask is mapped to an Equipped Function or another Agent to execute.","In order to reduce verbosity (and hence token usage), TaskGen uses StrictJSON that ensures JSON output from the Large Language Model (LLM), along with additional features such as type checking and iterative error correction.","Key to the philosophy of TaskGen is the management of information/memory on a need-to-know basis.","We empirically evaluate TaskGen on various environments such as 40x40 dynamic maze navigation with changing obstacle locations (100% solve rate), TextWorld escape room solving with dense rewards and detailed goals (96% solve rate), web browsing (69% of actions successful), solving the MATH dataset (71% solve rate over 100 Level-5 problems), Retrieval Augmented Generation on NaturalQuestions dataset (F1 score of 47.03%)"],"url":"http://arxiv.org/abs/2407.15734v1"}
{"created":"2024-07-22 15:35:09","title":"Zero-Shot Embeddings Inform Learning and Forgetting with Vision-Language Encoders","abstract":"Despite the proliferation of large vision-language foundation models, estimation of the learning and forgetting outcomes following fine-tuning of these models remains largely unexplored. Inspired by work highlighting the significance of the modality gap in contrastive dual-encoders, we propose the Inter-Intra Modal Measure (IIMM). Combining terms quantifying the similarity between image embeddings and the similarity between incorrect image and label embedding pairs, the IIMM functions as a strong predictor of performance changes with fine-tuning. Our extensive empirical analysis across four state-of-the-art vision-language models (CLIP, SigLIP, CoCa, EVA-02-CLIP) and five fine-tuning techniques (full fine-tuning, BitFit, attention-weight tuning, LoRA, CLIP-Adapter) demonstrates a strong, statistically significant linear relationship: fine-tuning on tasks with higher IIMM scores produces greater in-domain performance gains but also induces more severe out-of-domain performance degradation, with some parameter-efficient fine-tuning (PEFT) methods showing extreme forgetting. We compare our measure against transfer scores from state-of-the-art model selection methods and show that the IIMM is significantly more predictive of accuracy gains. With only a single forward pass of the target data, practitioners can leverage this key insight to heuristically evaluate the degree to which a model can be expected to improve following fine-tuning. Given additional knowledge about the model's performance on a few diverse tasks, this heuristic further evolves into a strong predictor of expected performance changes when training for new tasks.","sentences":["Despite the proliferation of large vision-language foundation models, estimation of the learning and forgetting outcomes following fine-tuning of these models remains largely unexplored.","Inspired by work highlighting the significance of the modality gap in contrastive dual-encoders, we propose the Inter-Intra Modal Measure (IIMM).","Combining terms quantifying the similarity between image embeddings and the similarity between incorrect image and label embedding pairs, the IIMM functions as a strong predictor of performance changes with fine-tuning.","Our extensive empirical analysis across four state-of-the-art vision-language models (CLIP, SigLIP, CoCa, EVA-02-CLIP) and five fine-tuning techniques (full fine-tuning, BitFit, attention-weight tuning, LoRA, CLIP-Adapter) demonstrates a strong, statistically significant linear relationship: fine-tuning on tasks with higher IIMM scores produces greater in-domain performance gains but also induces more severe out-of-domain performance degradation, with some parameter-efficient fine-tuning (PEFT) methods showing extreme forgetting.","We compare our measure against transfer scores from state-of-the-art model selection methods and show that the IIMM is significantly more predictive of accuracy gains.","With only a single forward pass of the target data, practitioners can leverage this key insight to heuristically evaluate the degree to which a model can be expected to improve following fine-tuning.","Given additional knowledge about the model's performance on a few diverse tasks, this heuristic further evolves into a strong predictor of expected performance changes when training for new tasks."],"url":"http://arxiv.org/abs/2407.15731v1"}
{"created":"2024-07-22 15:33:36","title":"Self-Sustainable Metasurface-Assisted mmWave Indoor Communication System","abstract":"In the design of a metasurface-assisted system for indoor environments, it is essential to take into account not only the performance gains and coverage extension provided by the metasurface but also the operating costs brought by its reconfigurability, such as powering and cabling. These costs can present challenges, particularly in indoor dense spaces (IDSs). A self-sustainable metasurface (SSM), which retains reconfigurability unlike a static metasurface (SMS), achieves a lower operating cost than a reconfigurable intelligent surface (RIS) by being self-sustainable through power harvesting. In this paper, in order to find a better trade-off between metasurface gain, coverage, and operating cost, the design and performance of an SSM-assisted indoor mmWave communication system are investigated. We first simplify the design of the SSM-assisted system by considering the use of SSMs in a preset-based manner and the formation of coverage groups by associating SSMs with the closest user equipments (UEs). We propose a two-stage iterative algorithm to maximize the minimum data rate in the system by jointly deciding the association between the UEs and the SSMs, the phase-shifts of the SSMs, and allocating time resources for each UE. The non-convexities that exist in the proposed optimization problem are tackled using the feasible point pursuit successive convex approximation method and the concave-convex procedure. To understand the best scenario for using SSM, the resulting performance is compared with that achieved with RIS and SMS. Our numerical results indicate that SSMs are best utilized in a small environment where self-sustainability is easier to achieve when the budget for operating costs is tight.","sentences":["In the design of a metasurface-assisted system for indoor environments, it is essential to take into account not only the performance gains and coverage extension provided by the metasurface but also the operating costs brought by its reconfigurability, such as powering and cabling.","These costs can present challenges, particularly in indoor dense spaces (IDSs).","A self-sustainable metasurface (SSM), which retains reconfigurability unlike a static metasurface (SMS), achieves a lower operating cost than a reconfigurable intelligent surface (RIS) by being self-sustainable through power harvesting.","In this paper, in order to find a better trade-off between metasurface gain, coverage, and operating cost, the design and performance of an SSM-assisted indoor mmWave communication system are investigated.","We first simplify the design of the SSM-assisted system by considering the use of SSMs in a preset-based manner and the formation of coverage groups by associating SSMs with the closest user equipments (UEs).","We propose a two-stage iterative algorithm to maximize the minimum data rate in the system by jointly deciding the association between the UEs and the SSMs, the phase-shifts of the SSMs, and allocating time resources for each UE.","The non-convexities that exist in the proposed optimization problem are tackled using the feasible point pursuit successive convex approximation method and the concave-convex procedure.","To understand the best scenario for using SSM, the resulting performance is compared with that achieved with RIS and SMS.","Our numerical results indicate that SSMs are best utilized in a small environment where self-sustainability is easier to achieve when the budget for operating costs is tight."],"url":"http://arxiv.org/abs/2407.15729v1"}
{"created":"2024-07-22 15:28:51","title":"Beyond Size and Class Balance: Alpha as a New Dataset Quality Metric for Deep Learning","abstract":"In deep learning, achieving high performance on image classification tasks requires diverse training sets. However, dataset diversity is incompletely understood. The current best practice is to try to maximize dataset size and class balance. Yet large, class-balanced datasets are not guaranteed to be diverse: images can still be arbitrarily similar. We hypothesized that, for a given model architecture, better model performance can be achieved by maximizing dataset diversity more directly. This could open a path for performance improvement without additional computational resources or architectural advances. To test this hypothesis, we introduce a comprehensive framework of diversity measures, developed in ecology, that generalizes familiar quantities like Shannon entropy by accounting for similarities and differences among images. (Dataset size and class balance emerge from this framework as special cases.) By analyzing thousands of subsets from seven medical datasets representing ultrasound, X-ray, CT, and pathology images, we found that the best correlates of performance were not size or class balance but $A$ -- ``big alpha'' -- a set of generalized entropy measures interpreted as the effective number of image-class pairs in the dataset, after accounting for similarities among images. One of these, $A_0$, explained 67\\% of the variance in balanced accuracy across all subsets, vs. 54\\% for class balance and just 39\\% for size. The best pair was size and $A_1$ (79\\%), which outperformed size and class balance (74\\%). $A$ performed best for subsets from individual datasets as well as across datasets, supporting the generality of these results. We propose maximizing $A$ as a potential new way to improve the performance of deep learning in medical imaging.","sentences":["In deep learning, achieving high performance on image classification tasks requires diverse training sets.","However, dataset diversity is incompletely understood.","The current best practice is to try to maximize dataset size and class balance.","Yet large, class-balanced datasets are not guaranteed to be diverse: images can still be arbitrarily similar.","We hypothesized that, for a given model architecture, better model performance can be achieved by maximizing dataset diversity more directly.","This could open a path for performance improvement without additional computational resources or architectural advances.","To test this hypothesis, we introduce a comprehensive framework of diversity measures, developed in ecology, that generalizes familiar quantities like Shannon entropy by accounting for similarities and differences among images.","(Dataset size and class balance emerge from this framework as special cases.)","By analyzing thousands of subsets from seven medical datasets representing ultrasound, X-ray, CT, and pathology images, we found that the best correlates of performance were not size or class balance but $A$ -- ``big alpha'' -- a set of generalized entropy measures interpreted as the effective number of image-class pairs in the dataset, after accounting for similarities among images.","One of these, $A_0$, explained 67\\% of the variance in balanced accuracy across all subsets, vs. 54\\% for class balance and just 39\\% for size.","The best pair was size and $A_1$ (79\\%), which outperformed size and class balance (74\\%).","$A$ performed best for subsets from individual datasets as well as across datasets, supporting the generality of these results.","We propose maximizing $A$ as a potential new way to improve the performance of deep learning in medical imaging."],"url":"http://arxiv.org/abs/2407.15724v1"}
{"created":"2024-07-22 15:27:55","title":"DStruct2Design: Data and Benchmarks for Data Structure Driven Generative Floor Plan Design","abstract":"Text conditioned generative models for images have yielded impressive results. Text conditioned floorplan generation as a special type of raster image generation task also received particular attention. However there are many use cases in floorpla generation where numerical properties of the generated result are more important than the aesthetics. For instance, one might want to specify sizes for certain rooms in a floorplan and compare the generated floorplan with given specifications Current approaches, datasets and commonly used evaluations do not support these kinds of constraints. As such, an attractive strategy is to generate an intermediate data structure that contains numerical properties of a floorplan which can be used to generate the final floorplan image. To explore this setting we (1) construct a new dataset for this data-structure to data-structure formulation of floorplan generation using two popular image based floorplan datasets RPLAN and ProcTHOR-10k, and provide the tools to convert further procedurally generated ProcTHOR floorplan data into our format. (2) We explore the task of floorplan generation given a partial or complete set of constraints and we design a series of metrics and benchmarks to enable evaluating how well samples generated from models respect the constraints. (3) We create multiple baselines by finetuning a large language model (LLM), Llama3, and demonstrate the feasibility of using floorplan data structure conditioned LLMs for the problem of floorplan generation respecting numerical constraints. We hope that our new datasets and benchmarks will encourage further research on different ways to improve the performance of LLMs and other generative modelling techniques for generating designs where quantitative constraints are only partially specified, but must be respected.","sentences":["Text conditioned generative models for images have yielded impressive results.","Text conditioned floorplan generation as a special type of raster image generation task also received particular attention.","However there are many use cases in floorpla generation where numerical properties of the generated result are more important than the aesthetics.","For instance, one might want to specify sizes for certain rooms in a floorplan and compare the generated floorplan with given specifications Current approaches, datasets and commonly used evaluations do not support these kinds of constraints.","As such, an attractive strategy is to generate an intermediate data structure that contains numerical properties of a floorplan which can be used to generate the final floorplan image.","To explore this setting we (1) construct a new dataset for this data-structure to data-structure formulation of floorplan generation using two popular image based floorplan datasets RPLAN and ProcTHOR-10k, and provide the tools to convert further procedurally generated ProcTHOR floorplan data into our format.","(2) We explore the task of floorplan generation given a partial or complete set of constraints and we design a series of metrics and benchmarks to enable evaluating how well samples generated from models respect the constraints.","(3) We create multiple baselines by finetuning a large language model (LLM), Llama3, and demonstrate the feasibility of using floorplan data structure conditioned LLMs for the problem of floorplan generation respecting numerical constraints.","We hope that our new datasets and benchmarks will encourage further research on different ways to improve the performance of LLMs and other generative modelling techniques for generating designs where quantitative constraints are only partially specified, but must be respected."],"url":"http://arxiv.org/abs/2407.15723v1"}
{"created":"2024-07-22 15:25:49","title":"MicroCam: Leveraging Smartphone Microscope Camera for Context-Aware Contact Surface Sensing","abstract":"The primary focus of this research is the discreet and subtle everyday contact interactions between mobile phones and their surrounding surfaces. Such interactions are anticipated to facilitate mobile context awareness, encompassing aspects such as dispensing medication updates, intelligently switching modes (e.g., silent mode), or initiating commands (e.g., deactivating an alarm). We introduce MicroCam, a contact-based sensing system that employs smartphone IMU data to detect the routine state of phone placement and utilizes a built-in microscope camera to capture intricate surface details. In particular, a natural dataset is collected to acquire authentic surface textures in situ for training and testing. Moreover, we optimize the deep neural network component of the algorithm, based on continual learning, to accurately discriminate between object categories (e.g., tables) and material constituents (e.g., wood). Experimental results highlight the superior accuracy, robustness and generalization of the proposed method. Lastly, we conducted a comprehensive discussion centered on our prototype, encompassing topics such as system performance and potential applications and scenarios.","sentences":["The primary focus of this research is the discreet and subtle everyday contact interactions between mobile phones and their surrounding surfaces.","Such interactions are anticipated to facilitate mobile context awareness, encompassing aspects such as dispensing medication updates, intelligently switching modes (e.g., silent mode), or initiating commands (e.g., deactivating an alarm).","We introduce MicroCam, a contact-based sensing system that employs smartphone IMU data to detect the routine state of phone placement and utilizes a built-in microscope camera to capture intricate surface details.","In particular, a natural dataset is collected to acquire authentic surface textures in situ for training and testing.","Moreover, we optimize the deep neural network component of the algorithm, based on continual learning, to accurately discriminate between object categories (e.g., tables) and material constituents (e.g., wood).","Experimental results highlight the superior accuracy, robustness and generalization of the proposed method.","Lastly, we conducted a comprehensive discussion centered on our prototype, encompassing topics such as system performance and potential applications and scenarios."],"url":"http://arxiv.org/abs/2407.15722v1"}
{"created":"2024-07-22 15:25:05","title":"Equality of morphic sequences","abstract":"Morphic sequences form a natural class of infinite sequences, typically defined as the coding of a fixed point of a morphism. Different morphisms and codings may yield the same morphic sequence. This paper investigates how to prove that two such representations of a morphic sequence by morphisms represent the same sequence. In particular, we focus on the smallest representations of the subsequences of the binary Fibonacci sequence obtained by only taking the even or odd elements. The proofs we give are induction proofs of several properties simultaneously, and are typically found fully automatically by a tool that we developed.","sentences":["Morphic sequences form a natural class of infinite sequences, typically defined as the coding of a fixed point of a morphism.","Different morphisms and codings may yield the same morphic sequence.","This paper investigates how to prove that two such representations of a morphic sequence by morphisms represent the same sequence.","In particular, we focus on the smallest representations of the subsequences of the binary Fibonacci sequence obtained by only taking the even or odd elements.","The proofs we give are induction proofs of several properties simultaneously, and are typically found fully automatically by a tool that we developed."],"url":"http://arxiv.org/abs/2407.15721v1"}
{"created":"2024-07-22 15:22:34","title":"Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability","abstract":"Large language models (LLMs) have emerged as powerful tools for many AI problems and exhibit remarkable in-context learning (ICL) capabilities. Compositional ability, solving unseen complex tasks that combine two or more simple tasks, is an essential reasoning ability for Artificial General Intelligence. Despite LLM's tremendous success, how they approach composite tasks, especially those not encountered during the pretraining phase, remains an open question and largely ununderstood. In this study, we delve into the ICL capabilities of LLMs on composite tasks, with only simple tasks as in-context examples. We develop a test suite of composite tasks that include linguistic and logical challenges and perform empirical studies across different LLM families. We observe that models exhibit divergent behaviors: (1) For simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability; (2) for more complex composite tasks that involving reasoning multiple steps, where each step represent one task, models typically underperform, and scaling up generally provide no improvements. We offer theoretical analysis in a simplified setting, explaining that models exhibit compositional capability when the task handles different input parts separately. We believe our work sheds new light on the capabilities of LLMs in solving composite tasks regarding the nature of the tasks and model scale. Our dataset and code are available at {\\url{https://github.com/OliverXUZY/LLM_Compose}}.","sentences":["Large language models (LLMs) have emerged as powerful tools for many AI problems and exhibit remarkable in-context learning (ICL) capabilities.","Compositional ability, solving unseen complex tasks that combine two or more simple tasks, is an essential reasoning ability for Artificial General Intelligence.","Despite LLM's tremendous success, how they approach composite tasks, especially those not encountered during the pretraining phase, remains an open question and largely ununderstood.","In this study, we delve into the ICL capabilities of LLMs on composite tasks, with only simple tasks as in-context examples.","We develop a test suite of composite tasks that include linguistic and logical challenges and perform empirical studies across different LLM families.","We observe that models exhibit divergent behaviors: (1) For simpler composite tasks that apply distinct mapping mechanisms to different input segments, the models demonstrate decent compositional ability, while scaling up the model enhances this ability; (2) for more complex composite tasks that involving reasoning multiple steps, where each step represent one task, models typically underperform, and scaling up generally provide no improvements.","We offer theoretical analysis in a simplified setting, explaining that models exhibit compositional capability when the task handles different input parts separately.","We believe our work sheds new light on the capabilities of LLMs in solving composite tasks regarding the nature of the tasks and model scale.","Our dataset and code are available at {\\url{https://github.com/OliverXUZY/LLM_Compose}}."],"url":"http://arxiv.org/abs/2407.15720v1"}
{"created":"2024-07-22 15:22:33","title":"GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI","abstract":"Alzheimer's Disease (AD) is an irreversible neurodegenerative disorder that often progresses from Mild Cognitive Impairment (MCI), leading to memory loss and significantly impacting patients' lives. Clinical trials indicate that early targeted interventions for MCI patients can potentially slow or halt the development and progression of AD. Previous research has shown that accurate medical classification requires the inclusion of extensive multimodal data, such as assessment scales and various neuroimaging techniques like Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). However, consistently tracking the diagnosis of the same individual over time and simultaneously collecting multimodal data poses significant challenges. To address this issue, we introduce GFE-Mamba, a classifier based on Generative Feature Extraction (GFE). This classifier effectively integrates data from assessment scales, MRI, and PET, enabling deeper multimodal fusion. It efficiently extracts both long and short sequence information and incorporates additional information beyond the pixel space. This approach not only improves classification accuracy but also enhances the interpretability and stability of the model. We constructed datasets of over 3000 samples based on the Alzheimer's Disease Neuroimaging Initiative (ADNI) for a two-step training process. Our experimental results demonstrate that the GFE-Mamba model is effective in predicting the conversion from MCI to AD and outperforms several state-of-the-art methods. Our source code and ADNI dataset processing code are available at https://github.com/Tinysqua/GFE-Mamba.","sentences":["Alzheimer's Disease (AD) is an irreversible neurodegenerative disorder that often progresses from Mild Cognitive Impairment (MCI), leading to memory loss and significantly impacting patients' lives.","Clinical trials indicate that early targeted interventions for MCI patients can potentially slow or halt the development and progression of AD.","Previous research has shown that accurate medical classification requires the inclusion of extensive multimodal data, such as assessment scales and various neuroimaging techniques like Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET).","However, consistently tracking the diagnosis of the same individual over time and simultaneously collecting multimodal data poses significant challenges.","To address this issue, we introduce GFE-Mamba, a classifier based on Generative Feature Extraction (GFE).","This classifier effectively integrates data from assessment scales, MRI, and PET, enabling deeper multimodal fusion.","It efficiently extracts both long and short sequence information and incorporates additional information beyond the pixel space.","This approach not only improves classification accuracy but also enhances the interpretability and stability of the model.","We constructed datasets of over 3000 samples based on the Alzheimer's Disease Neuroimaging Initiative (ADNI) for a two-step training process.","Our experimental results demonstrate that the GFE-Mamba model is effective in predicting the conversion from MCI to AD and outperforms several state-of-the-art methods.","Our source code and ADNI dataset processing code are available at https://github.com/Tinysqua/GFE-Mamba."],"url":"http://arxiv.org/abs/2407.15719v1"}
{"created":"2024-07-22 15:22:08","title":"Harmonizing Flows: Leveraging normalizing flows for unsupervised and source-free MRI harmonization","abstract":"Lack of standardization and various intrinsic parameters for magnetic resonance (MR) image acquisition results in heterogeneous images across different sites and devices, which adversely affects the generalization of deep neural networks. To alleviate this issue, this work proposes a novel unsupervised harmonization framework that leverages normalizing flows to align MR images, thereby emulating the distribution of a source domain. The proposed strategy comprises three key steps. Initially, a normalizing flow network is trained to capture the distribution characteristics of the source domain. Then, we train a shallow harmonizer network to reconstruct images from the source domain via their augmented counterparts. Finally, during inference, the harmonizer network is updated to ensure that the output images conform to the learned source domain distribution, as modeled by the normalizing flow network. Our approach, which is unsupervised, source-free, and task-agnostic is assessed in the context of both adults and neonatal cross-domain brain MRI segmentation, as well as neonatal brain age estimation, demonstrating its generalizability across tasks and population demographics. The results underscore its superior performance compared to existing methodologies. The code is available at https://github.com/farzad-bz/Harmonizing-Flows","sentences":["Lack of standardization and various intrinsic parameters for magnetic resonance (MR) image acquisition results in heterogeneous images across different sites and devices, which adversely affects the generalization of deep neural networks.","To alleviate this issue, this work proposes a novel unsupervised harmonization framework that leverages normalizing flows to align MR images, thereby emulating the distribution of a source domain.","The proposed strategy comprises three key steps.","Initially, a normalizing flow network is trained to capture the distribution characteristics of the source domain.","Then, we train a shallow harmonizer network to reconstruct images from the source domain via their augmented counterparts.","Finally, during inference, the harmonizer network is updated to ensure that the output images conform to the learned source domain distribution, as modeled by the normalizing flow network.","Our approach, which is unsupervised, source-free, and task-agnostic is assessed in the context of both adults and neonatal cross-domain brain MRI segmentation, as well as neonatal brain age estimation, demonstrating its generalizability across tasks and population demographics.","The results underscore its superior performance compared to existing methodologies.","The code is available at https://github.com/farzad-bz/Harmonizing-Flows"],"url":"http://arxiv.org/abs/2407.15717v1"}
{"created":"2024-07-22 15:22:07","title":"CrashEventLLM: Predicting System Crashes with Large Language Models","abstract":"As the dependence on computer systems expands across various domains, focusing on personal, industrial, and large-scale applications, there arises a compelling need to enhance their reliability to sustain business operations seamlessly and ensure optimal user satisfaction. System logs generated by these devices serve as valuable repositories of historical trends and past failures. The use of machine learning techniques for failure prediction has become commonplace, enabling the extraction of insights from past data to anticipate future behavior patterns. Recently, large language models have demonstrated remarkable capabilities in tasks including summarization, reasoning, and event prediction. Therefore, in this paper, we endeavor to investigate the potential of large language models in predicting system failures, leveraging insights learned from past failure behavior to inform reasoning and decision-making processes effectively. Our approach involves leveraging data from the Intel Computing Improvement Program (ICIP) system crash logs to identify significant events and develop CrashEventLLM. This model, built upon a large language model framework, serves as our foundation for crash event prediction. Specifically, our model utilizes historical data to forecast future crash events, informed by expert annotations. Additionally, it goes beyond mere prediction, offering insights into potential causes for each crash event. This work provides the preliminary insights into prompt-based large language models for the log-based event prediction task.","sentences":["As the dependence on computer systems expands across various domains, focusing on personal, industrial, and large-scale applications, there arises a compelling need to enhance their reliability to sustain business operations seamlessly and ensure optimal user satisfaction.","System logs generated by these devices serve as valuable repositories of historical trends and past failures.","The use of machine learning techniques for failure prediction has become commonplace, enabling the extraction of insights from past data to anticipate future behavior patterns.","Recently, large language models have demonstrated remarkable capabilities in tasks including summarization, reasoning, and event prediction.","Therefore, in this paper, we endeavor to investigate the potential of large language models in predicting system failures, leveraging insights learned from past failure behavior to inform reasoning and decision-making processes effectively.","Our approach involves leveraging data from the Intel Computing Improvement Program (ICIP) system crash logs to identify significant events and develop CrashEventLLM.","This model, built upon a large language model framework, serves as our foundation for crash event prediction.","Specifically, our model utilizes historical data to forecast future crash events, informed by expert annotations.","Additionally, it goes beyond mere prediction, offering insights into potential causes for each crash event.","This work provides the preliminary insights into prompt-based large language models for the log-based event prediction task."],"url":"http://arxiv.org/abs/2407.15716v1"}
{"created":"2024-07-22 15:21:58","title":"Cryptoeconomics and Tokenomics as Economics: A Survey with Opinions","abstract":"This paper surveys products and studies on cryptoeconomics and tokenomics from an economic perspective, as these terms are still (i) ill-defined and (ii) disconnected from economic disciplines. We first suggest that they can be novel when integrated; we then conduct a literature review and case study following consensus-building for decentralization and token value for autonomy. Integration requires simultaneous consideration of strategic behavior, spamming, Sybil attacks, free-riding, marginal cost, marginal utility and stabilizers. This survey is the first systematization of knowledge on cryptoeconomics and tokenomics, aiming to bridge the contexts of economics and blockchain.","sentences":["This paper surveys products and studies on cryptoeconomics and tokenomics from an economic perspective, as these terms are still (i) ill-defined and (ii) disconnected from economic disciplines.","We first suggest that they can be novel when integrated; we then conduct a literature review and case study following consensus-building for decentralization and token value for autonomy.","Integration requires simultaneous consideration of strategic behavior, spamming, Sybil attacks, free-riding, marginal cost, marginal utility and stabilizers.","This survey is the first systematization of knowledge on cryptoeconomics and tokenomics, aiming to bridge the contexts of economics and blockchain."],"url":"http://arxiv.org/abs/2407.15715v1"}
{"created":"2024-07-22 15:21:35","title":"Mamba meets crack segmentation","abstract":"Cracks pose safety risks to infrastructure and cannot be overlooked. The prevailing structures in existing crack segmentation networks predominantly consist of CNNs or Transformers. However, CNNs exhibit a deficiency in global modeling capability, hindering the representation to entire crack features. Transformers can capture long-range dependencies but suffer from high and quadratic complexity. Recently, Mamba has garnered extensive attention due to its linear spatial and computational complexity and its powerful global perception. This study explores the representation capabilities of Mamba to crack features. Specifically, this paper uncovers the connection between Mamba and the attention mechanism, providing a profound insight, an attention perspective, into interpreting Mamba and devising a novel Mamba module following the principles of attention blocks, namely CrackMamba. We compare CrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on two datasets comprising asphalt pavement and concrete pavement cracks, and steel cracks, respectively. The quantitative results show that CrackMamba stands out as the sole Mamba block consistently enhancing the baseline model's performance across all evaluation measures, while reducing its parameters and computational costs. Moreover, this paper substantiates that Mamba can achieve global receptive fields through both theoretical analysis and visual interpretability. The discoveries of this study offer a dual contribution. First, as a plug-and-play and simple yet effective Mamba module, CrackMamba exhibits immense potential for integration into various crack segmentation models. Second, the proposed innovative Mamba design concept, integrating Mamba with the attention mechanism, holds significant reference value for all Mamba-based computer vision models, not limited to crack segmentation networks, as investigated in this study.","sentences":["Cracks pose safety risks to infrastructure and cannot be overlooked.","The prevailing structures in existing crack segmentation networks predominantly consist of CNNs or Transformers.","However, CNNs exhibit a deficiency in global modeling capability, hindering the representation to entire crack features.","Transformers can capture long-range dependencies but suffer from high and quadratic complexity.","Recently, Mamba has garnered extensive attention due to its linear spatial and computational complexity and its powerful global perception.","This study explores the representation capabilities of Mamba to crack features.","Specifically, this paper uncovers the connection between Mamba and the attention mechanism, providing a profound insight, an attention perspective, into interpreting Mamba and devising a novel Mamba module following the principles of attention blocks, namely CrackMamba.","We compare CrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on two datasets comprising asphalt pavement and concrete pavement cracks, and steel cracks, respectively.","The quantitative results show that CrackMamba stands out as the sole Mamba block consistently enhancing the baseline model's performance across all evaluation measures, while reducing its parameters and computational costs.","Moreover, this paper substantiates that Mamba can achieve global receptive fields through both theoretical analysis and visual interpretability.","The discoveries of this study offer a dual contribution.","First, as a plug-and-play and simple yet effective Mamba module, CrackMamba exhibits immense potential for integration into various crack segmentation models.","Second, the proposed innovative Mamba design concept, integrating Mamba with the attention mechanism, holds significant reference value for all Mamba-based computer vision models, not limited to crack segmentation networks, as investigated in this study."],"url":"http://arxiv.org/abs/2407.15714v1"}
{"created":"2024-07-22 15:18:45","title":"AssistantBench: Can Web Agents Solve Realistic and Time-Consuming Tasks?","abstract":"Language agents, built on top of language models (LMs), are systems that can interact with complex environments, such as the open web. In this work, we examine whether such agents can perform realistic and time-consuming tasks on the web, e.g., monitoring real-estate markets or locating relevant nearby businesses. We introduce AssistantBench, a challenging new benchmark consisting of 214 realistic tasks that can be automatically evaluated, covering different scenarios and domains. We find that AssistantBench exposes the limitations of current systems, including language models and retrieval-augmented language models, as no model reaches an accuracy of more than 25 points. While closed-book LMs perform well, they exhibit low precision since they tend to hallucinate facts. State-of-the-art web agents reach a score of near zero. Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance. Moreover, we analyze failures of current systems and highlight that web navigation remains a major challenge.","sentences":["Language agents, built on top of language models (LMs), are systems that can interact with complex environments, such as the open web.","In this work, we examine whether such agents can perform realistic and time-consuming tasks on the web, e.g., monitoring real-estate markets or locating relevant nearby businesses.","We introduce AssistantBench, a challenging new benchmark consisting of 214 realistic tasks that can be automatically evaluated, covering different scenarios and domains.","We find that AssistantBench exposes the limitations of current systems, including language models and retrieval-augmented language models, as no model reaches an accuracy of more than 25 points.","While closed-book LMs perform well, they exhibit low precision since they tend to hallucinate facts.","State-of-the-art web agents reach a score of near zero.","Additionally, we introduce SeePlanAct (SPA), a new web agent that significantly outperforms previous agents, and an ensemble of SPA and closed-book models reaches the best overall performance.","Moreover, we analyze failures of current systems and highlight that web navigation remains a major challenge."],"url":"http://arxiv.org/abs/2407.15711v1"}
{"created":"2024-07-22 15:17:39","title":"SwinSF: Image Reconstruction from Spatial-Temporal Spike Streams","abstract":"The spike camera, with its high temporal resolution, low latency, and high dynamic range, addresses high-speed imaging challenges like motion blur. It captures photons at each pixel independently, creating binary spike streams rich in temporal information but challenging for image reconstruction. Current algorithms, both traditional and deep learning-based, still need to be improved in the utilization of the rich temporal detail and the restoration of the details of the reconstructed image. To overcome this, we introduce Swin Spikeformer (SwinSF), a novel model for dynamic scene reconstruction from spike streams. SwinSF is composed of Spike Feature Extraction, Spatial-Temporal Feature Extraction, and Final Reconstruction Module. It combines shifted window self-attention and proposed temporal spike attention, ensuring a comprehensive feature extraction that encapsulates both spatial and temporal dynamics, leading to a more robust and accurate reconstruction of spike streams. Furthermore, we build a new synthesized dataset for spike image reconstruction which matches the resolution of the latest spike camera, ensuring its relevance and applicability to the latest developments in spike camera imaging. Experimental results demonstrate that the proposed network SwinSF sets a new benchmark, achieving state-of-the-art performance across a series of datasets, including both real-world and synthesized data across various resolutions. Our codes and proposed dataset will be available soon.","sentences":["The spike camera, with its high temporal resolution, low latency, and high dynamic range, addresses high-speed imaging challenges like motion blur.","It captures photons at each pixel independently, creating binary spike streams rich in temporal information but challenging for image reconstruction.","Current algorithms, both traditional and deep learning-based, still need to be improved in the utilization of the rich temporal detail and the restoration of the details of the reconstructed image.","To overcome this, we introduce Swin Spikeformer (SwinSF), a novel model for dynamic scene reconstruction from spike streams.","SwinSF is composed of Spike Feature Extraction, Spatial-Temporal Feature Extraction, and Final Reconstruction Module.","It combines shifted window self-attention and proposed temporal spike attention, ensuring a comprehensive feature extraction that encapsulates both spatial and temporal dynamics, leading to a more robust and accurate reconstruction of spike streams.","Furthermore, we build a new synthesized dataset for spike image reconstruction which matches the resolution of the latest spike camera, ensuring its relevance and applicability to the latest developments in spike camera imaging.","Experimental results demonstrate that the proposed network SwinSF sets a new benchmark, achieving state-of-the-art performance across a series of datasets, including both real-world and synthesized data across various resolutions.","Our codes and proposed dataset will be available soon."],"url":"http://arxiv.org/abs/2407.15708v1"}
{"created":"2024-07-22 15:17:09","title":"Predicting the Best of N Visual Trackers","abstract":"We observe that the performance of SOTA visual trackers surprisingly strongly varies across different video attributes and datasets. No single tracker remains the best performer across all tracking attributes and datasets. To bridge this gap, for a given video sequence, we predict the \"Best of the N Trackers\", called the BofN meta-tracker. At its core, a Tracking Performance Prediction Network (TP2N) selects a predicted best performing visual tracker for the given video sequence using only a few initial frames. We also introduce a frame-level BofN meta-tracker which keeps predicting best performer after regular temporal intervals. The TP2N is based on self-supervised learning architectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with ViT-S as a backbone performs the best. The video-level BofN meta-tracker outperforms, by a large margin, existing SOTA trackers on nine standard benchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123, OTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofN meta-tracker effectively handling variations in the tracking scenarios within long sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is 88.7% and 91.1% with video and frame-level settings respectively. The best performing tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected average overlap is 67.88% and 70.98% with video and frame level settings, compared to the best performing ARTrack, 64.12%. This work also presents an extensive evaluation of competitive tracking methods on all commonly used benchmarks, following their protocols. The code, the trained models, and the results will soon be made publicly available on https://github.com/BasitAlawode/Best_of_N_Trackers.","sentences":["We observe that the performance of SOTA visual trackers surprisingly strongly varies across different video attributes and datasets.","No single tracker remains the best performer across all tracking attributes and datasets.","To bridge this gap, for a given video sequence, we predict the \"Best of the N Trackers\", called the BofN meta-tracker.","At its core, a Tracking Performance Prediction Network (TP2N) selects a predicted best performing visual tracker for the given video sequence using only a few initial frames.","We also introduce a frame-level BofN meta-tracker which keeps predicting best performer after regular temporal intervals.","The TP2N is based on self-supervised learning architectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with ViT-S as a backbone performs the best.","The video-level BofN meta-tracker outperforms, by a large margin, existing SOTA trackers on nine standard benchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123, OTB100, and WebUAV-3M.","Further improvement is achieved by the frame-level BofN meta-tracker effectively handling variations in the tracking scenarios within long sequences.","For instance, on GOT-10k, BofN meta-tracker average overlap is 88.7% and 91.1% with video and frame-level settings respectively.","The best performing tracker, RTS, achieves 85.20% AO.","On VOT2022, BofN expected average overlap is 67.88% and 70.98% with video and frame level settings, compared to the best performing ARTrack, 64.12%.","This work also presents an extensive evaluation of competitive tracking methods on all commonly used benchmarks, following their protocols.","The code, the trained models, and the results will soon be made publicly available on https://github.com/BasitAlawode/Best_of_N_Trackers."],"url":"http://arxiv.org/abs/2407.15707v1"}
{"created":"2024-07-22 15:16:47","title":"Multi-Modality Co-Learning for Efficient Skeleton-based Action Recognition","abstract":"Skeleton-based action recognition has garnered significant attention due to the utilization of concise and resilient skeletons. Nevertheless, the absence of detailed body information in skeletons restricts performance, while other multimodal methods require substantial inference resources and are inefficient when using multimodal data during both training and inference stages. To address this and fully harness the complementary multimodal features, we propose a novel multi-modality co-learning (MMCL) framework by leveraging the multimodal large language models (LLMs) as auxiliary networks for efficient skeleton-based action recognition, which engages in multi-modality co-learning during the training stage and keeps efficiency by employing only concise skeletons in inference. Our MMCL framework primarily consists of two modules. First, the Feature Alignment Module (FAM) extracts rich RGB features from video frames and aligns them with global skeleton features via contrastive learning. Second, the Feature Refinement Module (FRM) uses RGB images with temporal information and text instruction to generate instructive features based on the powerful generalization of multimodal LLMs. These instructive text features will further refine the classification scores and the refined scores will enhance the model's robustness and generalization in a manner similar to soft labels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA benchmarks consistently verify the effectiveness of our MMCL, which outperforms the existing skeleton-based action recognition methods. Meanwhile, experiments on UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization of our MMCL in zero-shot and domain-adaptive action recognition. Our code is publicly available at: https://github.com/liujf69/MMCL-Action.","sentences":["Skeleton-based action recognition has garnered significant attention due to the utilization of concise and resilient skeletons.","Nevertheless, the absence of detailed body information in skeletons restricts performance, while other multimodal methods require substantial inference resources and are inefficient when using multimodal data during both training and inference stages.","To address this and fully harness the complementary multimodal features, we propose a novel multi-modality co-learning (MMCL) framework by leveraging the multimodal large language models (LLMs) as auxiliary networks for efficient skeleton-based action recognition, which engages in multi-modality co-learning during the training stage and keeps efficiency by employing only concise skeletons in inference.","Our MMCL framework primarily consists of two modules.","First, the Feature Alignment Module (FAM) extracts rich RGB features from video frames and aligns them with global skeleton features via contrastive learning.","Second, the Feature Refinement Module (FRM) uses RGB images with temporal information and text instruction to generate instructive features based on the powerful generalization of multimodal LLMs.","These instructive text features will further refine the classification scores and the refined scores will enhance the model's robustness and generalization in a manner similar to soft labels.","Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA benchmarks consistently verify the effectiveness of our MMCL, which outperforms the existing skeleton-based action recognition methods.","Meanwhile, experiments on UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization of our MMCL in zero-shot and domain-adaptive action recognition.","Our code is publicly available at: https://github.com/liujf69/MMCL-Action."],"url":"http://arxiv.org/abs/2407.15706v1"}
{"created":"2024-07-22 15:10:41","title":"Estimating Probability Densities with Transformer and Denoising Diffusion","abstract":"Transformers are often the go-to architecture to build foundation models that ingest a large amount of training data. But these models do not estimate the probability density distribution when trained on regression problems, yet obtaining full probabilistic outputs is crucial to many fields of science, where the probability distribution of the answer can be non-Gaussian and multimodal. In this work, we demonstrate that training a probabilistic model using a denoising diffusion head on top of the Transformer provides reasonable probability density estimation even for high-dimensional inputs. The combined Transformer+Denoising Diffusion model allows conditioning the output probability density on arbitrary combinations of inputs and it is thus a highly flexible density function emulator of all possible input/output combinations. We illustrate our Transformer+Denoising Diffusion model by training it on a large dataset of astronomical observations and measured labels of stars within our Galaxy and we apply it to a variety of inference tasks to show that the model can infer labels accurately with reasonable distributions.","sentences":["Transformers are often the go-to architecture to build foundation models that ingest a large amount of training data.","But these models do not estimate the probability density distribution when trained on regression problems, yet obtaining full probabilistic outputs is crucial to many fields of science, where the probability distribution of the answer can be non-Gaussian and multimodal.","In this work, we demonstrate that training a probabilistic model using a denoising diffusion head on top of the Transformer provides reasonable probability density estimation even for high-dimensional inputs.","The combined Transformer+Denoising Diffusion model allows conditioning the output probability density on arbitrary combinations of inputs and it is thus a highly flexible density function emulator of all possible input/output combinations.","We illustrate our Transformer+Denoising Diffusion model by training it on a large dataset of astronomical observations and measured labels of stars within our Galaxy and we apply it to a variety of inference tasks to show that the model can infer labels accurately with reasonable distributions."],"url":"http://arxiv.org/abs/2407.15703v1"}
{"created":"2024-07-22 15:08:42","title":"Robotic Shepherding in Cluttered and Unknown Environments using Control Barrier Functions","abstract":"This paper introduces a novel control methodology designed to guide a collective of robotic-sheep in a cluttered and unknown environment using robotic-dogs. The dog-agents continuously scan the environment and compute a safe trajectory to guide the sheep to their final destination. The proposed optimization-based controller guarantees that the sheep reside within a desired distance from the reference trajectory through the use of Control Barrier Functions (CBF). Additional CBF constraints are employed simultaneously to ensure inter-agent and obstacle collision avoidance. The efficacy of the proposed approach is rigorously tested in simulation, which demonstrates the successful herding of the robotic-sheep within complex and cluttered environments.","sentences":["This paper introduces a novel control methodology designed to guide a collective of robotic-sheep in a cluttered and unknown environment using robotic-dogs.","The dog-agents continuously scan the environment and compute a safe trajectory to guide the sheep to their final destination.","The proposed optimization-based controller guarantees that the sheep reside within a desired distance from the reference trajectory through the use of Control Barrier Functions (CBF).","Additional CBF constraints are employed simultaneously to ensure inter-agent and obstacle collision avoidance.","The efficacy of the proposed approach is rigorously tested in simulation, which demonstrates the successful herding of the robotic-sheep within complex and cluttered environments."],"url":"http://arxiv.org/abs/2407.15701v1"}
{"created":"2024-07-22 15:07:27","title":"A Life-long Learning Intrusion Detection System for 6G-Enabled IoV","abstract":"The introduction of 6G technology into the Internet of Vehicles (IoV) promises to revolutionize connectivity with ultra-high data rates and seamless network coverage. However, this technological leap also brings significant challenges, particularly for the dynamic and diverse IoV landscape, which must meet the rigorous reliability and security requirements of 6G networks. Furthermore, integrating 6G will likely increase the IoV's susceptibility to a spectrum of emerging cyber threats. Therefore, it is crucial for security mechanisms to dynamically adapt and learn new attack patterns, keeping pace with the rapid evolution and diversification of these threats - a capability currently lacking in existing systems. This paper presents a novel intrusion detection system leveraging the paradigm of life-long (or continual) learning. Our methodology combines class-incremental learning with federated learning, an approach ideally suited to the distributed nature of the IoV. This strategy effectively harnesses the collective intelligence of Connected and Automated Vehicles (CAVs) and edge computing capabilities to train the detection system. To the best of our knowledge, this study is the first to synergize class-incremental learning with federated learning specifically for cyber attack detection. Through comprehensive experiments on a recent network traffic dataset, our system has exhibited a robust adaptability in learning new cyber attack patterns, while effectively retaining knowledge of previously encountered ones. Additionally, it has proven to maintain high accuracy and a low false positive rate.","sentences":["The introduction of 6G technology into the Internet of Vehicles (IoV) promises to revolutionize connectivity with ultra-high data rates and seamless network coverage.","However, this technological leap also brings significant challenges, particularly for the dynamic and diverse IoV landscape, which must meet the rigorous reliability and security requirements of 6G networks.","Furthermore, integrating 6G will likely increase the IoV's susceptibility to a spectrum of emerging cyber threats.","Therefore, it is crucial for security mechanisms to dynamically adapt and learn new attack patterns, keeping pace with the rapid evolution and diversification of these threats - a capability currently lacking in existing systems.","This paper presents a novel intrusion detection system leveraging the paradigm of life-long (or continual) learning.","Our methodology combines class-incremental learning with federated learning, an approach ideally suited to the distributed nature of the IoV.","This strategy effectively harnesses the collective intelligence of Connected and Automated Vehicles (CAVs) and edge computing capabilities to train the detection system.","To the best of our knowledge, this study is the first to synergize class-incremental learning with federated learning specifically for cyber attack detection.","Through comprehensive experiments on a recent network traffic dataset, our system has exhibited a robust adaptability in learning new cyber attack patterns, while effectively retaining knowledge of previously encountered ones.","Additionally, it has proven to maintain high accuracy and a low false positive rate."],"url":"http://arxiv.org/abs/2407.15700v1"}
{"created":"2024-07-22 15:01:45","title":"Supporting the Digital Autonomy of Elders Through LLM Assistance","abstract":"The internet offers tremendous access to services, social connections, and needed products. However, to those without sufficient experience, engaging with businesses and friends across the internet can be daunting due to the ever present danger of scammers and thieves, to say nothing of the myriad of potential computer viruses. Like a forest rich with both edible and poisonous plants, those familiar with the norms inhabit it safely with ease while newcomers need a guide. However, reliance on a human digital guide can be taxing and often impractical. We propose and pilot a simple but unexplored idea: could an LLM provide the necessary support to help the elderly who are separated by the digital divide safely achieve digital autonomy?","sentences":["The internet offers tremendous access to services, social connections, and needed products.","However, to those without sufficient experience, engaging with businesses and friends across the internet can be daunting due to the ever present danger of scammers and thieves, to say nothing of the myriad of potential computer viruses.","Like a forest rich with both edible and poisonous plants, those familiar with the norms inhabit it safely with ease while newcomers need a guide.","However, reliance on a human digital guide can be taxing and often impractical.","We propose and pilot a simple but unexplored idea: could an LLM provide the necessary support to help the elderly who are separated by the digital divide safely achieve digital autonomy?"],"url":"http://arxiv.org/abs/2407.15695v1"}
{"created":"2024-07-22 15:00:23","title":"Counter Turing Test ($CT^2$): Investigating AI-Generated Text Detection for Hindi -- Ranking LLMs based on Hindi AI Detectability Index ($ADI_{hi}$)","abstract":"The widespread adoption of large language models (LLMs) and awareness around multilingual LLMs have raised concerns regarding the potential risks and repercussions linked to the misapplication of AI-generated text, necessitating increased vigilance. While these models are primarily trained for English, their extensive training on vast datasets covering almost the entire web, equips them with capabilities to perform well in numerous other languages. AI-Generated Text Detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection. In this paper, we report our investigation on AGTD for an indic language Hindi. Our major contributions are in four folds: i) examined 26 LLMs to evaluate their proficiency in generating Hindi text, ii) introducing the AI-generated news article in Hindi ($AG_{hi}$) dataset, iii) evaluated the effectiveness of five recently proposed AGTD techniques: ConDA, J-Guard, RADAR, RAIDAR and Intrinsic Dimension Estimation for detecting AI-generated Hindi text, iv) proposed Hindi AI Detectability Index ($ADI_{hi}$) which shows a spectrum to understand the evolving landscape of eloquence of AI-generated text in Hindi. We will make the codes and datasets available to encourage further research.","sentences":["The widespread adoption of large language models (LLMs) and awareness around multilingual LLMs have raised concerns regarding the potential risks and repercussions linked to the misapplication of AI-generated text, necessitating increased vigilance.","While these models are primarily trained for English, their extensive training on vast datasets covering almost the entire web, equips them with capabilities to perform well in numerous other languages.","AI-Generated Text Detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by the emergence of techniques to bypass detection.","In this paper, we report our investigation on AGTD for an indic language Hindi.","Our major contributions are in four folds: i) examined 26 LLMs to evaluate their proficiency in generating Hindi text, ii) introducing the AI-generated news article in Hindi ($AG_{hi}$) dataset, iii) evaluated the effectiveness of five recently proposed AGTD techniques: ConDA, J-Guard, RADAR, RAIDAR and Intrinsic Dimension Estimation for detecting AI-generated Hindi text, iv) proposed Hindi AI Detectability Index ($ADI_{hi}$) which shows a spectrum to understand the evolving landscape of eloquence of AI-generated text in Hindi.","We will make the codes and datasets available to encourage further research."],"url":"http://arxiv.org/abs/2407.15694v1"}
{"created":"2024-07-22 14:54:40","title":"AI-Driven Fast and Early Detection of IoT Botnet Threats: A Comprehensive Network Traffic Analysis Approach","abstract":"In the rapidly evolving landscape of cyber threats targeting the Internet of Things (IoT) ecosystem, and in light of the surge in botnet-driven Distributed Denial of Service (DDoS) and brute force attacks, this study focuses on the early detection of IoT bots. It specifically addresses the detection of stealth bot communication that precedes and orchestrates attacks. This study proposes a comprehensive methodology for analyzing IoT network traffic, including considerations for both unidirectional and bidirectional flow, as well as packet formats. It explores a wide spectrum of network features critical for representing network traffic and characterizing benign IoT traffic patterns effectively. Moreover, it delves into the modeling of traffic using various semi-supervised learning techniques. Through extensive experimentation with the IoT-23 dataset - a comprehensive collection featuring diverse botnet types and traffic scenarios - we have demonstrated the feasibility of detecting botnet traffic corresponding to different operations and types of bots, specifically focusing on stealth command and control (C2) communications. The results obtained have demonstrated the feasibility of identifying C2 communication with a 100% success rate through packet-based methods and 94% via flow based approaches, with a false positive rate of 1.53%.","sentences":["In the rapidly evolving landscape of cyber threats targeting the Internet of Things (IoT) ecosystem, and in light of the surge in botnet-driven Distributed Denial of Service (DDoS) and brute force attacks, this study focuses on the early detection of IoT bots.","It specifically addresses the detection of stealth bot communication that precedes and orchestrates attacks.","This study proposes a comprehensive methodology for analyzing IoT network traffic, including considerations for both unidirectional and bidirectional flow, as well as packet formats.","It explores a wide spectrum of network features critical for representing network traffic and characterizing benign IoT traffic patterns effectively.","Moreover, it delves into the modeling of traffic using various semi-supervised learning techniques.","Through extensive experimentation with the IoT-23 dataset - a comprehensive collection featuring diverse botnet types and traffic scenarios - we have demonstrated the feasibility of detecting botnet traffic corresponding to different operations and types of bots, specifically focusing on stealth command and control (C2) communications.","The results obtained have demonstrated the feasibility of identifying C2 communication with a 100% success rate through packet-based methods and 94% via flow based approaches, with a false positive rate of 1.53%."],"url":"http://arxiv.org/abs/2407.15688v1"}
{"created":"2024-07-22 14:53:29","title":"Differentiable Convex Polyhedra Optimization from Multi-view Images","abstract":"This paper presents a novel approach for the differentiable rendering of convex polyhedra, addressing the limitations of recent methods that rely on implicit field supervision. Our technique introduces a strategy that combines non-differentiable computation of hyperplane intersection through duality transform with differentiable optimization for vertex positioning with three-plane intersection, enabling gradient-based optimization without the need for 3D implicit fields. This allows for efficient shape representation across a range of applications, from shape parsing to compact mesh reconstruction. This work not only overcomes the challenges of previous approaches but also sets a new standard for representing shapes with convex polyhedra.","sentences":["This paper presents a novel approach for the differentiable rendering of convex polyhedra, addressing the limitations of recent methods that rely on implicit field supervision.","Our technique introduces a strategy that combines non-differentiable computation of hyperplane intersection through duality transform with differentiable optimization for vertex positioning with three-plane intersection, enabling gradient-based optimization without the need for 3D implicit fields.","This allows for efficient shape representation across a range of applications, from shape parsing to compact mesh reconstruction.","This work not only overcomes the challenges of previous approaches but also sets a new standard for representing shapes with convex polyhedra."],"url":"http://arxiv.org/abs/2407.15686v1"}
{"created":"2024-07-22 14:53:00","title":"The Atlas of AI Incidents in Mobile Computing: Visualizing the Risks and Benefits of AI Gone Mobile","abstract":"Today's visualization tools for conveying the risks and benefits of AI technologies are largely tailored for those with technical expertise. To bridge this gap, we have developed a visualization that employs narrative patterns and interactive elements, enabling the broader public to gradually grasp the diverse risks and benefits associated with AI. Using a dataset of 54 real-world incidents involving AI in mobile computing, we examined design choices that enhance public understanding and provoke reflection on how certain AI applications - even those deemed low-risk by law - can still lead to significant incidents. Visualization: https://social-dynamics.net/mobile-ai-risks","sentences":["Today's visualization tools for conveying the risks and benefits of AI technologies are largely tailored for those with technical expertise.","To bridge this gap, we have developed a visualization that employs narrative patterns and interactive elements, enabling the broader public to gradually grasp the diverse risks and benefits associated with AI.","Using a dataset of 54 real-world incidents involving AI in mobile computing, we examined design choices that enhance public understanding and provoke reflection on how certain AI applications - even those deemed low-risk by law - can still lead to significant incidents.","Visualization: https://social-dynamics.net/mobile-ai-risks"],"url":"http://arxiv.org/abs/2407.15685v1"}
{"created":"2024-07-22 14:51:28","title":"Enhancing Transferability of Targeted Adversarial Examples: A Self-Universal Perspective","abstract":"Transfer-based targeted adversarial attacks against black-box deep neural networks (DNNs) have been proven to be significantly more challenging than untargeted ones. The impressive transferability of current SOTA, the generative methods, comes at the cost of requiring massive amounts of additional data and time-consuming training for each targeted label. This results in limited efficiency and flexibility, significantly hindering their deployment in practical applications. In this paper, we offer a self-universal perspective that unveils the great yet underexplored potential of input transformations in pursuing this goal. Specifically, transformations universalize gradient-based attacks with intrinsic but overlooked semantics inherent within individual images, exhibiting similar scalability and comparable results to time-consuming learning over massive additional data from diverse classes. We also contribute a surprising empirical insight that one of the most fundamental transformations, simple image scaling, is highly effective, scalable, sufficient, and necessary in enhancing targeted transferability. We further augment simple scaling with orthogonal transformations and block-wise applicability, resulting in the Simple, faSt, Self-universal yet Strong Scale Transformation (S$^4$ST) for self-universal TTA. On the ImageNet-Compatible benchmark dataset, our method achieves a 19.8% improvement in the average targeted transfer success rate against various challenging victim models over existing SOTA transformation methods while only consuming 36% time for attacking. It also outperforms resource-intensive attacks by a large margin in various challenging settings.","sentences":["Transfer-based targeted adversarial attacks against black-box deep neural networks (DNNs) have been proven to be significantly more challenging than untargeted ones.","The impressive transferability of current SOTA, the generative methods, comes at the cost of requiring massive amounts of additional data and time-consuming training for each targeted label.","This results in limited efficiency and flexibility, significantly hindering their deployment in practical applications.","In this paper, we offer a self-universal perspective that unveils the great yet underexplored potential of input transformations in pursuing this goal.","Specifically, transformations universalize gradient-based attacks with intrinsic but overlooked semantics inherent within individual images, exhibiting similar scalability and comparable results to time-consuming learning over massive additional data from diverse classes.","We also contribute a surprising empirical insight that one of the most fundamental transformations, simple image scaling, is highly effective, scalable, sufficient, and necessary in enhancing targeted transferability.","We further augment simple scaling with orthogonal transformations and block-wise applicability, resulting in the Simple, faSt, Self-universal yet Strong Scale Transformation (S$^4$ST) for self-universal TTA.","On the ImageNet-Compatible benchmark dataset, our method achieves a 19.8% improvement in the average targeted transfer success rate against various challenging victim models over existing SOTA transformation methods while only consuming 36% time for attacking.","It also outperforms resource-intensive attacks by a large margin in various challenging settings."],"url":"http://arxiv.org/abs/2407.15683v1"}
{"created":"2024-07-22 14:49:51","title":"HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning","abstract":"Hallucination has been a major problem for large language models and remains a critical challenge when it comes to multimodality in which vision-language models (VLMs) have to deal with not just textual but also visual inputs. Despite rapid progress in VLMs, resources for evaluating and addressing multimodal hallucination are limited and mostly focused on evaluation. This work introduces HaloQuest, a novel visual question answering dataset that captures various aspects of multimodal hallucination such as false premises, insufficient contexts, and visual challenges. A novel idea from HaloQuest is to leverage synthetic images, apart from real ones, to enable dataset creation at scale. With over 7.7K examples spanning across a wide variety of categories, HaloQuest was designed to be both a challenging benchmark for VLMs and a fine-tuning dataset for advancing multimodal reasoning. Our experiments reveal that current models struggle with HaloQuest, with all open-source VLMs achieving below 36% accuracy. On the other hand, fine-tuning on HaloQuest significantly reduces hallucination rates while preserving performance on standard reasoning tasks. Our results discover that benchmarking with generated images is highly correlated (r=0.97) with real images. Last but not least, we propose a novel Auto-Eval mechanism that is highly correlated with human raters (r=0.99) for evaluating VLMs. In sum, this work makes concrete strides towards understanding, evaluating, and mitigating hallucination in VLMs, serving as an important step towards more reliable multimodal AI systems in the future.","sentences":["Hallucination has been a major problem for large language models and remains a critical challenge when it comes to multimodality in which vision-language models (VLMs) have to deal with not just textual but also visual inputs.","Despite rapid progress in VLMs, resources for evaluating and addressing multimodal hallucination are limited and mostly focused on evaluation.","This work introduces HaloQuest, a novel visual question answering dataset that captures various aspects of multimodal hallucination such as false premises, insufficient contexts, and visual challenges.","A novel idea from HaloQuest is to leverage synthetic images, apart from real ones, to enable dataset creation at scale.","With over 7.7K examples spanning across a wide variety of categories, HaloQuest was designed to be both a challenging benchmark for VLMs and a fine-tuning dataset for advancing multimodal reasoning.","Our experiments reveal that current models struggle with HaloQuest, with all open-source VLMs achieving below 36% accuracy.","On the other hand, fine-tuning on HaloQuest significantly reduces hallucination rates while preserving performance on standard reasoning tasks.","Our results discover that benchmarking with generated images is highly correlated (r=0.97) with real images.","Last but not least, we propose a novel Auto-Eval mechanism that is highly correlated with human raters (r=0.99) for evaluating VLMs.","In sum, this work makes concrete strides towards understanding, evaluating, and mitigating hallucination in VLMs, serving as an important step towards more reliable multimodal AI systems in the future."],"url":"http://arxiv.org/abs/2407.15680v1"}
{"created":"2024-07-22 14:43:25","title":"Language models are robotic planners: reframing plans as goal refinement graphs","abstract":"Successful application of large language models (LLMs) to robotic planning and execution may pave the way to automate numerous real-world tasks. Promising recent research has been conducted showing that the knowledge contained in LLMs can be utilized in making goal-driven decisions that are enactable in interactive, embodied environments. Nonetheless, there is a considerable drop in correctness of programs generated by LLMs. We apply goal modeling techniques from software engineering to large language models generating robotic plans. Specifically, the LLM is prompted to generate a step refinement graph for a task. The executability and correctness of the program converted from this refinement graph is then evaluated. The approach results in programs that are more correct as judged by humans in comparison to previous work.","sentences":["Successful application of large language models (LLMs) to robotic planning and execution may pave the way to automate numerous real-world tasks.","Promising recent research has been conducted showing that the knowledge contained in LLMs can be utilized in making goal-driven decisions that are enactable in interactive, embodied environments.","Nonetheless, there is a considerable drop in correctness of programs generated by LLMs.","We apply goal modeling techniques from software engineering to large language models generating robotic plans.","Specifically, the LLM is prompted to generate a step refinement graph for a task.","The executability and correctness of the program converted from this refinement graph is then evaluated.","The approach results in programs that are more correct as judged by humans in comparison to previous work."],"url":"http://arxiv.org/abs/2407.15677v1"}
{"created":"2024-07-22 14:43:05","title":"Preventing Out-of-Gas Exceptions by Typing","abstract":"We continue the development of TinySol, a minimal object-oriented language based on Solidity, the standard smart-contract language used for the Ethereum platform. We first extend TinySol with exceptions and a gas mechanism, and equip it with a small-step operational semantics. Introducing the gas mechanism is fundamental for modelling real-life smart contracts in TinySol, since this is the way in which termination of Ethereum smart contracts is usually ensured. We then devise a type system for smart contracts guaranteeing that such programs never run out of gas at runtime. This is a desirable property for smart contracts, since a transaction that runs out of gas is aborted, but the price paid to run the code is not returned to the invoker.","sentences":["We continue the development of TinySol, a minimal object-oriented language based on Solidity, the standard smart-contract language used for the Ethereum platform.","We first extend TinySol with exceptions and a gas mechanism, and equip it with a small-step operational semantics.","Introducing the gas mechanism is fundamental for modelling real-life smart contracts in TinySol, since this is the way in which termination of Ethereum smart contracts is usually ensured.","We then devise a type system for smart contracts guaranteeing that such programs never run out of gas at runtime.","This is a desirable property for smart contracts, since a transaction that runs out of gas is aborted, but the price paid to run the code is not returned to the invoker."],"url":"http://arxiv.org/abs/2407.15676v1"}
{"created":"2024-07-22 14:42:34","title":"Flow-guided Motion Prediction with Semantics and Dynamic Occupancy Grid Maps","abstract":"Accurate prediction of driving scenes is essential for road safety and autonomous driving. Occupancy Grid Maps (OGMs) are commonly employed for scene prediction due to their structured spatial representation, flexibility across sensor modalities and integration of uncertainty. Recent studies have successfully combined OGMs with deep learning methods to predict the evolution of scene and learn complex behaviours. These methods, however, do not consider prediction of flow or velocity vectors in the scene. In this work, we propose a novel multi-task framework that leverages dynamic OGMs and semantic information to predict both future vehicle semantic grids and the future flow of the scene. This incorporation of semantic flow not only offers intermediate scene features but also enables the generation of warped semantic grids. Evaluation on the real-world NuScenes dataset demonstrates improved prediction capabilities and enhanced ability of the model to retain dynamic vehicles within the scene.","sentences":["Accurate prediction of driving scenes is essential for road safety and autonomous driving.","Occupancy Grid Maps (OGMs) are commonly employed for scene prediction due to their structured spatial representation, flexibility across sensor modalities and integration of uncertainty.","Recent studies have successfully combined OGMs with deep learning methods to predict the evolution of scene and learn complex behaviours.","These methods, however, do not consider prediction of flow or velocity vectors in the scene.","In this work, we propose a novel multi-task framework that leverages dynamic OGMs and semantic information to predict both future vehicle semantic grids and the future flow of the scene.","This incorporation of semantic flow not only offers intermediate scene features but also enables the generation of warped semantic grids.","Evaluation on the real-world NuScenes dataset demonstrates improved prediction capabilities and enhanced ability of the model to retain dynamic vehicles within the scene."],"url":"http://arxiv.org/abs/2407.15675v1"}
{"created":"2024-07-22 14:41:32","title":"IDA: Breaking Barriers in No-code UI Automation Through Large Language Models and Human-Centric Design","abstract":"Business users dedicate significant amounts of time to repetitive tasks within enterprise digital platforms, highlighting a critical need for automation. Despite advancements in low-code tools for UI automation, their complexity remains a significant barrier to adoption among non-technical business users. However, recent advancements in large language models (LLMs) have created new opportunities to overcome this barrier by offering more powerful, yet simpler and more human-centric programming environments. This paper presents IDA (Intelligent Digital Apprentice), a novel no-code Web UI automation tool designed specifically to empower business users with no technical background. IDA incorporates human-centric design principles, including guided programming by demonstration, semantic programming model, and teacher-student learning metaphor which is tailored to the skill set of business users. By leveraging LLMs, IDA overcomes some of the key technical barriers that have traditionally limited the possibility of no-code solutions. We have developed a prototype of IDA and conducted a user study involving real world business users and enterprise applications. The promising results indicate that users could effectively utilize IDA to create automation. The qualitative feedback indicates that IDA is perceived as user-friendly and trustworthy. This study contributes to unlocking the potential of AI assistants to enhance the productivity of business users through no-code user interface automation.","sentences":["Business users dedicate significant amounts of time to repetitive tasks within enterprise digital platforms, highlighting a critical need for automation.","Despite advancements in low-code tools for UI automation, their complexity remains a significant barrier to adoption among non-technical business users.","However, recent advancements in large language models (LLMs) have created new opportunities to overcome this barrier by offering more powerful, yet simpler and more human-centric programming environments.","This paper presents IDA (Intelligent Digital Apprentice), a novel no-code Web UI automation tool designed specifically to empower business users with no technical background.","IDA incorporates human-centric design principles, including guided programming by demonstration, semantic programming model, and teacher-student learning metaphor which is tailored to the skill set of business users.","By leveraging LLMs, IDA overcomes some of the key technical barriers that have traditionally limited the possibility of no-code solutions.","We have developed a prototype of IDA and conducted a user study involving real world business users and enterprise applications.","The promising results indicate that users could effectively utilize IDA to create automation.","The qualitative feedback indicates that IDA is perceived as user-friendly and trustworthy.","This study contributes to unlocking the potential of AI assistants to enhance the productivity of business users through no-code user interface automation."],"url":"http://arxiv.org/abs/2407.15673v1"}
{"created":"2024-07-22 14:41:29","title":"Computer Audition: From Task-Specific Machine Learning to Foundation Models","abstract":"Foundation models (FMs) are increasingly spearheading recent advances on a variety of tasks that fall under the purview of computer audition -- the use of machines to understand sounds. They feature several advantages over traditional pipelines: among others, the ability to consolidate multiple tasks in a single model, the option to leverage knowledge from other modalities, and the readily-available interaction with human users. Naturally, these promises have created substantial excitement in the audio community, and have led to a wave of early attempts to build new, general-purpose foundation models for audio. In the present contribution, we give an overview of computational audio analysis as it transitions from traditional pipelines towards auditory foundation models. Our work highlights the key operating principles that underpin those models, and showcases how they can accommodate multiple tasks that the audio community previously tackled separately.","sentences":["Foundation models (FMs) are increasingly spearheading recent advances on a variety of tasks that fall under the purview of computer audition -- the use of machines to understand sounds.","They feature several advantages over traditional pipelines: among others, the ability to consolidate multiple tasks in a single model, the option to leverage knowledge from other modalities, and the readily-available interaction with human users.","Naturally, these promises have created substantial excitement in the audio community, and have led to a wave of early attempts to build new, general-purpose foundation models for audio.","In the present contribution, we give an overview of computational audio analysis as it transitions from traditional pipelines towards auditory foundation models.","Our work highlights the key operating principles that underpin those models, and showcases how they can accommodate multiple tasks that the audio community previously tackled separately."],"url":"http://arxiv.org/abs/2407.15672v1"}
{"created":"2024-07-22 14:38:54","title":"Problems in AI, their roots in philosophy, and implications for science and society","abstract":"Artificial Intelligence (AI) is one of today's most relevant emergent technologies. In view thereof, this paper proposes that more attention should be paid to the philosophical aspects of AI technology and its use. It is argued that this deficit is generally combined with philosophical misconceptions about the growth of knowledge. To identify these misconceptions, reference is made to the ideas of the philosopher of science Karl Popper and the physicist David Deutsch. The works of both thinkers aim against mistaken theories of knowledge, such as inductivism, empiricism, and instrumentalism. This paper shows that these theories bear similarities to how current AI technology operates. It also shows that these theories are very much alive in the (public) discourse on AI, often called Bayesianism. In line with Popper and Deutsch, it is proposed that all these theories are based on mistaken philosophies of knowledge. This includes an analysis of the implications of these mistaken philosophies for the use of AI in science and society, including some of the likely problem situations that will arise. This paper finally provides a realistic outlook on Artificial General Intelligence (AGI) and three propositions on A(G)I and philosophy (i.e., epistemology).","sentences":["Artificial Intelligence (AI) is one of today's most relevant emergent technologies.","In view thereof, this paper proposes that more attention should be paid to the philosophical aspects of AI technology and its use.","It is argued that this deficit is generally combined with philosophical misconceptions about the growth of knowledge.","To identify these misconceptions, reference is made to the ideas of the philosopher of science Karl Popper and the physicist David Deutsch.","The works of both thinkers aim against mistaken theories of knowledge, such as inductivism, empiricism, and instrumentalism.","This paper shows that these theories bear similarities to how current AI technology operates.","It also shows that these theories are very much alive in the (public) discourse on AI, often called Bayesianism.","In line with Popper and Deutsch, it is proposed that all these theories are based on mistaken philosophies of knowledge.","This includes an analysis of the implications of these mistaken philosophies for the use of AI in science and society, including some of the likely problem situations that will arise.","This paper finally provides a realistic outlook on Artificial General Intelligence (AGI) and three propositions on A(G)I and philosophy (i.e., epistemology)."],"url":"http://arxiv.org/abs/2407.15671v1"}
{"created":"2024-07-22 14:37:58","title":"vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving","abstract":"Large Language Models (LLMs) are widely used across various domains, processing millions of daily requests. This surge in demand poses significant challenges in optimizing throughput and latency while keeping costs manageable. The Key-Value (KV) cache, a standard method for retaining previous computations, makes LLM inference highly bounded by memory. While batching strategies can enhance performance, they frequently lead to significant memory fragmentation. Even though cutting-edge systems like vLLM mitigate KV cache fragmentation using paged Attention mechanisms, they still suffer from inefficient memory and computational operations due to the tightly coupled page management and computation kernels.   This study introduces the vTensor, an innovative tensor structure for LLM inference based on GPU virtual memory management (VMM). vTensor addresses existing limitations by decoupling computation from memory defragmentation and offering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous approach, ensuring efficient, fragmentation-free memory management while accommodating various computation kernels across different LLM architectures. Experimental results indicate that vTensor achieves an average speedup of 1.86x across different models, with up to 2.42x in multi-turn chat scenarios. Additionally, vTensor provides average speedups of 2.12x and 3.15x in kernel evaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton prefix-prefilling kernels and vLLM paged Attention kernel, respectively. Furthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100 GPU compared to vLLM, enabling more memory-intensive workloads.","sentences":["Large Language Models (LLMs) are widely used across various domains, processing millions of daily requests.","This surge in demand poses significant challenges in optimizing throughput and latency while keeping costs manageable.","The Key-Value (KV) cache, a standard method for retaining previous computations, makes LLM inference highly bounded by memory.","While batching strategies can enhance performance, they frequently lead to significant memory fragmentation.","Even though cutting-edge systems like vLLM mitigate KV cache fragmentation using paged Attention mechanisms, they still suffer from inefficient memory and computational operations due to the tightly coupled page management and computation kernels.   ","This study introduces the vTensor, an innovative tensor structure for LLM inference based on GPU virtual memory management (VMM).","vTensor addresses existing limitations by decoupling computation from memory defragmentation and offering dynamic extensibility.","Our framework employs a CPU-GPU heterogeneous approach, ensuring efficient, fragmentation-free memory management while accommodating various computation kernels across different LLM architectures.","Experimental results indicate that vTensor achieves an average speedup of 1.86x across different models, with up to 2.42x in multi-turn chat scenarios.","Additionally, vTensor provides average speedups of 2.12x and 3.15x in kernel evaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton prefix-prefilling kernels and vLLM paged Attention kernel, respectively.","Furthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100 GPU compared to vLLM, enabling more memory-intensive workloads."],"url":"http://arxiv.org/abs/2407.15309v1"}
{"created":"2024-07-22 14:33:10","title":"Coca4ai: checking energy behaviors on AI data centers","abstract":"Monitoring energy behaviors in AI data centers is crucial, both to reduce their energy consumption and to raise awareness among their users which are key actors in the AI field. This paper shows a proof of concept of easy and lightweight monitoring of energy behaviors at the scale of a whole data center, a user or a job submission. Our system uses software wattmeters and we validate our setup with per node accurate external wattmeters. Results show that there is an interesting potential from the efficiency point of view, providing arguments to create user engagement thanks to energy monitoring.","sentences":["Monitoring energy behaviors in AI data centers is crucial, both to reduce their energy consumption and to raise awareness among their users which are key actors in the AI field.","This paper shows a proof of concept of easy and lightweight monitoring of energy behaviors at the scale of a whole data center, a user or a job submission.","Our system uses software wattmeters and we validate our setup with per node accurate external wattmeters.","Results show that there is an interesting potential from the efficiency point of view, providing arguments to create user engagement thanks to energy monitoring."],"url":"http://arxiv.org/abs/2407.15670v1"}
{"created":"2024-07-22 14:29:36","title":"SLVideo: A Sign Language Video Moment Retrieval Framework","abstract":"Sign Language Recognition has been studied and developed throughout the years to help the deaf and hard-of-hearing people in their day-to-day lives. These technologies leverage manual sign recognition algorithms, however, most of them lack the recognition of facial expressions, which are also an essential part of Sign Language as they allow the speaker to add expressiveness to their dialogue or even change the meaning of certain manual signs. SLVideo is a video moment retrieval software for Sign Language videos with a focus on both hands and facial signs. The system extracts embedding representations for the hand and face signs from video frames to capture the language signs in full. This will then allow the user to search for a specific sign language video segment with text queries, or to search by similar sign language videos. To test this system, a collection of five hours of annotated Sign Language videos is used as the dataset, and the initial results are promising in a zero-shot setting.SLVideo is shown to not only address the problem of searching sign language videos but also supports a Sign Language thesaurus with a search by similarity technique.   Project web page: https://novasearch.github.io/SLVideo/","sentences":["Sign Language Recognition has been studied and developed throughout the years to help the deaf and hard-of-hearing people in their day-to-day lives.","These technologies leverage manual sign recognition algorithms, however, most of them lack the recognition of facial expressions, which are also an essential part of Sign Language as they allow the speaker to add expressiveness to their dialogue or even change the meaning of certain manual signs.","SLVideo is a video moment retrieval software for Sign Language videos with a focus on both hands and facial signs.","The system extracts embedding representations for the hand and face signs from video frames to capture the language signs in full.","This will then allow the user to search for a specific sign language video segment with text queries, or to search by similar sign language videos.","To test this system, a collection of five hours of annotated Sign Language videos is used as the dataset, and the initial results are promising in a zero-shot setting.","SLVideo is shown to not only address the problem of searching sign language videos but also supports a Sign Language thesaurus with a search by similarity technique.   ","Project web page: https://novasearch.github.io/SLVideo/"],"url":"http://arxiv.org/abs/2407.15668v1"}
{"created":"2024-07-22 14:28:46","title":"A spatiotemporal deep learning framework for prediction of crack dynamics in heterogeneous solids: efficient mapping of concrete microstructures to its fracture properties","abstract":"A spatiotemporal deep learning framework is proposed that is capable of 2D full-field prediction of fracture in concrete mesostructures. This framework not only predicts fractures but also captures the entire history of the fracture process, from the crack initiation in the interfacial transition zone to the subsequent propagation of the cracks in the mortar matrix. In addition, a convolutional neural network is developed which can predict the averaged stress-strain curve of the mesostructures. The UNet modeling framework, which comprises an encoder-decoder section with skip connections, is used as the deep learning surrogate model. Training and test data are generated from high-fidelity fracture simulations of randomly generated concrete mesostructures. These mesostructures include geometric variabilities such as different aggregate particle geometrical features, spatial distribution, and the total volume fraction of aggregates. The fracture simulations are carried out in Abaqus, utilizing the cohesive phase-field fracture modeling technique as the fracture modeling approach. In this work, to reduce the number of training datasets, the spatial distribution of three sets of material properties for three-phase concrete mesostructures, along with the spatial phase-field damage index, are fed to the UNet to predict the corresponding stress and spatial damage index at the subsequent step. It is shown that after the training process using this methodology, the UNet model is capable of accurately predicting damage on the unseen test dataset by using 470 datasets. Moreover, another novel aspect of this work is the conversion of irregular finite element data into regular grids using a developed pipeline. This approach allows for the implementation of less complex UNet architecture and facilitates the integration of phase-field fracture equations into surrogate models for future developments.","sentences":["A spatiotemporal deep learning framework is proposed that is capable of 2D full-field prediction of fracture in concrete mesostructures.","This framework not only predicts fractures but also captures the entire history of the fracture process, from the crack initiation in the interfacial transition zone to the subsequent propagation of the cracks in the mortar matrix.","In addition, a convolutional neural network is developed which can predict the averaged stress-strain curve of the mesostructures.","The UNet modeling framework, which comprises an encoder-decoder section with skip connections, is used as the deep learning surrogate model.","Training and test data are generated from high-fidelity fracture simulations of randomly generated concrete mesostructures.","These mesostructures include geometric variabilities such as different aggregate particle geometrical features, spatial distribution, and the total volume fraction of aggregates.","The fracture simulations are carried out in Abaqus, utilizing the cohesive phase-field fracture modeling technique as the fracture modeling approach.","In this work, to reduce the number of training datasets, the spatial distribution of three sets of material properties for three-phase concrete mesostructures, along with the spatial phase-field damage index, are fed to the UNet to predict the corresponding stress and spatial damage index at the subsequent step.","It is shown that after the training process using this methodology, the UNet model is capable of accurately predicting damage on the unseen test dataset by using 470 datasets.","Moreover, another novel aspect of this work is the conversion of irregular finite element data into regular grids using a developed pipeline.","This approach allows for the implementation of less complex UNet architecture and facilitates the integration of phase-field fracture equations into surrogate models for future developments."],"url":"http://arxiv.org/abs/2407.15665v1"}
{"created":"2024-07-22 14:24:56","title":"MSSPlace: Multi-Sensor Place Recognition with Visual and Text Semantics","abstract":"Place recognition is a challenging task in computer vision, crucial for enabling autonomous vehicles and robots to navigate previously visited environments. While significant progress has been made in learnable multimodal methods that combine onboard camera images and LiDAR point clouds, the full potential of these methods remains largely unexplored in localization applications. In this paper, we study the impact of leveraging a multi-camera setup and integrating diverse data sources for multimodal place recognition, incorporating explicit visual semantics and text descriptions. Our proposed method named MSSPlace utilizes images from multiple cameras, LiDAR point clouds, semantic segmentation masks, and text annotations to generate comprehensive place descriptors. We employ a late fusion approach to integrate these modalities, providing a unified representation. Through extensive experiments on the Oxford RobotCar and NCLT datasets, we systematically analyze the impact of each data source on the overall quality of place descriptors. Our experiments demonstrate that combining data from multiple sensors significantly improves place recognition model performance compared to single modality approaches and leads to state-of-the-art quality. We also show that separate usage of visual or textual semantics (which are more compact representations of sensory data) can achieve promising results in place recognition. The code for our method is publicly available: https://github.com/alexmelekhin/MSSPlace","sentences":["Place recognition is a challenging task in computer vision, crucial for enabling autonomous vehicles and robots to navigate previously visited environments.","While significant progress has been made in learnable multimodal methods that combine onboard camera images and LiDAR point clouds, the full potential of these methods remains largely unexplored in localization applications.","In this paper, we study the impact of leveraging a multi-camera setup and integrating diverse data sources for multimodal place recognition, incorporating explicit visual semantics and text descriptions.","Our proposed method named MSSPlace utilizes images from multiple cameras, LiDAR point clouds, semantic segmentation masks, and text annotations to generate comprehensive place descriptors.","We employ a late fusion approach to integrate these modalities, providing a unified representation.","Through extensive experiments on the Oxford RobotCar and NCLT datasets, we systematically analyze the impact of each data source on the overall quality of place descriptors.","Our experiments demonstrate that combining data from multiple sensors significantly improves place recognition model performance compared to single modality approaches and leads to state-of-the-art quality.","We also show that separate usage of visual or textual semantics (which are more compact representations of sensory data) can achieve promising results in place recognition.","The code for our method is publicly available: https://github.com/alexmelekhin/MSSPlace"],"url":"http://arxiv.org/abs/2407.15663v1"}
{"created":"2024-07-22 14:18:52","title":"MuTT: A Multimodal Trajectory Transformer for Robot Skills","abstract":"High-level robot skills represent an increasingly popular paradigm in robot programming. However, configuring the skills' parameters for a specific task remains a manual and time-consuming endeavor. Existing approaches for learning or optimizing these parameters often require numerous real-world executions or do not work in dynamic environments. To address these challenges, we propose MuTT, a novel encoder-decoder transformer architecture designed to predict environment-aware executions of robot skills by integrating vision, trajectory, and robot skill parameters. Notably, we pioneer the fusion of vision and trajectory, introducing a novel trajectory projection. Furthermore, we illustrate MuTT's efficacy as a predictor when combined with a model-based robot skill optimizer. This approach facilitates the optimization of robot skill parameters for the current environment, without the need for real-world executions during optimization. Designed for compatibility with any representation of robot skills, MuTT demonstrates its versatility across three comprehensive experiments, showcasing superior performance across two different skill representations.","sentences":["High-level robot skills represent an increasingly popular paradigm in robot programming.","However, configuring the skills' parameters for a specific task remains a manual and time-consuming endeavor.","Existing approaches for learning or optimizing these parameters often require numerous real-world executions or do not work in dynamic environments.","To address these challenges, we propose MuTT, a novel encoder-decoder transformer architecture designed to predict environment-aware executions of robot skills by integrating vision, trajectory, and robot skill parameters.","Notably, we pioneer the fusion of vision and trajectory, introducing a novel trajectory projection.","Furthermore, we illustrate MuTT's efficacy as a predictor when combined with a model-based robot skill optimizer.","This approach facilitates the optimization of robot skill parameters for the current environment, without the need for real-world executions during optimization.","Designed for compatibility with any representation of robot skills, MuTT demonstrates its versatility across three comprehensive experiments, showcasing superior performance across two different skill representations."],"url":"http://arxiv.org/abs/2407.15660v1"}
{"created":"2024-07-22 14:18:52","title":"DriveDiTFit: Fine-tuning Diffusion Transformers for Autonomous Driving","abstract":"In autonomous driving, deep models have shown remarkable performance across various visual perception tasks with the demand of high-quality and huge-diversity training datasets. Such datasets are expected to cover various driving scenarios with adverse weather, lighting conditions and diverse moving objects. However, manually collecting these data presents huge challenges and expensive cost. With the rapid development of large generative models, we propose DriveDiTFit, a novel method for efficiently generating autonomous Driving data by Fine-tuning pre-trained Diffusion Transformers (DiTs). Specifically, DriveDiTFit utilizes a gap-driven modulation technique to carefully select and efficiently fine-tune a few parameters in DiTs according to the discrepancy between the pre-trained source data and the target driving data. Additionally, DriveDiTFit develops an effective weather and lighting condition embedding module to ensure diversity in the generated data, which is initialized by a nearest-semantic-similarity initialization approach. Through progressive tuning scheme to refined the process of detail generation in early diffusion process and enlarging the weights corresponding to small objects in training loss, DriveDiTFit ensures high-quality generation of small moving objects in the generated data. Extensive experiments conducted on driving datasets confirm that our method could efficiently produce diverse real driving data. The source codes will be available at https://github.com/TtuHamg/DriveDiTFit.","sentences":["In autonomous driving, deep models have shown remarkable performance across various visual perception tasks with the demand of high-quality and huge-diversity training datasets.","Such datasets are expected to cover various driving scenarios with adverse weather, lighting conditions and diverse moving objects.","However, manually collecting these data presents huge challenges and expensive cost.","With the rapid development of large generative models, we propose DriveDiTFit, a novel method for efficiently generating autonomous Driving data by Fine-tuning pre-trained Diffusion Transformers (DiTs).","Specifically, DriveDiTFit utilizes a gap-driven modulation technique to carefully select and efficiently fine-tune a few parameters in DiTs according to the discrepancy between the pre-trained source data and the target driving data.","Additionally, DriveDiTFit develops an effective weather and lighting condition embedding module to ensure diversity in the generated data, which is initialized by a nearest-semantic-similarity initialization approach.","Through progressive tuning scheme to refined the process of detail generation in early diffusion process and enlarging the weights corresponding to small objects in training loss, DriveDiTFit ensures high-quality generation of small moving objects in the generated data.","Extensive experiments conducted on driving datasets confirm that our method could efficiently produce diverse real driving data.","The source codes will be available at https://github.com/TtuHamg/DriveDiTFit."],"url":"http://arxiv.org/abs/2407.15661v1"}
{"created":"2024-07-22 14:17:29","title":"Evaluation of Reinforcement Learning for Autonomous Penetration Testing using A3C, Q-learning and DQN","abstract":"Penetration testing is the process of searching for security weaknesses by simulating an attack. It is usually performed by experienced professionals, where scanning and attack tools are applied. By automating the execution of such tools, the need for human interaction and decision-making could be reduced. In this work, a Network Attack Simulator (NASim) was used as an environment to train reinforcement learning agents to solve three predefined security scenarios. These scenarios cover techniques of exploitation, post-exploitation and wiretapping. A large hyperparameter grid search was performed to find the best hyperparameter combinations. The algorithms Q-learning, DQN and A3C were used, whereby A3C was able to solve all scenarios and achieve generalization. In addition, A3C could solve these scenarios with fewer actions than the baseline automated penetration testing. Although the training was performed on rather small scenarios and with small state and action spaces for the agents, the results show that a penetration test can successfully be performed by the RL agent.","sentences":["Penetration testing is the process of searching for security weaknesses by simulating an attack.","It is usually performed by experienced professionals, where scanning and attack tools are applied.","By automating the execution of such tools, the need for human interaction and decision-making could be reduced.","In this work, a Network Attack Simulator (NASim) was used as an environment to train reinforcement learning agents to solve three predefined security scenarios.","These scenarios cover techniques of exploitation, post-exploitation and wiretapping.","A large hyperparameter grid search was performed to find the best hyperparameter combinations.","The algorithms Q-learning, DQN and A3C were used, whereby A3C was able to solve all scenarios and achieve generalization.","In addition, A3C could solve these scenarios with fewer actions than the baseline automated penetration testing.","Although the training was performed on rather small scenarios and with small state and action spaces for the agents, the results show that a penetration test can successfully be performed by the RL agent."],"url":"http://arxiv.org/abs/2407.15656v1"}
{"created":"2024-07-22 14:05:27","title":"TreeSBA: Tree-Transformer for Self-Supervised Sequential Brick Assembly","abstract":"Inferring step-wise actions to assemble 3D objects with primitive bricks from images is a challenging task due to complex constraints and the vast number of possible combinations. Recent studies have demonstrated promising results on sequential LEGO brick assembly through the utilization of LEGO-Graph modeling to predict sequential actions. However, existing approaches are class-specific and require significant computational and 3D annotation resources. In this work, we first propose a computationally efficient breadth-first search (BFS) LEGO-Tree structure to model the sequential assembly actions by considering connections between consecutive layers. Based on the LEGO-Tree structure, we then design a class-agnostic tree-transformer framework to predict the sequential assembly actions from the input multi-view images. A major challenge of the sequential brick assembly task is that the step-wise action labels are costly and tedious to obtain in practice. We mitigate this problem by leveraging synthetic-to-real transfer learning. Specifically, our model is first pre-trained on synthetic data with full supervision from the available action labels. We then circumvent the requirement for action labels in the real data by proposing an action-to-silhouette projection that replaces action labels with input image silhouettes for self-supervision. Without any annotation on the real data, our model outperforms existing methods with 3D supervision by 7.8% and 11.3% in mIoU on the MNIST and ModelNet Construction datasets, respectively.","sentences":["Inferring step-wise actions to assemble 3D objects with primitive bricks from images is a challenging task due to complex constraints and the vast number of possible combinations.","Recent studies have demonstrated promising results on sequential LEGO brick assembly through the utilization of LEGO-Graph modeling to predict sequential actions.","However, existing approaches are class-specific and require significant computational and 3D annotation resources.","In this work, we first propose a computationally efficient breadth-first search (BFS) LEGO-Tree structure to model the sequential assembly actions by considering connections between consecutive layers.","Based on the LEGO-Tree structure, we then design a class-agnostic tree-transformer framework to predict the sequential assembly actions from the input multi-view images.","A major challenge of the sequential brick assembly task is that the step-wise action labels are costly and tedious to obtain in practice.","We mitigate this problem by leveraging synthetic-to-real transfer learning.","Specifically, our model is first pre-trained on synthetic data with full supervision from the available action labels.","We then circumvent the requirement for action labels in the real data by proposing an action-to-silhouette projection that replaces action labels with input image silhouettes for self-supervision.","Without any annotation on the real data, our model outperforms existing methods with 3D supervision by 7.8% and 11.3% in mIoU on the MNIST and ModelNet Construction datasets, respectively."],"url":"http://arxiv.org/abs/2407.15648v1"}
