{"text": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs.","cats":{"gnn":1}}
{"text": "We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions.","cats":{"gnn":1}}
{"text": "Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes.","cats":{"gnn":1}}
{"text": "In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.","cats":{"gnn":1}}
{"text": "Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature.","cats":{"gnn":1}}
{"text": "These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph.","cats":{"gnn":1}}
{"text": "In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework.","cats":{"gnn":1}}
{"text": "Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.","cats":{"gnn":1}}
{"text": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.","cats":{"gnn":1}}
{"text": "By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.","cats":{"gnn":1}}
{"text": "In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems.","cats":{"gnn":1}}
{"text": "Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","cats":{"gnn":1}}