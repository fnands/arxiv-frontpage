{"text": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs.","cats":{"gnn":1}}
{"text": "We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions.","cats":{"gnn":1}}
{"text": "Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes.","cats":{"gnn":1}}
{"text": "In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.","cats":{"gnn":1}}
{"text": "Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature.","cats":{"gnn":1}}
{"text": "These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph.","cats":{"gnn":1}}
{"text": "In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework.","cats":{"gnn":1}}
{"text": "Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.","cats":{"gnn":1}}
{"text": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.","cats":{"gnn":1}}
{"text": "By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.","cats":{"gnn":1}}
{"text": "In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems.","cats":{"gnn":1}}
{"text": "Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","cats":{"gnn":1}}
{"text": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available.","cats":{"gnn":0}}
{"text": "In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance.","cats":{"gnn":0}}
{"text": "Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient.","cats":{"gnn":0}}
{"text": "We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.","cats":{"gnn":0}}
{"text": "To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets.","cats":{"gnn":0}}
{"text": "In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet.","cats":{"gnn":0}}
{"text": "Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.","cats":{"gnn":0}}
{"text": "Source code is at this https URL.","cats":{"gnn":0}}